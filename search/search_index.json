{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GeminiTeacher","text":"<p>GeminiTeacher is an AI-powered course creation toolkit using Google's Gemini LLM. It transforms raw text, markdown, or documents into structured, lesson-based educational content. Ideal for educators, developers, and content creators.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started, follow these two simple steps:</p> <ol> <li>Installation and Setup: A step-by-step guide to install the package and configure your Google API key.</li> <li>Usage Guide: A comprehensive walkthrough of how to generate courses using both the command-line interface and the Python API.</li> </ol>"},{"location":"#key-documentation-sections","title":"Key Documentation Sections","text":"<ul> <li> <p>Installation Guide Everything you need to get up and running.</p> </li> <li> <p>Usage Guide Detailed instructions for CLI and Python API usage, from basic to advanced.</p> </li> <li> <p>Modules Overview A technical deep-dive into the package's architecture and core components.</p> </li> <li> <p>Parallel Processing Learn how to leverage parallel generation for maximum speed.</p> </li> </ul>"},{"location":"installation/","title":"Installation and Setup","text":"<p>This guide provides the steps to install GeminiTeacher and configure your environment.</p>"},{"location":"installation/#1-install-the-package","title":"1. Install the Package","text":"<p>Install GeminiTeacher directly from the Python Package Index (PyPI):</p> <pre><code>pip install geminiteacher\n</code></pre> <p>This command installs the package along with its command-line interface.</p>"},{"location":"installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>To verify that the command-line tool is properly installed, you can run:</p> <pre><code>geminiteacher --help\n</code></pre> <p>If you see the help message, the installation was successful.</p> <p>If the command is not found, your Python scripts directory might not be in your system's PATH. In that case, you can always run the tool using Python's module syntax:</p> <pre><code>python -m geminiteacher.app.generate_course --help\n</code></pre>"},{"location":"installation/#2-set-up-your-google-api-key","title":"2. Set Up Your Google API Key","text":"<p>GeminiTeacher requires a Google API key with access to the Gemini family of models.</p> <p>You must provide this key to the application. The recommended way is to set it as an environment variable, which keeps your key secure and out of your source code.</p>"},{"location":"installation/#for-linuxmacos","title":"For Linux/macOS","text":"<pre><code>export GOOGLE_API_KEY=\"your-api-key-here\"\n</code></pre> <p>To make this permanent, add the line to your <code>~/.bashrc</code>, <code>~/.zshrc</code>, or other shell configuration file.</p>"},{"location":"installation/#for-windows-powershell","title":"For Windows (PowerShell)","text":"<pre><code>$env:GOOGLE_API_KEY=\"your-api-key-here\"\n</code></pre> <p>To make this permanent, add this line to your PowerShell profile script.</p>"},{"location":"installation/#for-windows-command-prompt","title":"For Windows (Command Prompt)","text":"<pre><code>set GOOGLE_API_KEY=your-api-key-here\n</code></pre> <p>You can also place the key directly in a <code>config.yaml</code> file, but using an environment variable is best practice. </p>"},{"location":"modules/","title":"GeminiTeacher Modules","text":"<p>GeminiTeacher consists of several modules that work together to provide a complete course generation solution.</p>"},{"location":"modules/#core-modules","title":"Core Modules","text":""},{"location":"modules/#coursegenerator-geminiteachercoursemaker","title":"CourseGenerator (<code>geminiteacher.coursemaker</code>)","text":"<p>The main module responsible for generating course content.</p> <pre><code>from geminiteacher import create_course, configure_gemini_llm\n\n# Configure the language model\nllm = configure_gemini_llm(temperature=0.2)\n\n# Generate a course\ncourse = create_course(\n    content=\"Your content here...\",\n    llm=llm,\n    max_chapters=5\n)\n\n# Access the generated course\nprint(f\"Course summary: {course.summary}\")\nfor chapter in course.chapters:\n    print(f\"Chapter: {chapter.title}\")\n    print(f\"Summary: {chapter.summary}\")\n</code></pre>"},{"location":"modules/#parallel-processing-geminiteacherparallel","title":"Parallel Processing (<code>geminiteacher.parallel</code>)","text":"<p>Provides parallel processing capabilities for faster course generation.</p> <pre><code>from geminiteacher import parallel_generate_chapters, configure_gemini_llm\nfrom geminiteacher import Course, ChapterContent\n\n# Configure the language model\nllm = configure_gemini_llm(temperature=0.2)\n\n# Generate chapters in parallel\nchapter_titles = [\"Introduction\", \"Basic Concepts\", \"Advanced Topics\"]\nchapters = parallel_generate_chapters(\n    content=\"Your content here...\",\n    chapter_titles=chapter_titles,\n    llm=llm,\n    max_workers=3,\n    course_title=\"My Course\",\n    output_dir=\"output\"\n)\n\n# Create a course with the generated chapters\ncourse = Course(\n    summary=\"Course summary\",\n    chapters=chapters\n)\n</code></pre>"},{"location":"modules/#command-line-application-geminiteacherapp","title":"Command-line Application (<code>geminiteacher.app</code>)","text":"<p>A full-featured command-line application for generating courses.</p>"},{"location":"modules/#command-line-usage","title":"Command-line Usage","text":"<p>After installation, you can use the <code>geminiteacher</code> command directly:</p> <pre><code># Basic usage with config file\ngeminiteacher --config config.yaml\n\n# Specify input file and output directory\ngeminiteacher --input content.txt --output-dir courses\n\n# Set course title and use parallel processing\ngeminiteacher --input content.txt --title \"Machine Learning Basics\" --parallel\n</code></pre>"},{"location":"modules/#programmatic-usage","title":"Programmatic Usage","text":"<p>You can also use the app module programmatically:</p> <pre><code>from geminiteacher.app import create_course_with_progressive_save, configure_logging\n\n# Configure logging\nlogger = configure_logging(log_file=\"generation.log\", verbose=True)\n\n# Generate a course with progressive saving\ncourse = create_course_with_progressive_save(\n    content=\"Your content here...\",\n    course_title=\"Python Programming\",\n    output_dir=\"courses\",\n    temperature=0.2,\n    max_chapters=5,\n    verbose=True,\n    logger=logger\n)\n</code></pre>"},{"location":"modules/#data-models","title":"Data Models","text":"<p>GeminiTeacher uses the following data models:</p>"},{"location":"modules/#course","title":"Course","text":"<pre><code>class Course:\n    \"\"\"\n    Represents a complete course with summary and chapters.\n    \"\"\"\n    summary: str\n    chapters: List[ChapterContent]\n</code></pre>"},{"location":"modules/#chaptercontent","title":"ChapterContent","text":"<pre><code>class ChapterContent:\n    \"\"\"\n    Represents a single chapter in a course.\n    \"\"\"\n    title: str\n    summary: str\n    explanation: str\n    extension: str\n</code></pre>"},{"location":"parallel/","title":"Parallel Processing","text":"<p>The <code>parallel</code> module provides tools for parallel execution of tasks with controlled API rate limiting and robust error handling. This module is particularly useful for accelerating the chapter generation process while respecting API rate limits.</p>"},{"location":"parallel/#overview","title":"Overview","text":"<p>The parallel processing capabilities include:</p> <ol> <li>Controlled Parallel Execution: Execute tasks in parallel with configurable workers</li> <li>Rate Limit Management: Add randomized delays between API requests to avoid rate limits</li> <li>Robust Error Handling: Automatically retry failed requests with exponential backoff</li> <li>Ordered Results: Ensure outputs are returned in the same order as inputs, regardless of completion time</li> <li>Progressive File Saving: Each chapter is saved to disk as soon as it's generated, preventing data loss</li> </ol>"},{"location":"parallel/#key-functions","title":"Key Functions","text":""},{"location":"parallel/#parallel_generate_chapters","title":"parallel_generate_chapters","text":"<p>The main orchestration function that handles parallel generation of course chapters:</p> <pre><code>import geminiteacher as gt\nfrom geminiteacher.parallel import parallel_generate_chapters\n\n# Define chapter titles to generate\nchapter_titles = [\n    \"Introduction to Machine Learning\",\n    \"Supervised Learning Algorithms\",\n    \"Unsupervised Learning Techniques\"\n]\n\n# Generate chapters in parallel\nchapters = parallel_generate_chapters(\n    chapter_titles=chapter_titles,\n    content=\"Your raw content here\",\n    max_workers=4,              # Number of parallel workers (processes)\n    delay_range=(0.2, 0.8),     # Random delay between API requests in seconds\n    max_retries=3,              # Number of retry attempts for failed requests\n    course_title=\"ML_Course\",   # Title for saved files\n    output_dir=\"courses\"        # Directory to save generated chapters\n)\n\n# Process the generated chapters\nfor i, chapter in enumerate(chapters):\n    print(f\"Chapter {i+1}: {chapter.title}\")\n</code></pre>"},{"location":"parallel/#generate_chapter_with_retry","title":"generate_chapter_with_retry","text":"<p>A robust wrapper around the standard <code>generate_chapter</code> function that adds retry logic:</p> <pre><code>import geminiteacher as gt\nfrom geminiteacher.parallel import generate_chapter_with_retry\n\n# Generate a single chapter with retry logic\nchapter = generate_chapter_with_retry(\n    chapter_title=\"Introduction to Neural Networks\",\n    content=\"Your raw content here\",\n    max_retries=3,              # Maximum number of retry attempts\n    retry_delay=1.0             # Base delay between retries (will increase exponentially)\n)\n\nprint(f\"Chapter title: {chapter.title}\")\nprint(f\"Summary: {chapter.summary[:100]}...\")\n</code></pre>"},{"location":"parallel/#parallel_map_with_delay","title":"parallel_map_with_delay","text":"<p>A generic function for applying any function to a list of items in parallel with controlled delays:</p> <pre><code>from geminiteacher.parallel import parallel_map_with_delay\nimport time\n\n# Define a function to execute in parallel\ndef process_item(item, prefix=\"Item\"):\n    # Simulate some work\n    time.sleep(0.5)\n    return f\"{prefix}: {item}\"\n\n# Items to process\nitems = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\"]\n\n# Process items in parallel with controlled delays\nresults = parallel_map_with_delay(\n    func=process_item,\n    items=items,\n    max_workers=3,              # Number of parallel workers\n    delay_range=(0.1, 0.5),     # Random delay between task submissions\n    prefix=\"Processed\"          # Additional parameter passed to process_item\n)\n\n# Results are in the same order as the input items\nfor item, result in zip(items, results):\n    print(f\"Original: {item} \u2192 Result: {result}\")\n</code></pre>"},{"location":"parallel/#progressive-file-saving","title":"Progressive File Saving","text":"<p>A key feature of the parallel processing module is its ability to save chapters to disk as they are generated. This provides several benefits:</p> <ol> <li>Data Safety: Even if the process is interrupted, completed chapters are already saved</li> <li>Progress Tracking: You can monitor progress by watching files appear in the output directory</li> <li>Immediate Access: Start reviewing early chapters while later ones are still being generated</li> </ol> <p>Example of how files are saved:</p> <pre><code>import geminiteacher as gt\n\ncourse = gt.create_course_parallel(\n    content=\"Your content here\",\n    course_title=\"Data_Science\",\n    output_dir=\"my_courses\"\n)\n\n# Files will be saved in a structure like:\n# my_courses/\n#   \u2514\u2500\u2500 Data_Science/\n#       \u251c\u2500\u2500 chapter_01_Introduction_to_Data_Science.md\n#       \u251c\u2500\u2500 chapter_02_Data_Collection_and_Cleaning.md\n#       \u2514\u2500\u2500 chapter_03_Exploratory_Data_Analysis.md\n</code></pre> <p>Each chapter file contains the structured content with title, summary, explanation, and extension sections.</p>"},{"location":"parallel/#api-rate-limits-consideration","title":"API Rate Limits Consideration","text":"<p>When working with external APIs like Google's Gemini, rate limits are an important consideration. The <code>parallel</code> module helps manage these limits through controlled submission timing:</p> <ol> <li>Random Delays: Adds a configurable random delay between API requests to avoid overwhelming the API</li> <li>Exponential Backoff: When retrying failed requests, uses exponential backoff to gradually increase wait times</li> <li>Configurable Workers: Allows limiting the number of concurrent processes to respect API parallelism limits</li> </ol>"},{"location":"parallel/#recommended-settings-for-google-gemini-api","title":"Recommended Settings for Google Gemini API","text":"<p>For the Google Gemini API, the following settings work well for most scenarios:</p> <ul> <li><code>max_workers</code>: 2-6 (depending on your API tier)</li> <li><code>delay_range</code>: (0.2, 1.0) seconds</li> <li><code>max_retries</code>: 3</li> </ul> <p>These settings balance speed with API reliability. For higher API tiers with more generous rate limits, you can increase <code>max_workers</code> and decrease the delay range.</p>"},{"location":"parallel/#error-handling","title":"Error Handling","text":"<p>The parallel module implements comprehensive error handling:</p> <ol> <li>Retries for Empty Responses: Automatically retries when the API returns empty content</li> <li>Exception Recovery: Catches and handles API errors with automatic retries</li> <li>Fallback Content: If all retries fail, returns a structured error message instead of failing completely</li> </ol> <p>This ensures robustness even when dealing with unreliable network conditions or API instability.</p>"},{"location":"parallel/#api-reference","title":"API Reference","text":""},{"location":"parallel/#core-functions","title":"Core Functions","text":"<p>Generate multiple chapters in parallel with retry logic and rate limiting.</p> <p>This function orchestrates the parallel generation of multiple chapters, handling API rate limits and retrying failed requests. Each chapter is saved to disk as soon as it's generated.</p> <p>Generate a chapter with retry logic for handling API failures.</p> <p>This function wraps the generate_chapter function with retry logic to handle transient API errors, timeouts, or empty responses.</p> <p>Execute a function on multiple items in parallel with a delay between submissions.</p> <p>This function uses ProcessPoolExecutor to parallelize the execution of a function across multiple items, while introducing a random delay between task submissions to avoid overwhelming external APIs with simultaneous requests.</p>"},{"location":"parallel/#geminiteacher.parallel.parallel_generate_chapters--parameters","title":"Parameters","text":"<p>chapter_titles : List[str]     List of chapter titles to generate. content : str     The raw content to use for generating chapters. llm : Optional[BaseLanguageModel], optional     The language model to use. If None, a default model will be configured. temperature : float, optional     The temperature setting for generation. Default is 0.0. custom_prompt : Optional[str], optional     Custom instructions to append to the chapter generation prompt. max_workers : Optional[int], optional     Maximum number of worker processes. If None, uses the default. delay_range : tuple, optional     Range (min, max) in seconds for the random delay between task submissions.     Default is (0.1, 0.5). max_retries : int, optional     Maximum number of retry attempts per chapter. Default is 3. course_title : str, optional     Title of the course for saving files. Default is \"course\". output_dir : str, optional     Directory to save the chapter files. Default is \"output\".</p>"},{"location":"parallel/#geminiteacher.parallel.parallel_generate_chapters--returns","title":"Returns","text":"<p>List[ChapterContent]     List of generated chapter contents in the same order as the input titles.</p> Source code in <code>src\\geminiteacher\\parallel.py</code> <pre><code>def parallel_generate_chapters(\n    chapter_titles: List[str],\n    content: str,\n    llm: Optional[BaseLanguageModel] = None,\n    temperature: float = 0.0,\n    custom_prompt: Optional[str] = None,\n    max_workers: Optional[int] = None,\n    delay_range: tuple = (0.1, 0.5),\n    max_retries: int = 3,\n    course_title: str = \"course\",\n    output_dir: str = \"output\"\n) -&gt; List[ChapterContent]:\n    \"\"\"\n    Generate multiple chapters in parallel with retry logic and rate limiting.\n\n    This function orchestrates the parallel generation of multiple chapters,\n    handling API rate limits and retrying failed requests. Each chapter is\n    saved to disk as soon as it's generated.\n\n    Parameters\n    ----------\n    chapter_titles : List[str]\n        List of chapter titles to generate.\n    content : str\n        The raw content to use for generating chapters.\n    llm : Optional[BaseLanguageModel], optional\n        The language model to use. If None, a default model will be configured.\n    temperature : float, optional\n        The temperature setting for generation. Default is 0.0.\n    custom_prompt : Optional[str], optional\n        Custom instructions to append to the chapter generation prompt.\n    max_workers : Optional[int], optional\n        Maximum number of worker processes. If None, uses the default.\n    delay_range : tuple, optional\n        Range (min, max) in seconds for the random delay between task submissions.\n        Default is (0.1, 0.5).\n    max_retries : int, optional\n        Maximum number of retry attempts per chapter. Default is 3.\n    course_title : str, optional\n        Title of the course for saving files. Default is \"course\".\n    output_dir : str, optional\n        Directory to save the chapter files. Default is \"output\".\n\n    Returns\n    -------\n    List[ChapterContent]\n        List of generated chapter contents in the same order as the input titles.\n    \"\"\"\n    logger = logging.getLogger(\"geminiteacher.parallel\")\n    logger.info(f\"Starting parallel generation of {len(chapter_titles)} chapters with {max_workers or multiprocessing.cpu_count()} workers\")\n\n    # Get API key from the LLM if provided or environment\n    api_key = None\n    model_name = \"gemini-1.5-pro\"\n\n    if llm is not None:\n        # Try to extract API key and model name from the provided LLM\n        try:\n            # This assumes LLM is a ChatGoogleGenerativeAI instance\n            api_key = getattr(llm, \"google_api_key\", None)\n            model_name = getattr(llm, \"model\", model_name)\n            logger.info(f\"Using model: {model_name}\")\n        except Exception:\n            logger.warning(\"Could not extract API key from provided LLM, will use environment variables\")\n\n    # Create a list of (index, chapter_title) tuples to preserve order\n    indexed_titles = list(enumerate(chapter_titles))\n\n    logger.info(f\"Using delay range: {delay_range[0]}-{delay_range[1]}s between tasks\")\n    logger.info(f\"Saving chapters progressively to {output_dir}/{course_title}/\")\n\n    # Generate chapters in parallel with delay between submissions and save each one as it completes\n    results = parallel_map_with_delay(\n        _worker_generate_and_save_chapter,\n        indexed_titles,\n        max_workers=max_workers,\n        delay_range=delay_range,\n        content=content,\n        course_title=course_title,\n        output_dir=output_dir,\n        api_key=api_key,\n        model_name=model_name,\n        temperature=temperature,\n        custom_prompt=custom_prompt,\n        max_retries=max_retries,\n        retry_delay=1.0\n    )\n\n    # Extract just the chapter content from the results (idx, chapter, file_path)\n    chapters = [result[1] for result in results]\n\n    logger.info(f\"Completed parallel generation of {len(chapters)} chapters\")\n    return chapters \n</code></pre>"},{"location":"parallel/#geminiteacher.parallel.generate_chapter_with_retry--parameters","title":"Parameters","text":"<p>chapter_title : str     The title of the chapter to generate. content : str     The raw content to use for generating the chapter. llm : Optional[BaseLanguageModel], optional     The language model to use. If None, a default model will be configured. temperature : float, optional     The temperature setting for generation. Default is 0.0. custom_prompt : Optional[str], optional     Custom instructions to append to the chapter generation prompt. max_retries : int, optional     Maximum number of retry attempts. Default is 3. retry_delay : float, optional     Base delay between retries in seconds. Default is 1.0.</p>"},{"location":"parallel/#geminiteacher.parallel.generate_chapter_with_retry--returns","title":"Returns","text":"<p>ChapterContent     The generated chapter content.</p>"},{"location":"parallel/#geminiteacher.parallel.generate_chapter_with_retry--notes","title":"Notes","text":"<p>This function implements an exponential backoff strategy for retries, with each retry attempt waiting longer than the previous one.</p> Source code in <code>src\\geminiteacher\\parallel.py</code> <pre><code>def generate_chapter_with_retry(\n    chapter_title: str, \n    content: str, \n    llm: Optional[BaseLanguageModel] = None,\n    temperature: float = 0.0,\n    custom_prompt: Optional[str] = None,\n    max_retries: int = 3,\n    retry_delay: float = 1.0\n) -&gt; ChapterContent:\n    \"\"\"\n    Generate a chapter with retry logic for handling API failures.\n\n    This function wraps the generate_chapter function with retry logic to handle\n    transient API errors, timeouts, or empty responses.\n\n    Parameters\n    ----------\n    chapter_title : str\n        The title of the chapter to generate.\n    content : str\n        The raw content to use for generating the chapter.\n    llm : Optional[BaseLanguageModel], optional\n        The language model to use. If None, a default model will be configured.\n    temperature : float, optional\n        The temperature setting for generation. Default is 0.0.\n    custom_prompt : Optional[str], optional\n        Custom instructions to append to the chapter generation prompt.\n    max_retries : int, optional\n        Maximum number of retry attempts. Default is 3.\n    retry_delay : float, optional\n        Base delay between retries in seconds. Default is 1.0.\n\n    Returns\n    -------\n    ChapterContent\n        The generated chapter content.\n\n    Notes\n    -----\n    This function implements an exponential backoff strategy for retries,\n    with each retry attempt waiting longer than the previous one.\n    \"\"\"\n    logger = logging.getLogger(\"geminiteacher.parallel\")\n\n    for attempt in range(max_retries + 1):\n        try:\n            logger.info(f\"Generating chapter '{chapter_title}' (attempt {attempt + 1}/{max_retries + 1})\")\n            chapter = generate_chapter(\n                chapter_title=chapter_title,\n                content=content,\n                llm=llm,\n                temperature=temperature,\n                custom_prompt=custom_prompt\n            )\n\n            # Check if we got a valid response (non-empty explanation)\n            if chapter.explanation.strip():\n                logger.info(f\"Successfully generated chapter '{chapter_title}' (length: {len(chapter.explanation)} chars)\")\n                return chapter\n            else:\n                raise ValueError(\"Empty chapter explanation received\")\n\n        except Exception as e:\n            if attempt &lt; max_retries:\n                # Calculate backoff with jitter\n                backoff = retry_delay * (2 ** attempt) + random.uniform(0, 1)\n                logger.warning(\n                    f\"Chapter generation failed for '{chapter_title}' \"\n                    f\"(attempt {attempt + 1}/{max_retries + 1}): {str(e)}. \"\n                    f\"Retrying in {backoff:.2f}s...\"\n                )\n                time.sleep(backoff)\n            else:\n                logger.error(\n                    f\"All retry attempts failed for chapter '{chapter_title}'. \"\n                    f\"Last error: {str(e)}\"\n                )\n                # Return a basic chapter with error information\n                return ChapterContent(\n                    title=chapter_title,\n                    summary=\"Error: Failed to generate chapter content after multiple attempts.\",\n                    explanation=f\"The chapter generation process encountered repeated errors: {str(e)}\",\n                    extension=\"Please try regenerating this chapter or check your API configuration.\"\n                )\n</code></pre>"},{"location":"parallel/#geminiteacher.parallel.parallel_map_with_delay--parameters","title":"Parameters","text":"<p>func : Callable[..., T]     The function to execute in parallel. items : List[Any]     The list of items to process. max_workers : Optional[int], optional     Maximum number of worker processes. If None, uses the default     (typically the number of CPU cores). delay_range : tuple, optional     Range (min, max) in seconds for the random delay between task submissions.     Default is (0.1, 0.5). **kwargs     Additional keyword arguments to pass to the function.</p>"},{"location":"parallel/#geminiteacher.parallel.parallel_map_with_delay--returns","title":"Returns","text":"<p>List[T]     List of results in the same order as the input items.</p>"},{"location":"parallel/#geminiteacher.parallel.parallel_map_with_delay--examples","title":"Examples","text":"<p>def process_item(item, factor=1): ...     return item * factor items = [1, 2, 3, 4, 5] results = parallel_map_with_delay(process_item, items, factor=2) print(results) [2, 4, 6, 8, 10]</p> Source code in <code>src\\geminiteacher\\parallel.py</code> <pre><code>def parallel_map_with_delay(\n    func: Callable[..., T],\n    items: List[Any],\n    max_workers: Optional[int] = None,\n    delay_range: tuple = (0.1, 0.5),\n    **kwargs\n) -&gt; List[T]:\n    \"\"\"\n    Execute a function on multiple items in parallel with a delay between submissions.\n\n    This function uses ProcessPoolExecutor to parallelize the execution of a function\n    across multiple items, while introducing a random delay between task submissions\n    to avoid overwhelming external APIs with simultaneous requests.\n\n    Parameters\n    ----------\n    func : Callable[..., T]\n        The function to execute in parallel.\n    items : List[Any]\n        The list of items to process.\n    max_workers : Optional[int], optional\n        Maximum number of worker processes. If None, uses the default\n        (typically the number of CPU cores).\n    delay_range : tuple, optional\n        Range (min, max) in seconds for the random delay between task submissions.\n        Default is (0.1, 0.5).\n    **kwargs\n        Additional keyword arguments to pass to the function.\n\n    Returns\n    -------\n    List[T]\n        List of results in the same order as the input items.\n\n    Examples\n    --------\n    &gt;&gt;&gt; def process_item(item, factor=1):\n    ...     return item * factor\n    &gt;&gt;&gt; items = [1, 2, 3, 4, 5]\n    &gt;&gt;&gt; results = parallel_map_with_delay(process_item, items, factor=2)\n    &gt;&gt;&gt; print(results)\n    [2, 4, 6, 8, 10]\n    \"\"\"\n    logger = logging.getLogger(\"geminiteacher.parallel\")\n    results = []\n    total_items = len(items)\n\n    # Export the current log level to the environment for worker processes\n    os.environ[\"GeminiTeacher_LOG_LEVEL\"] = logger.getEffectiveLevel().__str__()\n\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        # Submit all tasks with a delay between submissions\n        futures = []\n        for i, item in enumerate(items):\n            # Add a small random delay to avoid overwhelming the API\n            delay = random.uniform(delay_range[0], delay_range[1])\n            logger.debug(f\"Submitting task {i+1}/{total_items} with delay: {delay:.2f}s\")\n            time.sleep(delay)\n\n            # Submit the task to the process pool\n            future = executor.submit(func, item, **kwargs)\n            futures.append(future)\n            logger.info(f\"Submitted task {i+1}/{total_items}\")\n\n        # Collect results in the original order\n        for i, future in enumerate(futures):\n            try:\n                logger.info(f\"Waiting for task {i+1}/{total_items} to complete\")\n                result = future.result()\n                results.append(result)\n                logger.info(f\"Completed task {i+1}/{total_items}\")\n            except Exception as e:\n                logger.error(f\"Task {i+1}/{total_items} failed: {str(e)}\")\n                # Re-raise the exception to maintain the expected behavior\n                raise\n\n    return results\n</code></pre>"},{"location":"parallel/#performance-considerations","title":"Performance Considerations","text":"<p>When using parallel processing, consider the following to optimize performance:</p> <ol> <li>CPU Cores: The optimal <code>max_workers</code> is typically close to the number of available CPU cores</li> <li>Memory Usage: Each worker process requires memory, so limit <code>max_workers</code> on memory-constrained systems</li> <li>API Rate Limits: Always respect API rate limits by adjusting <code>delay_range</code> and <code>max_workers</code></li> <li>Task Granularity: Parallel processing works best when individual tasks take significant time</li> </ol>"},{"location":"parallel/#integration-with-course-generator","title":"Integration with Course Generator","text":"<p>The parallel module integrates seamlessly with the coursemaker module through the <code>create_course_parallel</code> function:</p> <pre><code>import geminiteacher as gt\n\n# Generate a course using parallel processing\ncourse = gt.create_course_parallel(\n    \"Your raw content here\",\n    max_workers=4,\n    delay_range=(0.2, 0.8),\n    max_retries=3,\n    course_title=\"Advanced_Topics\",\n    output_dir=\"output/courses\"\n)\n\nprint(f\"Generated {len(course.chapters)} chapters in parallel\")\n</code></pre>"},{"location":"parallel/#limitations","title":"Limitations","text":"<ul> <li>Increased memory usage compared to sequential processing</li> <li>Potential for higher API costs due to faster request rates</li> <li>Debugging can be more complex in parallel environments</li> </ul>"},{"location":"parallel/#future-enhancements","title":"Future Enhancements","text":"<p>Future versions may include: - Adaptive rate limiting based on API response times - Better telemetry for monitoring API usage - Support for concurrent.futures.ThreadPoolExecutor for I/O-bound tasks - Dynamic worker allocation based on system resources </p>"},{"location":"usage/","title":"Usage Guide","text":"<p>This guide covers the primary ways to use GeminiTeacher, from the simple command-line interface (CLI) to the flexible Python API.</p> <p>For setup instructions, please see the Installation and Setup guide first.</p>"},{"location":"usage/#command-line-usage-recommended","title":"Command-Line Usage (Recommended)","text":"<p>The easiest and most powerful way to use GeminiTeacher is through its command-line interface, which is ideal for generating courses without writing any code. We recommend using a configuration file for the best experience.</p>"},{"location":"usage/#method-1-using-a-configyaml-file-best-practice","title":"Method 1: Using a <code>config.yaml</code> File (Best Practice)","text":"<p>Create a <code>config.yaml</code> file to manage all your settings in one place. This is the most organized and repeatable way to generate courses.</p> <ol> <li> <p>Create a <code>config.yaml</code> file:</p> <p>```yaml</p> </li> <li> <p>Run the command:</p> <p>Simply point the CLI to your configuration file.</p> <p><code>bash geminiteacher --config config.yaml</code></p> </li> </ol>"},{"location":"usage/#configyaml","title":"config.yaml","text":""},{"location":"usage/#-inputoutput-settings-","title":"--- Input/Output Settings ---","text":"<p>input:   # Path to your raw content file (e.g., a text file with your notes).   path: \"input/content.txt\" output:   # Directory where the generated course files will be saved.   directory: \"output/MyFirstCourse\"</p>"},{"location":"usage/#-course-settings-","title":"--- Course Settings ---","text":"<p>course:   # The title of your course.   title: \"Introduction to Artificial Intelligence\"   # Optional: Path to a file with custom instructions for the AI.   custom_prompt: \"prompts/formal_tone_prompt.txt\"</p>"},{"location":"usage/#-generation-settings-","title":"--- Generation Settings ---","text":"<p>generation:   # Controls the \"creativity\" of the AI. Lower is more predictable. (0.0-1.0)   temperature: 0.2   # The target number of chapters for the course.   max_chapters: 8   # If true, the AI will generate exactly <code>max_chapters</code>.   fixed_chapter_count: true</p>"},{"location":"usage/#-performance-settings-","title":"--- Performance Settings ---","text":"<p>parallel:   # Enable parallel processing to generate chapters simultaneously for speed.   enabled: true   # Number of parallel processes to run. Defaults to your CPU count.   max_workers: 4 ```</p>"},{"location":"usage/#method-2-using-command-line-flags","title":"Method 2: Using Command-Line Flags","text":"<p>You can also control the generation process directly with command-line arguments. These are useful for quick, one-off tasks.</p> <pre><code># Basic usage with a text file\ngeminiteacher --input content.txt --output-dir courses --title \"Machine Learning Basics\"\n\n# Enable parallel processing for faster generation\ngeminiteacher --input content.txt --parallel --max-workers 4\n\n# Use a custom prompt file and set the temperature\ngeminiteacher --input content.txt --custom-prompt prompts.txt --temperature 0.3\n</code></pre>"},{"location":"usage/#all-command-line-options","title":"All Command-Line Options","text":"<p>Here is the full list of available command-line options. Any option passed as a flag will override the corresponding value in a <code>config.yaml</code> file.</p> Option Argument Description <code>--config</code>, <code>-c</code> <code>PATH</code> Path to the YAML configuration file. <code>--input</code>, <code>-i</code> <code>PATH</code> Path to the input content file. <code>--output-dir</code>, <code>-o</code> <code>PATH</code> Directory to save generated course files. <code>--title</code>, <code>-t</code> <code>TEXT</code> Course title. <code>--custom-prompt</code>, <code>-p</code> <code>PATH</code> Path to a custom prompt instructions file. <code>--temperature</code> <code>FLOAT</code> Temperature for generation (0.0-1.0). <code>--max-chapters</code> <code>INTEGER</code> Maximum number of chapters to generate. <code>--fixed-chapter-count</code> (flag) If set, generates exactly <code>max-chapters</code>. <code>--parallel</code> (flag) If set, use parallel processing for chapter generation. <code>--max-workers</code> <code>INTEGER</code> Max worker processes for parallel generation. <code>--verbose</code>, <code>-v</code> (flag) Enable verbose output for detailed progress logging. <code>--log-file</code> <code>PATH</code> Optional path to a file to save logs."},{"location":"usage/#troubleshooting-the-cli","title":"Troubleshooting the CLI","text":"<p>\"command not found\" error?</p> <p>If your shell cannot find the <code>geminiteacher</code> command, you can run it directly as a Python module:</p> <pre><code>python -m geminiteacher.app.generate_course --config config.yaml\n</code></pre> <p>This happens when your Python scripts directory is not in your system's <code>PATH</code>. See the installation guide for more details.</p> <p>API Key Errors?</p> <p>If you get authentication errors, ensure your <code>GOOGLE_API_KEY</code> is set correctly as an environment variable, as described in the installation guide.</p>"},{"location":"usage/#python-api-usage","title":"Python API Usage","text":"<p>For programmatic use, you can import GeminiTeacher directly into your Python code.</p>"},{"location":"usage/#high-level-functions-recommended","title":"High-Level Functions (Recommended)","text":"<p>The easiest way to use the API is with the <code>create_course</code> and <code>create_course_parallel</code> functions.</p>"},{"location":"usage/#sequential-generation","title":"Sequential Generation","text":"<p>This is the simplest method, generating one component at a time.</p> <pre><code>import geminiteacher as gt\n\n# Your raw content to transform into a course\nwith open(\"content.txt\", \"r\") as f:\n    content = f.read()\n\n# Generate the full course structure\ncourse = gt.create_course(\n    content=content,\n    max_chapters=5,\n    temperature=0.2\n)\n\n# Print the generated course structure\nprint(f\"Course Summary: {course.summary}\\n\")\nfor chapter in course.chapters:\n    print(f\"- {chapter.title}\")\n</code></pre>"},{"location":"usage/#parallel-generation-for-speed","title":"Parallel Generation for Speed","text":"<p>Use <code>create_course_parallel</code> for large documents to significantly speed up chapter generation.</p> <pre><code>import geminiteacher as gt\n\nwith open(\"content.txt\", \"r\") as f:\n    content = f.read()\n\n# Generate a course with chapters created in parallel\n# This also saves chapters to disk as they complete\ncourse = gt.create_course_parallel(\n    content=content,\n    course_title=\"My Parallel Course\",\n    output_dir=\"courses_output\",\n    max_chapters=10,\n    max_workers=4\n)\n\nprint(f\"Generated {len(course.chapters)} chapters in parallel.\")\n</code></pre>"},{"location":"usage/#using-individual-components","title":"Using Individual Components","text":"<p>For maximum control, you can use the individual functions from the generation pipeline.</p> <pre><code>from geminiteacher import (\n    configure_gemini_llm, \n    generate_toc, \n    generate_chapter, \n    generate_summary\n)\n\n# 1. Configure the LLM\nllm = configure_gemini_llm(temperature=0.1)\n\n# 2. Generate the Table of Contents\nchapter_titles = generate_toc(content, llm=llm, max_chapters=5)\n\n# 3. Generate each chapter individually\nchapters = [\n    generate_chapter(title, content, llm=llm) \n    for title in chapter_titles\n]\n\n# 4. Generate the final summary\nsummary = generate_summary(content, chapters, llm=llm)\n\nprint(\"Course generation complete!\")\n</code></pre>"}]}