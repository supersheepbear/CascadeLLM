{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GeminiTeacher","text":"<p>GeminiTeacher is an AI-powered course creation toolkit using Google's Gemini LLM. It transforms raw text, markdown, or documents into structured, lesson-based educational content. Ideal for educators, developers, and content creators.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>All the information you need, from installation to advanced usage, has been consolidated into a single, comprehensive guide.</p> <ul> <li>The Complete Usage Guide Click here to learn how to install, configure, and use the GUI, CLI, and Python API.</li> </ul>"},{"location":"#key-documentation-sections","title":"Key Documentation Sections","text":"<ul> <li> <p>The Complete Usage Guide Everything you need to get up and running.</p> </li> <li> <p>Modules Overview A technical deep-dive into the package's architecture and core components.</p> </li> <li> <p>Parallel Processing Learn how to leverage parallel generation for maximum speed.</p> </li> </ul>"},{"location":"modules/","title":"GeminiTeacher Modules","text":"<p>GeminiTeacher consists of several modules that work together to provide a complete course generation solution.</p>"},{"location":"modules/#core-modules","title":"Core Modules","text":""},{"location":"modules/#coursegenerator-geminiteachercoursemaker","title":"CourseGenerator (<code>geminiteacher.coursemaker</code>)","text":"<p>The main module responsible for generating course content.</p> <pre><code>from geminiteacher import create_course, configure_gemini_llm\n\n# Configure the language model\nllm = configure_gemini_llm(temperature=0.2)\n\n# Generate a course\ncourse = create_course(\n    content=\"Your content here...\",\n    llm=llm,\n    max_chapters=5\n)\n\n# Access the generated course\nprint(f\"Course summary: {course.summary}\")\nfor chapter in course.chapters:\n    print(f\"Chapter: {chapter.title}\")\n    print(f\"Summary: {chapter.summary}\")\n</code></pre>"},{"location":"modules/#parallel-processing-geminiteacherparallel","title":"Parallel Processing (<code>geminiteacher.parallel</code>)","text":"<p>Provides parallel processing capabilities for faster course generation.</p> <pre><code>from geminiteacher import parallel_generate_chapters, configure_gemini_llm\nfrom geminiteacher import Course, ChapterContent\n\n# Configure the language model\nllm = configure_gemini_llm(temperature=0.2)\n\n# Generate chapters in parallel\nchapter_titles = [\"Introduction\", \"Basic Concepts\", \"Advanced Topics\"]\nchapters = parallel_generate_chapters(\n    content=\"Your content here...\",\n    chapter_titles=chapter_titles,\n    llm=llm,\n    max_workers=3,\n    course_title=\"My Course\",\n    output_dir=\"output\"\n)\n\n# Create a course with the generated chapters\ncourse = Course(\n    summary=\"Course summary\",\n    chapters=chapters\n)\n</code></pre>"},{"location":"modules/#command-line-application-geminiteacherapp","title":"Command-line Application (<code>geminiteacher.app</code>)","text":"<p>A full-featured command-line application for generating courses.</p>"},{"location":"modules/#command-line-usage","title":"Command-line Usage","text":"<p>After installation, you can use the <code>geminiteacher</code> command directly:</p> <pre><code># Basic usage with config file\ngeminiteacher --config config.yaml\n\n# Specify input file and output directory\ngeminiteacher --input content.txt --output-dir courses\n\n# Set course title and use parallel processing\ngeminiteacher --input content.txt --title \"Machine Learning Basics\" --parallel\n</code></pre>"},{"location":"modules/#programmatic-usage","title":"Programmatic Usage","text":"<p>You can also use the app module programmatically:</p> <pre><code>from geminiteacher.app import create_course_with_progressive_save, configure_logging\n\n# Configure logging\nlogger = configure_logging(log_file=\"generation.log\", verbose=True)\n\n# Generate a course with progressive saving\ncourse = create_course_with_progressive_save(\n    content=\"Your content here...\",\n    course_title=\"Python Programming\",\n    output_dir=\"courses\",\n    temperature=0.2,\n    max_chapters=5,\n    verbose=True,\n    logger=logger\n)\n</code></pre>"},{"location":"modules/#data-models","title":"Data Models","text":"<p>GeminiTeacher uses the following data models:</p>"},{"location":"modules/#course","title":"Course","text":"<pre><code>class Course:\n    \"\"\"\n    Represents a complete course with summary and chapters.\n    \"\"\"\n    summary: str\n    chapters: List[ChapterContent]\n</code></pre>"},{"location":"modules/#chaptercontent","title":"ChapterContent","text":"<pre><code>class ChapterContent:\n    \"\"\"\n    Represents a single chapter in a course.\n    \"\"\"\n    title: str\n    summary: str\n    explanation: str\n    extension: str\n</code></pre>"},{"location":"parallel/","title":"Parallel Processing","text":"<p>The <code>parallel</code> module provides tools for parallel execution of tasks with controlled API rate limiting and robust error handling. This module is particularly useful for accelerating the chapter generation process while respecting API rate limits.</p>"},{"location":"parallel/#overview","title":"Overview","text":"<p>The parallel processing capabilities include:</p> <ol> <li>Controlled Parallel Execution: Execute tasks in parallel with configurable workers</li> <li>Rate Limit Management: Add randomized delays between API requests to avoid rate limits</li> <li>Robust Error Handling: Automatically retry failed requests with exponential backoff</li> <li>Ordered Results: Ensure outputs are returned in the same order as inputs, regardless of completion time</li> <li>Progressive File Saving: Each chapter is saved to disk as soon as it's generated, preventing data loss</li> </ol>"},{"location":"parallel/#key-functions","title":"Key Functions","text":""},{"location":"parallel/#parallel_generate_chapters","title":"parallel_generate_chapters","text":"<p>The main orchestration function that handles parallel generation of course chapters:</p> <pre><code>import geminiteacher as gt\nfrom geminiteacher.parallel import parallel_generate_chapters\n\n# Define chapter titles to generate\nchapter_titles = [\n    \"Introduction to Machine Learning\",\n    \"Supervised Learning Algorithms\",\n    \"Unsupervised Learning Techniques\"\n]\n\n# Generate chapters in parallel\nchapters = parallel_generate_chapters(\n    chapter_titles=chapter_titles,\n    content=\"Your raw content here\",\n    max_workers=4,              # Number of parallel workers (processes)\n    delay_range=(0.2, 0.8),     # Random delay between API requests in seconds\n    max_retries=3,              # Number of retry attempts for failed requests\n    course_title=\"ML_Course\",   # Title for saved files\n    output_dir=\"courses\"        # Directory to save generated chapters\n)\n\n# Process the generated chapters\nfor i, chapter in enumerate(chapters):\n    print(f\"Chapter {i+1}: {chapter.title}\")\n</code></pre>"},{"location":"parallel/#generate_chapter_with_retry","title":"generate_chapter_with_retry","text":"<p>A robust wrapper around the standard <code>generate_chapter</code> function that adds retry logic:</p> <pre><code>import geminiteacher as gt\nfrom geminiteacher.parallel import generate_chapter_with_retry\n\n# Generate a single chapter with retry logic\nchapter = generate_chapter_with_retry(\n    chapter_title=\"Introduction to Neural Networks\",\n    content=\"Your raw content here\",\n    max_retries=3,              # Maximum number of retry attempts\n    retry_delay=1.0             # Base delay between retries (will increase exponentially)\n)\n\nprint(f\"Chapter title: {chapter.title}\")\nprint(f\"Summary: {chapter.summary[:100]}...\")\n</code></pre>"},{"location":"parallel/#parallel_map_with_delay","title":"parallel_map_with_delay","text":"<p>A generic function for applying any function to a list of items in parallel with controlled delays:</p> <pre><code>from geminiteacher.parallel import parallel_map_with_delay\nimport time\n\n# Define a function to execute in parallel\ndef process_item(item, prefix=\"Item\"):\n    # Simulate some work\n    time.sleep(0.5)\n    return f\"{prefix}: {item}\"\n\n# Items to process\nitems = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\"]\n\n# Process items in parallel with controlled delays\nresults = parallel_map_with_delay(\n    func=process_item,\n    items=items,\n    max_workers=3,              # Number of parallel workers\n    delay_range=(0.1, 0.5),     # Random delay between task submissions\n    prefix=\"Processed\"          # Additional parameter passed to process_item\n)\n\n# Results are in the same order as the input items\nfor item, result in zip(items, results):\n    print(f\"Original: {item} \u2192 Result: {result}\")\n</code></pre>"},{"location":"parallel/#progressive-file-saving","title":"Progressive File Saving","text":"<p>A key feature of the parallel processing module is its ability to save chapters to disk as they are generated. This provides several benefits:</p> <ol> <li>Data Safety: Even if the process is interrupted, completed chapters are already saved</li> <li>Progress Tracking: You can monitor progress by watching files appear in the output directory</li> <li>Immediate Access: Start reviewing early chapters while later ones are still being generated</li> </ol> <p>Example of how files are saved:</p> <pre><code>import geminiteacher as gt\n\ncourse = gt.create_course_parallel(\n    content=\"Your content here\",\n    course_title=\"Data_Science\",\n    output_dir=\"my_courses\"\n)\n\n# Files will be saved in a structure like:\n# my_courses/\n#   \u2514\u2500\u2500 Data_Science/\n#       \u251c\u2500\u2500 chapter_01_Introduction_to_Data_Science.md\n#       \u251c\u2500\u2500 chapter_02_Data_Collection_and_Cleaning.md\n#       \u2514\u2500\u2500 chapter_03_Exploratory_Data_Analysis.md\n</code></pre> <p>Each chapter file contains the structured content with title, summary, explanation, and extension sections.</p>"},{"location":"parallel/#api-rate-limits-consideration","title":"API Rate Limits Consideration","text":"<p>When working with external APIs like Google's Gemini, rate limits are an important consideration. The <code>parallel</code> module helps manage these limits through controlled submission timing:</p> <ol> <li>Random Delays: Adds a configurable random delay between API requests to avoid overwhelming the API</li> <li>Exponential Backoff: When retrying failed requests, uses exponential backoff to gradually increase wait times</li> <li>Configurable Workers: Allows limiting the number of concurrent processes to respect API parallelism limits</li> </ol>"},{"location":"parallel/#recommended-settings-for-google-gemini-api","title":"Recommended Settings for Google Gemini API","text":"<p>For the Google Gemini API, the following settings work well for most scenarios:</p> <ul> <li><code>max_workers</code>: 2-6 (depending on your API tier)</li> <li><code>delay_range</code>: (0.2, 1.0) seconds</li> <li><code>max_retries</code>: 3</li> </ul> <p>These settings balance speed with API reliability. For higher API tiers with more generous rate limits, you can increase <code>max_workers</code> and decrease the delay range.</p>"},{"location":"parallel/#error-handling","title":"Error Handling","text":"<p>The parallel module implements comprehensive error handling:</p> <ol> <li>Retries for Empty Responses: Automatically retries when the API returns empty content</li> <li>Exception Recovery: Catches and handles API errors with automatic retries</li> <li>Fallback Content: If all retries fail, returns a structured error message instead of failing completely</li> </ol> <p>This ensures robustness even when dealing with unreliable network conditions or API instability.</p>"},{"location":"parallel/#api-reference","title":"API Reference","text":""},{"location":"parallel/#core-functions","title":"Core Functions","text":"<p>Generate multiple chapters in parallel with retry logic and rate limiting.</p> <p>This function orchestrates the parallel generation of multiple chapters, handling API rate limits and retrying failed requests. Each chapter is saved to disk as soon as it's generated.</p> <p>Generate a chapter with retry logic for handling API failures.</p> <p>This function wraps the generate_chapter function with retry logic to handle transient API errors, timeouts, or empty responses.</p> <p>Execute a function on multiple items in parallel with a delay between submissions.</p> <p>This function uses ProcessPoolExecutor to parallelize the execution of a function across multiple items, while introducing a random delay between task submissions to avoid overwhelming external APIs with simultaneous requests.</p>"},{"location":"parallel/#geminiteacher.parallel.parallel_generate_chapters--parameters","title":"Parameters","text":"<p>chapter_titles : List[str]     List of chapter titles to generate. content : str     The raw content to use for generating chapters. llm : Optional[BaseLanguageModel], optional     The language model to use. If None, a default model will be configured. temperature : float, optional     The temperature setting for generation. Default is 0.0. custom_prompt : Optional[str], optional     Custom instructions to append to the chapter generation prompt. max_workers : Optional[int], optional     Maximum number of worker processes. If None, uses the default. delay_range : tuple, optional     Range (min, max) in seconds for the random delay between task submissions.     Default is (0.1, 0.5). max_retries : int, optional     Maximum number of retry attempts per chapter. Default is 3. course_title : str, optional     Title of the course for saving files. Default is \"course\". output_dir : str, optional     Directory to save the chapter files. Default is \"output\".</p>"},{"location":"parallel/#geminiteacher.parallel.parallel_generate_chapters--returns","title":"Returns","text":"<p>List[ChapterContent]     List of generated chapter contents in the same order as the input titles.</p> Source code in <code>src\\geminiteacher\\parallel.py</code> <pre><code>def parallel_generate_chapters(\n    chapter_titles: List[str],\n    content: str,\n    llm: Optional[BaseLanguageModel] = None,\n    temperature: float = 0.0,\n    custom_prompt: Optional[str] = None,\n    max_workers: Optional[int] = None,\n    delay_range: tuple = (0.1, 0.5),\n    max_retries: int = 3,\n    course_title: str = \"course\",\n    output_dir: str = \"output\"\n) -&gt; List[ChapterContent]:\n    \"\"\"\n    Generate multiple chapters in parallel with retry logic and rate limiting.\n\n    This function orchestrates the parallel generation of multiple chapters,\n    handling API rate limits and retrying failed requests. Each chapter is\n    saved to disk as soon as it's generated.\n\n    Parameters\n    ----------\n    chapter_titles : List[str]\n        List of chapter titles to generate.\n    content : str\n        The raw content to use for generating chapters.\n    llm : Optional[BaseLanguageModel], optional\n        The language model to use. If None, a default model will be configured.\n    temperature : float, optional\n        The temperature setting for generation. Default is 0.0.\n    custom_prompt : Optional[str], optional\n        Custom instructions to append to the chapter generation prompt.\n    max_workers : Optional[int], optional\n        Maximum number of worker processes. If None, uses the default.\n    delay_range : tuple, optional\n        Range (min, max) in seconds for the random delay between task submissions.\n        Default is (0.1, 0.5).\n    max_retries : int, optional\n        Maximum number of retry attempts per chapter. Default is 3.\n    course_title : str, optional\n        Title of the course for saving files. Default is \"course\".\n    output_dir : str, optional\n        Directory to save the chapter files. Default is \"output\".\n\n    Returns\n    -------\n    List[ChapterContent]\n        List of generated chapter contents in the same order as the input titles.\n    \"\"\"\n    logger = logging.getLogger(\"geminiteacher.parallel\")\n    logger.info(f\"Starting parallel generation of {len(chapter_titles)} chapters with {max_workers or multiprocessing.cpu_count()} workers\")\n\n    # Get API key from the LLM if provided or environment\n    api_key = None\n    model_name = \"gemini-1.5-pro\"\n\n    if llm is not None:\n        # Try to extract API key and model name from the provided LLM\n        try:\n            # This assumes LLM is a ChatGoogleGenerativeAI instance\n            api_key = getattr(llm, \"google_api_key\", None)\n            model_name = getattr(llm, \"model\", model_name)\n            logger.info(f\"Using model: {model_name}\")\n        except Exception:\n            logger.warning(\"Could not extract API key from provided LLM, will use environment variables\")\n\n    # Create a list of (index, chapter_title) tuples to preserve order\n    indexed_titles = list(enumerate(chapter_titles))\n\n    logger.info(f\"Using delay range: {delay_range[0]}-{delay_range[1]}s between tasks\")\n    logger.info(f\"Saving chapters progressively to {output_dir}/{course_title}/\")\n\n    # Generate chapters in parallel with delay between submissions and save each one as it completes\n    results = parallel_map_with_delay(\n        _worker_generate_and_save_chapter,\n        indexed_titles,\n        max_workers=max_workers,\n        delay_range=delay_range,\n        content=content,\n        course_title=course_title,\n        output_dir=output_dir,\n        api_key=api_key,\n        model_name=model_name,\n        temperature=temperature,\n        custom_prompt=custom_prompt,\n        max_retries=max_retries,\n        retry_delay=1.0\n    )\n\n    # Extract just the chapter content from the results (idx, chapter, file_path)\n    chapters = [result[1] for result in results]\n\n    logger.info(f\"Completed parallel generation of {len(chapters)} chapters\")\n    return chapters \n</code></pre>"},{"location":"parallel/#geminiteacher.parallel.generate_chapter_with_retry--parameters","title":"Parameters","text":"<p>chapter_title : str     The title of the chapter to generate. content : str     The raw content to use for generating the chapter. llm : Optional[BaseLanguageModel], optional     The language model to use. If None, a default model will be configured. temperature : float, optional     The temperature setting for generation. Default is 0.0. custom_prompt : Optional[str], optional     Custom instructions to append to the chapter generation prompt. max_retries : int, optional     Maximum number of retry attempts. Default is 3. retry_delay : float, optional     Base delay between retries in seconds. Default is 1.0.</p>"},{"location":"parallel/#geminiteacher.parallel.generate_chapter_with_retry--returns","title":"Returns","text":"<p>ChapterContent     The generated chapter content.</p>"},{"location":"parallel/#geminiteacher.parallel.generate_chapter_with_retry--notes","title":"Notes","text":"<p>This function implements an exponential backoff strategy for retries, with each retry attempt waiting longer than the previous one.</p> Source code in <code>src\\geminiteacher\\parallel.py</code> <pre><code>def generate_chapter_with_retry(\n    chapter_title: str, \n    content: str, \n    llm: Optional[BaseLanguageModel] = None,\n    temperature: float = 0.0,\n    custom_prompt: Optional[str] = None,\n    max_retries: int = 3,\n    retry_delay: float = 1.0\n) -&gt; ChapterContent:\n    \"\"\"\n    Generate a chapter with retry logic for handling API failures.\n\n    This function wraps the generate_chapter function with retry logic to handle\n    transient API errors, timeouts, or empty responses.\n\n    Parameters\n    ----------\n    chapter_title : str\n        The title of the chapter to generate.\n    content : str\n        The raw content to use for generating the chapter.\n    llm : Optional[BaseLanguageModel], optional\n        The language model to use. If None, a default model will be configured.\n    temperature : float, optional\n        The temperature setting for generation. Default is 0.0.\n    custom_prompt : Optional[str], optional\n        Custom instructions to append to the chapter generation prompt.\n    max_retries : int, optional\n        Maximum number of retry attempts. Default is 3.\n    retry_delay : float, optional\n        Base delay between retries in seconds. Default is 1.0.\n\n    Returns\n    -------\n    ChapterContent\n        The generated chapter content.\n\n    Notes\n    -----\n    This function implements an exponential backoff strategy for retries,\n    with each retry attempt waiting longer than the previous one.\n    \"\"\"\n    logger = logging.getLogger(\"geminiteacher.parallel\")\n\n    for attempt in range(max_retries + 1):\n        try:\n            logger.info(f\"Generating chapter '{chapter_title}' (attempt {attempt + 1}/{max_retries + 1})\")\n            chapter = generate_chapter(\n                chapter_title=chapter_title,\n                content=content,\n                llm=llm,\n                temperature=temperature,\n                custom_prompt=custom_prompt\n            )\n\n            # Check if we got a valid response (non-empty explanation)\n            if chapter.explanation.strip():\n                logger.info(f\"Successfully generated chapter '{chapter_title}' (length: {len(chapter.explanation)} chars)\")\n                return chapter\n            else:\n                raise ValueError(\"Empty chapter explanation received\")\n\n        except Exception as e:\n            if attempt &lt; max_retries:\n                # Calculate backoff with jitter\n                backoff = retry_delay * (2 ** attempt) + random.uniform(0, 1)\n                logger.warning(\n                    f\"Chapter generation failed for '{chapter_title}' \"\n                    f\"(attempt {attempt + 1}/{max_retries + 1}): {str(e)}. \"\n                    f\"Retrying in {backoff:.2f}s...\"\n                )\n                time.sleep(backoff)\n            else:\n                logger.error(\n                    f\"All retry attempts failed for chapter '{chapter_title}'. \"\n                    f\"Last error: {str(e)}\"\n                )\n                # Return a basic chapter with error information\n                return ChapterContent(\n                    title=chapter_title,\n                    summary=\"Error: Failed to generate chapter content after multiple attempts.\",\n                    explanation=f\"The chapter generation process encountered repeated errors: {str(e)}\",\n                    extension=\"Please try regenerating this chapter or check your API configuration.\"\n                )\n</code></pre>"},{"location":"parallel/#geminiteacher.parallel.parallel_map_with_delay--parameters","title":"Parameters","text":"<p>func : Callable[..., T]     The function to execute in parallel. items : List[Any]     The list of items to process. max_workers : Optional[int], optional     Maximum number of worker processes. If None, uses the default     (typically the number of CPU cores). delay_range : tuple, optional     Range (min, max) in seconds for the random delay between task submissions.     Default is (0.1, 0.5). **kwargs     Additional keyword arguments to pass to the function.</p>"},{"location":"parallel/#geminiteacher.parallel.parallel_map_with_delay--returns","title":"Returns","text":"<p>List[T]     List of results in the same order as the input items.</p>"},{"location":"parallel/#geminiteacher.parallel.parallel_map_with_delay--examples","title":"Examples","text":"<p>def process_item(item, factor=1): ...     return item * factor items = [1, 2, 3, 4, 5] results = parallel_map_with_delay(process_item, items, factor=2) print(results) [2, 4, 6, 8, 10]</p> Source code in <code>src\\geminiteacher\\parallel.py</code> <pre><code>def parallel_map_with_delay(\n    func: Callable[..., T],\n    items: List[Any],\n    max_workers: Optional[int] = None,\n    delay_range: tuple = (0.1, 0.5),\n    **kwargs\n) -&gt; List[T]:\n    \"\"\"\n    Execute a function on multiple items in parallel with a delay between submissions.\n\n    This function uses ProcessPoolExecutor to parallelize the execution of a function\n    across multiple items, while introducing a random delay between task submissions\n    to avoid overwhelming external APIs with simultaneous requests.\n\n    Parameters\n    ----------\n    func : Callable[..., T]\n        The function to execute in parallel.\n    items : List[Any]\n        The list of items to process.\n    max_workers : Optional[int], optional\n        Maximum number of worker processes. If None, uses the default\n        (typically the number of CPU cores).\n    delay_range : tuple, optional\n        Range (min, max) in seconds for the random delay between task submissions.\n        Default is (0.1, 0.5).\n    **kwargs\n        Additional keyword arguments to pass to the function.\n\n    Returns\n    -------\n    List[T]\n        List of results in the same order as the input items.\n\n    Examples\n    --------\n    &gt;&gt;&gt; def process_item(item, factor=1):\n    ...     return item * factor\n    &gt;&gt;&gt; items = [1, 2, 3, 4, 5]\n    &gt;&gt;&gt; results = parallel_map_with_delay(process_item, items, factor=2)\n    &gt;&gt;&gt; print(results)\n    [2, 4, 6, 8, 10]\n    \"\"\"\n    logger = logging.getLogger(\"geminiteacher.parallel\")\n    results = []\n    total_items = len(items)\n\n    # Export the current log level to the environment for worker processes\n    os.environ[\"GeminiTeacher_LOG_LEVEL\"] = logger.getEffectiveLevel().__str__()\n\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        # Submit all tasks with a delay between submissions\n        futures = []\n        for i, item in enumerate(items):\n            # Add a small random delay to avoid overwhelming the API\n            delay = random.uniform(delay_range[0], delay_range[1])\n            logger.debug(f\"Submitting task {i+1}/{total_items} with delay: {delay:.2f}s\")\n            time.sleep(delay)\n\n            # Submit the task to the process pool\n            future = executor.submit(func, item, **kwargs)\n            futures.append(future)\n            logger.info(f\"Submitted task {i+1}/{total_items}\")\n\n        # Collect results in the original order\n        for i, future in enumerate(futures):\n            try:\n                logger.info(f\"Waiting for task {i+1}/{total_items} to complete\")\n                result = future.result()\n                results.append(result)\n                logger.info(f\"Completed task {i+1}/{total_items}\")\n            except Exception as e:\n                logger.error(f\"Task {i+1}/{total_items} failed: {str(e)}\")\n                # Re-raise the exception to maintain the expected behavior\n                raise\n\n    return results\n</code></pre>"},{"location":"parallel/#performance-considerations","title":"Performance Considerations","text":"<p>When using parallel processing, consider the following to optimize performance:</p> <ol> <li>CPU Cores: The optimal <code>max_workers</code> is typically close to the number of available CPU cores</li> <li>Memory Usage: Each worker process requires memory, so limit <code>max_workers</code> on memory-constrained systems</li> <li>API Rate Limits: Always respect API rate limits by adjusting <code>delay_range</code> and <code>max_workers</code></li> <li>Task Granularity: Parallel processing works best when individual tasks take significant time</li> </ol>"},{"location":"parallel/#integration-with-course-generator","title":"Integration with Course Generator","text":"<p>The parallel module integrates seamlessly with the coursemaker module through the <code>create_course_parallel</code> function:</p> <pre><code>import geminiteacher as gt\n\n# Generate a course using parallel processing\ncourse = gt.create_course_parallel(\n    \"Your raw content here\",\n    max_workers=4,\n    delay_range=(0.2, 0.8),\n    max_retries=3,\n    course_title=\"Advanced_Topics\",\n    output_dir=\"output/courses\"\n)\n\nprint(f\"Generated {len(course.chapters)} chapters in parallel\")\n</code></pre>"},{"location":"parallel/#limitations","title":"Limitations","text":"<ul> <li>Increased memory usage compared to sequential processing</li> <li>Potential for higher API costs due to faster request rates</li> <li>Debugging can be more complex in parallel environments</li> </ul>"},{"location":"parallel/#future-enhancements","title":"Future Enhancements","text":"<p>Future versions may include: - Adaptive rate limiting based on API response times - Better telemetry for monitoring API usage - Support for concurrent.futures.ThreadPoolExecutor for I/O-bound tasks - Dynamic worker allocation based on system resources </p>"},{"location":"usage/","title":"Usage Guide","text":"<p>This guide provides a complete walkthrough for using GeminiTeacher, from installation to advanced usage.</p>"},{"location":"usage/#1-installation-setup","title":"1. Installation &amp; Setup","text":"<p>First, let's get the application installed and configured.</p>"},{"location":"usage/#install-the-package","title":"Install the Package","text":"<p>Install GeminiTeacher directly from the Python Package Index (PyPI):</p> <pre><code>pip install geminiteacher\n</code></pre>"},{"location":"usage/#set-up-your-google-api-key","title":"Set Up Your Google API Key","text":"<p>GeminiTeacher requires a Google API key with access to the Gemini family of models. You can provide this key in two ways:</p> <ol> <li>In the GUI: The graphical interface has a dedicated field for your API key.</li> <li>As an Environment Variable (for CLI/API use): This is the recommended way for command-line or programmatic use.</li> </ol> <pre><code># For Linux/macOS\nexport GOOGLE_API_KEY=\"your-api-key-here\"\n\n# For Windows (PowerShell)\n$env:GOOGLE_API_KEY=\"your-api-key-here\"\n</code></pre>"},{"location":"usage/#2-gui-usage-recommended","title":"2. GUI Usage (Recommended)","text":"<p>The graphical user interface is the easiest way to use GeminiTeacher.</p>"},{"location":"usage/#launching-the-gui","title":"Launching the GUI","text":"<p>After installation, run the following command in your terminal:</p> <pre><code>uv run geminiteacher-gui\n</code></pre>"},{"location":"usage/#using-the-interface","title":"Using the Interface","text":"<p>The GUI provides simple fields for all options. - API Key &amp; Model: Paste your API key and specify the Gemini model to use (defaults to <code>gemini-2.5-flash</code>). - File Paths: Use the \"Browse...\" buttons to select your input file and output directory. - Settings Caching: Your settings are automatically saved when you close the app and reloaded next time. - Real-time Logging: A log window shows you the real-time progress of the generation.</p>"},{"location":"usage/#3-command-line-cli-usage","title":"3. Command-Line (CLI) Usage","text":"<p>For power users and automation, the CLI provides full control over the generation process.</p>"},{"location":"usage/#method-1-using-a-configyaml-file-best-practice","title":"Method 1: Using a <code>config.yaml</code> File (Best Practice)","text":"<p>Create a <code>config.yaml</code> to define your settings, then run:</p> <pre><code>uv run python -m geminiteacher.app.generate_course --config config.yaml\n</code></pre> <p>Here is an example <code>config.yaml</code>:</p> <pre><code># --- Input/Output Settings ---\ninput:\n  path: \"input/content.txt\"\noutput:\n  directory: \"output/MyFirstCourse\"\n\n# --- Course Settings ---\ncourse:\n  title: \"Introduction to Artificial Intelligence\"\n\n# ... and so on for all other options.\n</code></pre>"},{"location":"usage/#method-2-using-command-line-flags","title":"Method 2: Using Command-Line Flags","text":"<p>Pass arguments directly for one-off tasks.</p> <pre><code># Example for a large, important document\nuv run python -m geminiteacher.app.generate_course \\\n  --input \"input/xianxingdaishu.md\" \\\n  --output-dir \"output/linear_algebra_course\" \\\n  --title \"Linear Algebra Fundamentals\" \\\n  --parallel \\\n  --max-workers 14 \\\n  --verbose\n</code></pre> <p>For a full list of commands and options, please refer to the Advanced CLI Options section below.</p>"},{"location":"usage/#4-python-api-usage","title":"4. Python API Usage","text":"<p>You can also use GeminiTeacher programmatically in your own Python scripts.</p> <pre><code>import geminiteacher as gt\n\n# Generate a course with parallel processing\ncourse = gt.create_course_parallel(\n    content=\"path/to/your/content.txt\",\n    course_title=\"My Programmatic Course\",\n    output_dir=\"courses_output\",\n    max_chapters=10,\n    max_workers=4\n)\n\nprint(f\"Generated {len(course.chapters)} chapters in parallel.\")\n</code></pre> Advanced CLI Options (Click to Expand)  ### All Command-Line Options  Here is the full list of available command-line options.  | Option                  | Alias | Type      | Description                                                                                                                              | | ----------------------- | ----- | --------- | ---------------------------------------------------------------------------------------------------------------------------------------- | | `--config`              | `-c`  | `PATH`    | Path to the YAML configuration file. If used, all other options can be defined here.                                                     | | `--input`               | `-i`  | `PATH`    | **Required.** Path to the input content file (e.g., `.txt`, `.md`).                                                                       | | `--output-dir`          | `-o`  | `PATH`    | **Required.** Directory where the generated course files will be saved.                                                                  | | `--title`               | `-t`  | `TEXT`    | **Required.** The title of your course.                                                                                                  | | `--custom-prompt`       | `-p`  | `PATH`    | Optional path to a file containing custom instructions for the AI to follow during generation.                                           | | `--model-name`          |       | `TEXT`    | The specific Gemini model to use (e.g., `gemini-1.5-pro`, `gemini-2.5-flash`). Default: `gemini-1.5-pro`.                                    | | `--temperature`         |       | `FLOAT`   | Controls the \"creativity\" or randomness of the AI's output. A value from `0.0` (most predictable) to `1.0` (most creative). Default: `0.2`. | | `--max-chapters`        |       | `INTEGER` | The target number of chapters for the course. The final number may be less if the AI deems it appropriate. Default: `10`.                | | `--fixed-chapter-count` |       | `FLAG`    | If set, forces the AI to generate exactly the number of chapters specified by `--max-chapters`.                                          | | `--parallel`            |       | `FLAG`    | If set, enables parallel processing to generate chapters simultaneously for a significant speed boost.                                   | | `--max-workers`         |       | `INTEGER` | When using `--parallel`, this sets the number of concurrent processes. Defaults to the number of CPU cores on your machine.              | | `--delay-min`           |       | `FLOAT`   | The minimum random delay (in seconds) between parallel API requests to avoid rate limiting. Default: `0.2`.                              | | `--delay-max`           |       | `FLOAT`   | The maximum random delay (in seconds) between parallel API requests. Default: `0.8`.                                                     | | `--max-retries`         |       | `INTEGER` | The maximum number of times to retry a failed API call for a chapter before giving up. Default: `3`.                                     | | `--verbose`             | `-v`  | `FLAG`    | Enable verbose output for detailed real-time progress logging, which is very helpful for debugging.                                    | | `--log-file`            |       | `PATH`    | Optional path to a file where all log output will be saved.                                                                              |"}]}