{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CascadeLLM","text":"<p>CascadeLLM is a toolkit for building advanced LLM chains and automated workflows. It contains modular components for chaining LLM prompts and transforming content through structured processing pipelines.</p>"},{"location":"#available-modules","title":"Available Modules","text":""},{"location":"#course-generator","title":"Course Generator","text":"<p>The <code>coursemaker</code> module transforms raw content into a structured, educational curriculum with the following steps:</p> <ol> <li>Table of Contents Generation: Automatically creates chapter titles based on content analysis</li> <li>Chapter Explanation Generation: Produces detailed, structured explanations for each chapter</li> <li>Course Summary: Creates a comprehensive summary of the entire course</li> </ol> <p>All outputs are in simplified Chinese, with well-structured formatting for each component.</p> <pre><code>from cascadellm.coursemaker import create_course\n\n# Generate a structured course from raw content\ncourse = create_course(\"Your raw content here\")\n\n# Access the generated course components\nprint(f\"Generated {len(course.chapters)} chapters\")\nfor chapter in course.chapters:\n    print(f\"- {chapter.title}\")\n</code></pre> <p>Learn more about the Course Generator</p>"},{"location":"#installation","title":"Installation","text":"<p>Clone the repository and install from source:</p> <pre><code>git clone https://github.com/supersheepbear/CascadeLLM.git\ncd CascadeLLM\npip install -e .\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.9+</li> <li>langchain &gt;= 0.1.0</li> <li>langchain-core &gt;= 0.1.0</li> <li>pydantic &gt;= 2.0.0</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! This repository is designed to be a collection of modular components for building LLM-powered applications. If you have ideas for new modules or improvements to existing ones, feel free to contribute.</p>"},{"location":"coursemaker/","title":"Course Generator","text":"<p>The <code>coursemaker</code> module provides tools for automatically generating structured educational courses from raw content. It's built on LangChain and designed for creating comprehensive learning materials with minimal effort.</p>"},{"location":"coursemaker/#overview","title":"Overview","text":"<p>The course generation process follows a four-phase pipeline:</p> <ol> <li>Table of Contents Generation: The system analyzes the input content and creates a logical structure with chapter titles.</li> <li>Prompt Template Creation: For each chapter, specialized prompts are created to guide the LLM.</li> <li>Chapter Generation: Detailed explanations are created for each chapter with a consistent structure.</li> <li>Summary Generation: A comprehensive course summary ties everything together.</li> </ol> <p>All generated content is in simplified Chinese and follows a structured format designed for educational purposes.</p>"},{"location":"coursemaker/#installation","title":"Installation","text":"<pre><code>git clone https://github.com/supersheepbear/CascadeLLM.git\ncd CascadeLLM\npip install -e .\n</code></pre> <p>This package requires the following dependencies: - langchain &gt;= 0.1.0 - langchain-core &gt;= 0.1.0 - pydantic &gt;= 2.0.0 - langchain-google-genai</p>"},{"location":"coursemaker/#setting-up-google-gemini-api","title":"Setting Up Google Gemini API","text":"<p>This module uses Google's Gemini API for all LLM operations:</p> <ol> <li>Install the required package:</li> </ol> <pre><code>pip install langchain-google-genai\n</code></pre> <ol> <li>Set up your Gemini API key as an environment variable:</li> </ol> <pre><code># On Linux/macOS\nexport GOOGLE_API_KEY=your_gemini_api_key_here\n\n# On Windows PowerShell\n$env:GOOGLE_API_KEY = \"your_gemini_api_key_here\"\n\n# On Windows Command Prompt\nset GOOGLE_API_KEY=your_gemini_api_key_here\n</code></pre>"},{"location":"coursemaker/#usage","title":"Usage","text":""},{"location":"coursemaker/#basic-usage","title":"Basic Usage","text":"<p>Generate a complete course with a single function call:</p> <pre><code>from cascadellm.coursemaker import create_course, configure_gemini_llm\n\n# Load content from a file or string\nwith open(\"your_content.txt\", \"r\") as f:\n    raw_content = f.read()\n\n# Configure the Gemini API\nllm = configure_gemini_llm()\n\n# Generate a structured course\ncourse = create_course(raw_content, llm=llm)\n\n# The course object contains all generated components\nprint(f\"Generated {len(course.chapters)} chapters\")\nprint(f\"Summary: {course.summary[:100]}...\")\n</code></pre>"},{"location":"coursemaker/#accessing-course-components","title":"Accessing Course Components","text":"<p>The course generator produces structured data that you can easily access:</p> <pre><code># Iterate through chapters\nfor i, chapter in enumerate(course.chapters):\n    print(f\"Chapter {i+1}: {chapter.title}\")\n    print(\"Summary:\", chapter.summary)\n    print(\"Explanation:\", chapter.explanation[:100] + \"...\")\n    print(\"Extension:\", chapter.extension[:100] + \"...\")\n    print(\"---\")\n\n# Access the course summary\nprint(\"Course Summary:\", course.summary)\n</code></pre>"},{"location":"coursemaker/#using-individual-components","title":"Using Individual Components","text":"<p>You can also use the individual components of the pipeline:</p> <pre><code>from cascadellm.coursemaker import generate_toc, generate_chapter, generate_summary, configure_gemini_llm\n\n# Configure the Gemini API\nllm = configure_gemini_llm()\n\n# Generate just the table of contents\nchapter_titles = generate_toc(raw_content, llm=llm)\nprint(f\"Generated {len(chapter_titles)} chapter titles\")\n\n# Generate a single chapter\nchapter = generate_chapter(\"Introduction to AI\", raw_content, llm=llm)\nprint(chapter.title)\nprint(chapter.summary)\n\n# Generate a summary from chapters\nchapters = [chapter]  # You would typically have multiple chapters\nsummary = generate_summary(raw_content, chapters, llm=llm)\nprint(summary)\n</code></pre>"},{"location":"coursemaker/#customizing-gemini-parameters","title":"Customizing Gemini Parameters","text":"<p>You can customize the Gemini API parameters:</p> <pre><code># Configure with custom parameters\nllm = configure_gemini_llm(\n    model_name=\"gemini-1.5-flash\",  # Use a different model\n    temperature=0.3                  # Adjust temperature for more varied outputs\n)\n\n# Use the customized model\ncourse = create_course(raw_content, llm=llm)\n</code></pre>"},{"location":"coursemaker/#api-reference","title":"API Reference","text":""},{"location":"coursemaker/#core-functions","title":"Core Functions","text":"<p>Create a complete structured course from raw content.</p> <p>This function orchestrates the entire course creation process: 1. Generate a table of contents 2. Create detailed explanations for each chapter 3. Generate a comprehensive course summary</p> <p>Generate a table of contents from raw content.</p> <p>This function takes raw text content and uses an LLM to generate a structured table of contents with 1-10 chapter titles.</p> <p>Generate a structured explanation for a single chapter.</p> <p>This function uses an LLM to create a detailed, structured explanation for a chapter based on its title and the original content.</p> <p>Generate a comprehensive summary for the entire course.</p> <p>Configure and return a Google Gemini model for use with the coursemaker module.</p>"},{"location":"coursemaker/#cascadellm.coursemaker.create_course--parameters","title":"Parameters","text":"<p>content : str     The raw content to transform into a course. llm : BaseLanguageModel, optional     The language model to use for generation. If None, the function will     automatically configure a Gemini model. temperature : float, optional     The temperature setting for the LLM, affecting randomness in output.     Default is 0.0 (deterministic output). verbose : bool, optional     Whether to print progress messages during course generation.     Default is False. max_chapters : int, optional     Maximum number of chapters to generate. Default is 10.</p>"},{"location":"coursemaker/#cascadellm.coursemaker.create_course--returns","title":"Returns","text":"<p>Course     A complete course object with all components.</p>"},{"location":"coursemaker/#cascadellm.coursemaker.create_course--examples","title":"Examples","text":"<p>course = create_course(\"Raw content about a topic...\") print(f\"Generated {len(course.chapters)} chapters\")</p> <p>course = create_course(\"Extended content...\", max_chapters=20) print(f\"Generated {len(course.chapters)} chapters\")</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def create_course(content: str, llm: Optional[BaseLanguageModel] = None, temperature: float = 0.0, verbose: bool = False, max_chapters: int = 10) -&gt; Course:\n    \"\"\"\n    Create a complete structured course from raw content.\n\n    This function orchestrates the entire course creation process:\n    1. Generate a table of contents\n    2. Create detailed explanations for each chapter\n    3. Generate a comprehensive course summary\n\n    Parameters\n    ----\n    content : str\n        The raw content to transform into a course.\n    llm : BaseLanguageModel, optional\n        The language model to use for generation. If None, the function will\n        automatically configure a Gemini model.\n    temperature : float, optional\n        The temperature setting for the LLM, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n    verbose : bool, optional\n        Whether to print progress messages during course generation.\n        Default is False.\n    max_chapters : int, optional\n        Maximum number of chapters to generate. Default is 10.\n\n    Returns\n    ----\n    Course\n        A complete course object with all components.\n\n    Examples\n    -----\n    &gt;&gt;&gt; course = create_course(\"Raw content about a topic...\")\n    &gt;&gt;&gt; print(f\"Generated {len(course.chapters)} chapters\")\n\n    &gt;&gt;&gt; course = create_course(\"Extended content...\", max_chapters=20)\n    &gt;&gt;&gt; print(f\"Generated {len(course.chapters)} chapters\")\n    \"\"\"\n    # Initialize the course with the original content\n    course = Course(content=content)\n\n    # If no LLM is provided, configure Gemini\n    if llm is None:\n        if verbose:\n            print(\"Configuring default Gemini LLM...\")\n        llm = get_default_llm(temperature)\n\n    # Step 1: Generate the table of contents\n    if verbose:\n        print(f\"Generating table of contents (max {max_chapters} chapters)...\")\n    chapter_titles = generate_toc(content, llm=llm, temperature=temperature, max_chapters=max_chapters)\n    if verbose:\n        print(f\"Generated {len(chapter_titles)} chapter titles\")\n\n    # Step 2: Generate content for each chapter\n    chapters = []\n    for i, title in enumerate(chapter_titles):\n        if verbose:\n            print(f\"Generating chapter {i+1}/{len(chapter_titles)}: {title}\")\n        chapter = generate_chapter(title, content, llm=llm, temperature=temperature)\n        chapters.append(chapter)\n\n    course.chapters = chapters\n\n    # Step 3: Generate the course summary\n    if chapters:\n        if verbose:\n            print(\"Generating course summary...\")\n        course.summary = generate_summary(content, chapters, llm=llm, temperature=temperature)\n        if verbose:\n            print(\"Course generation complete!\")\n\n    return course\n</code></pre>"},{"location":"coursemaker/#cascadellm.coursemaker.generate_toc--parameters","title":"Parameters","text":"<p>content : str     The raw text content to analyze and create a table of contents for. llm : BaseLanguageModel, optional     The language model to use for generation. If None, the function will      automatically configure a Gemini model. temperature : float, optional     The temperature setting for the LLM, affecting randomness in output.     Default is 0.0 (deterministic output). max_chapters : int, optional     Maximum number of chapters to generate. Default is 10.</p>"},{"location":"coursemaker/#cascadellm.coursemaker.generate_toc--returns","title":"Returns","text":"<p>List[str]     A list of chapter titles without numbering.</p>"},{"location":"coursemaker/#cascadellm.coursemaker.generate_toc--examples","title":"Examples","text":"<p>toc = generate_toc(\"This is a text about machine learning...\") print(toc) ['Introduction to Machine Learning', 'Types of Machine Learning', ...]</p> <p>toc = generate_toc(\"Content about history...\", max_chapters=20) print(len(toc)) 15  # The actual number may vary based on content</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def generate_toc(content: str, llm: Optional[BaseLanguageModel] = None, temperature: float = 0.0, max_chapters: int = 10) -&gt; List[str]:\n    \"\"\"\n    Generate a table of contents from raw content.\n\n    This function takes raw text content and uses an LLM to generate\n    a structured table of contents with 1-10 chapter titles.\n\n    Parameters\n    ----\n    content : str\n        The raw text content to analyze and create a table of contents for.\n    llm : BaseLanguageModel, optional\n        The language model to use for generation. If None, the function will \n        automatically configure a Gemini model.\n    temperature : float, optional\n        The temperature setting for the LLM, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n    max_chapters : int, optional\n        Maximum number of chapters to generate. Default is 10.\n\n    Returns\n    ----\n    List[str]\n        A list of chapter titles without numbering.\n\n    Examples\n    -----\n    &gt;&gt;&gt; toc = generate_toc(\"This is a text about machine learning...\")\n    &gt;&gt;&gt; print(toc)\n    ['Introduction to Machine Learning', 'Types of Machine Learning', ...]\n\n    &gt;&gt;&gt; toc = generate_toc(\"Content about history...\", max_chapters=20)\n    &gt;&gt;&gt; print(len(toc))\n    15  # The actual number may vary based on content\n    \"\"\"\n    # Create the prompt template\n    prompt = create_toc_prompt(max_chapters=max_chapters)\n\n    # If no LLM is provided, configure Gemini\n    if llm is None:\n        llm = get_default_llm(temperature)\n\n    # Create the LLM chain\n    chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n    )\n\n    # Invoke the chain with the content\n    result = chain.invoke({\n        \"content\": content,\n        \"max_chapters\": max_chapters\n    })\n\n    # Parse the result to extract the chapter titles\n    text = result.get(\"text\", \"\")\n\n    # Split the text by newlines and extract chapter titles\n    lines = text.strip().split(\"\\n\")\n    chapter_titles = []\n\n    for line in lines:\n        # Remove leading numbers, dots, and whitespace\n        line = line.strip()\n        # Match patterns like \"1. \", \"1) \", \"Chapter 1: \", etc.\n        import re\n        line = re.sub(r\"^(\\d+[\\.\\):]|Chapter\\s+\\d+:?)\\s*\", \"\", line)\n\n        if line:  # Skip empty lines\n            chapter_titles.append(line)\n\n    return chapter_titles\n</code></pre>"},{"location":"coursemaker/#cascadellm.coursemaker.generate_chapter--parameters","title":"Parameters","text":"<p>chapter_title : str     The title of the chapter to explain. content : str     The original raw content to base the explanation on. llm : BaseLanguageModel, optional     The language model to use for generation. If None, the function will      automatically configure a Gemini model. temperature : float, optional     The temperature setting for the LLM, affecting randomness in output.     Default is 0.0 (deterministic output).</p>"},{"location":"coursemaker/#cascadellm.coursemaker.generate_chapter--returns","title":"Returns","text":"<p>ChapterContent     A structured object containing the chapter's title, summary, explanation, and extension.</p>"},{"location":"coursemaker/#cascadellm.coursemaker.generate_chapter--examples","title":"Examples","text":"<p>chapter = generate_chapter(\"Machine Learning Basics\", \"Content about ML...\") print(chapter.summary) 'A brief introduction to the fundamental concepts of machine learning...'</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def generate_chapter(chapter_title: str, content: str, llm: Optional[BaseLanguageModel] = None, temperature: float = 0.0) -&gt; ChapterContent:\n    \"\"\"\n    Generate a structured explanation for a single chapter.\n\n    This function uses an LLM to create a detailed, structured explanation\n    for a chapter based on its title and the original content.\n\n    Parameters\n    ----\n    chapter_title : str\n        The title of the chapter to explain.\n    content : str\n        The original raw content to base the explanation on.\n    llm : BaseLanguageModel, optional\n        The language model to use for generation. If None, the function will \n        automatically configure a Gemini model.\n    temperature : float, optional\n        The temperature setting for the LLM, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n\n    Returns\n    ----\n    ChapterContent\n        A structured object containing the chapter's title, summary, explanation, and extension.\n\n    Examples\n    -----\n    &gt;&gt;&gt; chapter = generate_chapter(\"Machine Learning Basics\", \"Content about ML...\")\n    &gt;&gt;&gt; print(chapter.summary)\n    'A brief introduction to the fundamental concepts of machine learning...'\n    \"\"\"\n    # Create the prompt template\n    prompt = create_chapter_prompt_template()\n\n    # If no LLM is provided, configure Gemini\n    if llm is None:\n        llm = get_default_llm(temperature)\n\n    # Create the LLM chain\n    chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n    )\n\n    # Invoke the chain with the chapter title and content\n    result = chain.invoke({\n        \"chapter_title\": chapter_title,\n        \"content\": content,\n    })\n\n    # Parse the result to extract the structured content\n    try:\n        return parse_chapter_content(chapter_title, result.get(\"text\", \"\"))\n    except Exception as e:\n        # Handle parsing errors by creating a basic chapter with an error note\n        return ChapterContent(\n            title=chapter_title,\n            summary=f\"Error parsing chapter content: {str(e)}\",\n            explanation=\"The LLM response format was unexpected.\",\n            extension=\"Please check the LLM configuration and prompt template.\"\n        )\n</code></pre>"},{"location":"coursemaker/#cascadellm.coursemaker.generate_summary--parameters","title":"Parameters","text":"<p>content : str     The original raw content to base the summary on. chapters : List[ChapterContent]     The list of chapter content objects to include in the summary. llm : BaseLanguageModel, optional     The language model to use for generation. If None, the function will      automatically configure a Gemini model. temperature : float, optional     The temperature setting for the LLM, affecting randomness in output.     Default is 0.0 (deterministic output).</p>"},{"location":"coursemaker/#cascadellm.coursemaker.generate_summary--returns","title":"Returns","text":"<p>str     A comprehensive summary of the entire course.</p>"},{"location":"coursemaker/#cascadellm.coursemaker.generate_summary--examples","title":"Examples","text":"<p>chapters = [ChapterContent(title=\"Chapter 1\", summary=\"Summary 1\")] summary = generate_summary(\"Original content\", chapters) print(summary[:50]) 'This course covers the following key concepts...'</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def generate_summary(content: str, chapters: List[ChapterContent], llm: Optional[BaseLanguageModel] = None, temperature: float = 0.0) -&gt; str:\n    \"\"\"\n    Generate a comprehensive summary for the entire course.\n\n    Parameters\n    ----\n    content : str\n        The original raw content to base the summary on.\n    chapters : List[ChapterContent]\n        The list of chapter content objects to include in the summary.\n    llm : BaseLanguageModel, optional\n        The language model to use for generation. If None, the function will \n        automatically configure a Gemini model.\n    temperature : float, optional\n        The temperature setting for the LLM, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n\n    Returns\n    ----\n    str\n        A comprehensive summary of the entire course.\n\n    Examples\n    -----\n    &gt;&gt;&gt; chapters = [ChapterContent(title=\"Chapter 1\", summary=\"Summary 1\")]\n    &gt;&gt;&gt; summary = generate_summary(\"Original content\", chapters)\n    &gt;&gt;&gt; print(summary[:50])\n    'This course covers the following key concepts...'\n    \"\"\"\n    # Create the prompt template\n    prompt = create_summary_prompt_template()\n\n    # Prepare the chapter summaries\n    chapters_summary = \"\\n\\n\".join([\n        f\"\u7ae0\u8282 {i+1}: {chapter.title}\\n{chapter.summary}\"\n        for i, chapter in enumerate(chapters)\n    ])\n\n    # If no LLM is provided, configure Gemini\n    if llm is None:\n        llm = get_default_llm(temperature)\n\n    # Create the LLM chain\n    chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n    )\n\n    # Invoke the chain with the content and chapter summaries\n    result = chain.invoke({\n        \"content\": content,\n        \"chapters_summary\": chapters_summary,\n    })\n\n    return result.get(\"text\", \"\")\n</code></pre>"},{"location":"coursemaker/#cascadellm.coursemaker.configure_gemini_llm--parameters","title":"Parameters","text":"<p>api_key : str, optional     The Google API key for accessing Gemini models. If None, will use the      GOOGLE_API_KEY environment variable. model_name : str, optional     The name of the Gemini model to use. Default is \"gemini-1.5-pro\". temperature : float, optional     The temperature setting for generation, affecting randomness in output.     Default is 0.0 (deterministic output).</p>"},{"location":"coursemaker/#cascadellm.coursemaker.configure_gemini_llm--returns","title":"Returns","text":"<p>BaseLanguageModel     A configured Gemini language model ready to use with coursemaker functions.</p>"},{"location":"coursemaker/#cascadellm.coursemaker.configure_gemini_llm--examples","title":"Examples","text":"<p>llm = configure_gemini_llm(temperature=0.2) course = create_course(\"Content to transform...\", llm=llm)</p>"},{"location":"coursemaker/#cascadellm.coursemaker.configure_gemini_llm--notes","title":"Notes","text":"<p>This function requires the langchain-google-genai package to be installed: pip install langchain-google-genai</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def configure_gemini_llm(api_key: Optional[str] = None, model_name: str = \"gemini-1.5-pro\", temperature: float = 0.0) -&gt; BaseLanguageModel:\n    \"\"\"\n    Configure and return a Google Gemini model for use with the coursemaker module.\n\n    Parameters\n    ----\n    api_key : str, optional\n        The Google API key for accessing Gemini models. If None, will use the \n        GOOGLE_API_KEY environment variable.\n    model_name : str, optional\n        The name of the Gemini model to use. Default is \"gemini-1.5-pro\".\n    temperature : float, optional\n        The temperature setting for generation, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n\n    Returns\n    ----\n    BaseLanguageModel\n        A configured Gemini language model ready to use with coursemaker functions.\n\n    Examples\n    -----\n    &gt;&gt;&gt; llm = configure_gemini_llm(temperature=0.2)\n    &gt;&gt;&gt; course = create_course(\"Content to transform...\", llm=llm)\n\n    Notes\n    -----\n    This function requires the langchain-google-genai package to be installed:\n    pip install langchain-google-genai\n    \"\"\"\n    try:\n        from langchain_google_genai import ChatGoogleGenerativeAI\n    except ImportError:\n        raise ImportError(\n            \"The langchain-google-genai package is required to use Gemini models. \"\n            \"Please install it with: pip install langchain-google-genai\"\n        )\n\n    # Use the provided API key or get it from environment variables\n    if api_key is None:\n        import os\n        api_key = os.environ.get(\"GOOGLE_API_KEY\")\n        if not api_key:\n            raise ValueError(\n                \"No API key provided. Either pass an api_key parameter or \"\n                \"set the GOOGLE_API_KEY environment variable.\"\n            )\n\n    return ChatGoogleGenerativeAI(\n        model=model_name,\n        temperature=temperature,\n        google_api_key=api_key\n    )\n</code></pre>"},{"location":"coursemaker/#data-models","title":"Data Models","text":"<p>               Bases: <code>BaseModel</code></p> <p>A complete course with chapters and summary.</p> <p>               Bases: <code>BaseModel</code></p> <p>A structured representation of a chapter's content.</p>"},{"location":"coursemaker/#cascadellm.coursemaker.Course--attributes","title":"Attributes","text":"<p>content : str     The original raw content used to generate the course. chapters : List[ChapterContent]     The list of chapters in the course. summary : str     A comprehensive summary of the entire course.</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>class Course(BaseModel):\n    \"\"\"\n    A complete course with chapters and summary.\n\n    Attributes\n    ----\n    content : str\n        The original raw content used to generate the course.\n    chapters : List[ChapterContent]\n        The list of chapters in the course.\n    summary : str\n        A comprehensive summary of the entire course.\n    \"\"\"\n\n    content: str\n    chapters: List[ChapterContent] = []\n    summary: str = \"\"\n</code></pre>"},{"location":"coursemaker/#cascadellm.coursemaker.ChapterContent--attributes","title":"Attributes","text":"<p>title : str     The title of the chapter. summary : str     A brief summary or introduction to the chapter. explanation : str     The detailed explanation of the chapter's content. extension : str     Additional thoughts or extensions related to the chapter.</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>class ChapterContent(BaseModel):\n    \"\"\"\n    A structured representation of a chapter's content.\n\n    Attributes\n    ----\n    title : str\n        The title of the chapter.\n    summary : str\n        A brief summary or introduction to the chapter.\n    explanation : str\n        The detailed explanation of the chapter's content.\n    extension : str\n        Additional thoughts or extensions related to the chapter.\n    \"\"\"\n\n    title: str\n    summary: str = \"\"\n    explanation: str = \"\"\n    extension: str = \"\"\n</code></pre>"},{"location":"coursemaker/#helper-functions","title":"Helper Functions","text":"<p>Create the prompt template for TOC generation.</p> <p>Create a prompt template for generating chapter explanations.</p> <p>This template instructs the LLM to generate a structured explanation for a single chapter, based on its title and the original content.</p> <p>Create a prompt template for generating a course summary.</p> <p>This template instructs the LLM to generate a comprehensive summary of the entire course, based on the original content and chapter summaries.</p> <p>Parse the raw text output from the LLM into structured chapter content.</p> <p>This function extracts the summary, explanation, and extension sections from the LLM's response and organizes them into a ChapterContent object.</p>"},{"location":"coursemaker/#cascadellm.coursemaker.create_toc_prompt--parameters","title":"Parameters","text":"<p>max_chapters : int, optional     Maximum number of chapters to generate. Default is 10.</p>"},{"location":"coursemaker/#cascadellm.coursemaker.create_toc_prompt--returns","title":"Returns","text":"<p>ChatPromptTemplate     A prompt template for generating a table of contents.</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def create_toc_prompt(max_chapters: int = 10) -&gt; ChatPromptTemplate:\n    \"\"\"\n    Create the prompt template for TOC generation.\n\n    Parameters\n    ----\n    max_chapters : int, optional\n        Maximum number of chapters to generate. Default is 10.\n\n    Returns\n    ----\n    ChatPromptTemplate\n        A prompt template for generating a table of contents.\n    \"\"\"\n    return ChatPromptTemplate.from_template(\n        \"\"\"You are a professional educator creating a structured learning curriculum.\n\n        Below is raw content that needs to be organized into a meaningful table of contents.\n        Create a logical structure with 1-{max_chapters} chapter titles in simplified Chinese.\n        Format your response as a numbered list, with each chapter on a new line.\n\n        DO NOT include any explanations, introductions, or additional text.\n        ONLY include the numbered list of chapter titles.\n\n        Raw content:\n        {content}\n        \"\"\"\n    )\n</code></pre>"},{"location":"coursemaker/#cascadellm.coursemaker.create_chapter_prompt_template--returns","title":"Returns","text":"<p>ChatPromptTemplate     A prompt template for generating chapter explanations.</p>"},{"location":"coursemaker/#cascadellm.coursemaker.create_chapter_prompt_template--examples","title":"Examples","text":"<p>prompt = create_chapter_prompt_template() prompt.format(chapter_title=\"Introduction to AI\", content=\"...\")</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def create_chapter_prompt_template() -&gt; ChatPromptTemplate:\n    \"\"\"\n    Create a prompt template for generating chapter explanations.\n\n    This template instructs the LLM to generate a structured explanation\n    for a single chapter, based on its title and the original content.\n\n    Returns\n    ----\n    ChatPromptTemplate\n        A prompt template for generating chapter explanations.\n\n    Examples\n    -----\n    &gt;&gt;&gt; prompt = create_chapter_prompt_template()\n    &gt;&gt;&gt; prompt.format(chapter_title=\"Introduction to AI\", content=\"...\")\n    \"\"\"\n    return ChatPromptTemplate.from_template(\n        \"\"\"\u4f60\u662f\u4e00\u4f4d\u4e13\u4e1a\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u6b63\u5728\u521b\u5efa\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u5b66\u4e60\u8bfe\u7a0b\u3002\n\n        \u8bf7\u57fa\u4e8e\u4ee5\u4e0b\u5185\u5bb9\uff0c\u4e3a\u7ae0\u8282\u300a{chapter_title}\u300b\u521b\u5efa\u8be6\u7ec6\u7684\u8bb2\u89e3\u3002\n        \u4f60\u7684\u56de\u590d\u5fc5\u987b\u4f7f\u7528\u4ee5\u4e0b\u7ed3\u6784\uff1a\n\n        # \u6807\u9898\u4e0e\u6458\u8981\n        [\u6b64\u7ae0\u8282\u7684\u6807\u9898\uff0c\u4ee5\u53ca2-3\u53e5\u8bdd\u6982\u62ec\u4e3b\u8981\u5185\u5bb9]\n\n        # \u7cfb\u7edf\u6027\u8bb2\u89e3\n        [\u8be6\u7ec6\u89e3\u91ca\u672c\u7ae0\u8282\u7684\u6838\u5fc3\u6982\u5ff5\uff0c\u63d0\u4f9b\u6e05\u6670\u7684\u5b9a\u4e49\u3001\u793a\u4f8b\u548c\u5e94\u7528\u573a\u666f]\n\n        # \u62d3\u5c55\u601d\u8003\n        [\u63d0\u4f9b\u989d\u5916\u7684\u601d\u8003\u89d2\u5ea6\u3001\u5e94\u7528\u5efa\u8bae\u6216\u76f8\u5173\u9886\u57df\u7684\u8fde\u63a5]\n\n        \u8bf7\u786e\u4fdd\u4f60\u7684\u89e3\u91ca\u9488\u5bf9\u521d\u5b66\u8005\uff0c\u4f7f\u7528\u901a\u4fd7\u6613\u61c2\u7684\u8bed\u8a00\uff0c\u5e76\u4fdd\u6301\u903b\u8f91\u6e05\u6670\u3002\n\n        \u539f\u59cb\u5185\u5bb9:\n        {content}\n        \"\"\"\n    )\n</code></pre>"},{"location":"coursemaker/#cascadellm.coursemaker.create_summary_prompt_template--returns","title":"Returns","text":"<p>ChatPromptTemplate     A prompt template for generating course summaries.</p>"},{"location":"coursemaker/#cascadellm.coursemaker.create_summary_prompt_template--examples","title":"Examples","text":"<p>prompt = create_summary_prompt_template() prompt.format(content=\"...\", chapters_summary=\"...\")</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def create_summary_prompt_template() -&gt; ChatPromptTemplate:\n    \"\"\"\n    Create a prompt template for generating a course summary.\n\n    This template instructs the LLM to generate a comprehensive summary\n    of the entire course, based on the original content and chapter summaries.\n\n    Returns\n    ----\n    ChatPromptTemplate\n        A prompt template for generating course summaries.\n\n    Examples\n    -----\n    &gt;&gt;&gt; prompt = create_summary_prompt_template()\n    &gt;&gt;&gt; prompt.format(content=\"...\", chapters_summary=\"...\")\n    \"\"\"\n    return ChatPromptTemplate.from_template(\n        \"\"\"\u4f60\u662f\u4e00\u4f4d\u4e13\u4e1a\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u6b63\u5728\u4e3a\u4e00\u95e8\u8bfe\u7a0b\u521b\u5efa\u603b\u7ed3\u3002\n\n        \u8bf7\u57fa\u4e8e\u539f\u59cb\u5185\u5bb9\u548c\u5404\u7ae0\u8282\u7684\u6458\u8981\uff0c\u521b\u5efa\u4e00\u4e2a\u5168\u9762\u7684\u8bfe\u7a0b\u603b\u7ed3\u3002\n        \u603b\u7ed3\u5e94\u8be5\u6982\u62ec\u8bfe\u7a0b\u7684\u4e3b\u8981\u5185\u5bb9\u3001\u6838\u5fc3\u6982\u5ff5\u548c\u5b66\u4e60\u4ef7\u503c\u3002\n        \u4f7f\u7528\u7b80\u4f53\u4e2d\u6587\uff0c\u786e\u4fdd\u8bed\u8a00\u901a\u4fd7\u6613\u61c2\uff0c\u5e76\u7a81\u51fa\u8bfe\u7a0b\u7684\u5173\u952e\u8981\u70b9\u3002\n\n        \u539f\u59cb\u5185\u5bb9:\n        {content}\n\n        \u7ae0\u8282\u6458\u8981:\n        {chapters_summary}\n        \"\"\"\n    )\n</code></pre>"},{"location":"coursemaker/#cascadellm.coursemaker.parse_chapter_content--parameters","title":"Parameters","text":"<p>chapter_title : str     The title of the chapter. text : str     The raw text output from the LLM.</p>"},{"location":"coursemaker/#cascadellm.coursemaker.parse_chapter_content--returns","title":"Returns","text":"<p>ChapterContent     A structured object containing the chapter's components.</p>"},{"location":"coursemaker/#cascadellm.coursemaker.parse_chapter_content--raises","title":"Raises","text":"<p>ValueError     If the text cannot be parsed into the expected sections.</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def parse_chapter_content(chapter_title: str, text: str) -&gt; ChapterContent:\n    \"\"\"\n    Parse the raw text output from the LLM into structured chapter content.\n\n    This function extracts the summary, explanation, and extension sections\n    from the LLM's response and organizes them into a ChapterContent object.\n\n    Parameters\n    ----\n    chapter_title : str\n        The title of the chapter.\n    text : str\n        The raw text output from the LLM.\n\n    Returns\n    ----\n    ChapterContent\n        A structured object containing the chapter's components.\n\n    Raises\n    -----\n    ValueError\n        If the text cannot be parsed into the expected sections.\n    \"\"\"\n    # Initialize the chapter content with the title\n    chapter_content = ChapterContent(title=chapter_title)\n\n    # Define section markers\n    sections = {\n        \"summary\": [\"# \u6807\u9898\u4e0e\u6458\u8981\", \"# \u6458\u8981\", \"\u6807\u9898\u4e0e\u6458\u8981\"],\n        \"explanation\": [\"# \u7cfb\u7edf\u6027\u8bb2\u89e3\", \"# \u8bb2\u89e3\", \"\u7cfb\u7edf\u6027\u8bb2\u89e3\"],\n        \"extension\": [\"# \u62d3\u5c55\u601d\u8003\", \"# \u62d3\u5c55\", \"\u62d3\u5c55\u601d\u8003\"]\n    }\n\n    # Split the text by section markers\n    section_text = {}\n    lines = text.split(\"\\n\")\n    current_section = None\n\n    for line in lines:\n        stripped_line = line.strip()\n\n        # Check if this line is a section marker\n        new_section = None\n        for section, markers in sections.items():\n            if any(stripped_line.startswith(marker) for marker in markers):\n                new_section = section\n                break\n\n        if new_section:\n            current_section = new_section\n            section_text[current_section] = []\n        elif current_section:\n            section_text[current_section].append(line)\n\n    # Join the lines for each section and add to the chapter content\n    if \"summary\" in section_text and section_text[\"summary\"]:\n        chapter_content.summary = \"\\n\".join(section_text[\"summary\"]).strip()\n\n    if \"explanation\" in section_text and section_text[\"explanation\"]:\n        chapter_content.explanation = \"\\n\".join(section_text[\"explanation\"]).strip()\n\n    if \"extension\" in section_text and section_text[\"extension\"]:\n        chapter_content.extension = \"\\n\".join(section_text[\"extension\"]).strip()\n\n    return chapter_content \n</code></pre>"},{"location":"coursemaker/#limitations","title":"Limitations","text":"<ul> <li>The current implementation is designed for text content only.</li> <li>All output is in simplified Chinese.</li> <li>For production use, a valid Google API key must be provided.</li> </ul>"},{"location":"coursemaker/#future-enhancements","title":"Future Enhancements","text":"<p>Future versions may include: - Support for multiple languages - Image and multimedia content handling - Customizable output formats </p>"},{"location":"modules/","title":"Modules","text":""},{"location":"modules/#foo-module","title":"Foo Module","text":""},{"location":"modules/#cascadellm.foo.foo","title":"<code>foo(bar)</code>","text":"<p>Summary line.</p> <p>Extended description of function.</p> <p>Parameters:</p> Name Type Description Default <code>bar</code> <code>str</code> <p>Description of input argument.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Description of return value</p> Source code in <code>src\\cascadellm\\foo.py</code> <pre><code>def foo(bar: str) -&gt; str:\n    \"\"\"Summary line.\n\n    Extended description of function.\n\n    Args:\n        bar: Description of input argument.\n\n    Returns:\n        Description of return value\n    \"\"\"\n\n    return bar\n</code></pre>"},{"location":"modules/#course-generator-module","title":"Course Generator Module","text":"<p>The Course Generator module provides tools for automatically generating structured educational courses from raw content.</p> <p>This module contains the logic for the automated course generation system.</p>"},{"location":"modules/#cascadellm.coursemaker.ChapterContent","title":"<code>ChapterContent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A structured representation of a chapter's content.</p>"},{"location":"modules/#cascadellm.coursemaker.ChapterContent--attributes","title":"Attributes","text":"<p>title : str     The title of the chapter. summary : str     A brief summary or introduction to the chapter. explanation : str     The detailed explanation of the chapter's content. extension : str     Additional thoughts or extensions related to the chapter.</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>class ChapterContent(BaseModel):\n    \"\"\"\n    A structured representation of a chapter's content.\n\n    Attributes\n    ----\n    title : str\n        The title of the chapter.\n    summary : str\n        A brief summary or introduction to the chapter.\n    explanation : str\n        The detailed explanation of the chapter's content.\n    extension : str\n        Additional thoughts or extensions related to the chapter.\n    \"\"\"\n\n    title: str\n    summary: str = \"\"\n    explanation: str = \"\"\n    extension: str = \"\"\n</code></pre>"},{"location":"modules/#cascadellm.coursemaker.Course","title":"<code>Course</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A complete course with chapters and summary.</p>"},{"location":"modules/#cascadellm.coursemaker.Course--attributes","title":"Attributes","text":"<p>content : str     The original raw content used to generate the course. chapters : List[ChapterContent]     The list of chapters in the course. summary : str     A comprehensive summary of the entire course.</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>class Course(BaseModel):\n    \"\"\"\n    A complete course with chapters and summary.\n\n    Attributes\n    ----\n    content : str\n        The original raw content used to generate the course.\n    chapters : List[ChapterContent]\n        The list of chapters in the course.\n    summary : str\n        A comprehensive summary of the entire course.\n    \"\"\"\n\n    content: str\n    chapters: List[ChapterContent] = []\n    summary: str = \"\"\n</code></pre>"},{"location":"modules/#cascadellm.coursemaker.configure_gemini_llm","title":"<code>configure_gemini_llm(api_key=None, model_name='gemini-1.5-pro', temperature=0.0)</code>","text":"<p>Configure and return a Google Gemini model for use with the coursemaker module.</p>"},{"location":"modules/#cascadellm.coursemaker.configure_gemini_llm--parameters","title":"Parameters","text":"<p>api_key : str, optional     The Google API key for accessing Gemini models. If None, will use the      GOOGLE_API_KEY environment variable. model_name : str, optional     The name of the Gemini model to use. Default is \"gemini-1.5-pro\". temperature : float, optional     The temperature setting for generation, affecting randomness in output.     Default is 0.0 (deterministic output).</p>"},{"location":"modules/#cascadellm.coursemaker.configure_gemini_llm--returns","title":"Returns","text":"<p>BaseLanguageModel     A configured Gemini language model ready to use with coursemaker functions.</p>"},{"location":"modules/#cascadellm.coursemaker.configure_gemini_llm--examples","title":"Examples","text":"<p>llm = configure_gemini_llm(temperature=0.2) course = create_course(\"Content to transform...\", llm=llm)</p>"},{"location":"modules/#cascadellm.coursemaker.configure_gemini_llm--notes","title":"Notes","text":"<p>This function requires the langchain-google-genai package to be installed: pip install langchain-google-genai</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def configure_gemini_llm(api_key: Optional[str] = None, model_name: str = \"gemini-1.5-pro\", temperature: float = 0.0) -&gt; BaseLanguageModel:\n    \"\"\"\n    Configure and return a Google Gemini model for use with the coursemaker module.\n\n    Parameters\n    ----\n    api_key : str, optional\n        The Google API key for accessing Gemini models. If None, will use the \n        GOOGLE_API_KEY environment variable.\n    model_name : str, optional\n        The name of the Gemini model to use. Default is \"gemini-1.5-pro\".\n    temperature : float, optional\n        The temperature setting for generation, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n\n    Returns\n    ----\n    BaseLanguageModel\n        A configured Gemini language model ready to use with coursemaker functions.\n\n    Examples\n    -----\n    &gt;&gt;&gt; llm = configure_gemini_llm(temperature=0.2)\n    &gt;&gt;&gt; course = create_course(\"Content to transform...\", llm=llm)\n\n    Notes\n    -----\n    This function requires the langchain-google-genai package to be installed:\n    pip install langchain-google-genai\n    \"\"\"\n    try:\n        from langchain_google_genai import ChatGoogleGenerativeAI\n    except ImportError:\n        raise ImportError(\n            \"The langchain-google-genai package is required to use Gemini models. \"\n            \"Please install it with: pip install langchain-google-genai\"\n        )\n\n    # Use the provided API key or get it from environment variables\n    if api_key is None:\n        import os\n        api_key = os.environ.get(\"GOOGLE_API_KEY\")\n        if not api_key:\n            raise ValueError(\n                \"No API key provided. Either pass an api_key parameter or \"\n                \"set the GOOGLE_API_KEY environment variable.\"\n            )\n\n    return ChatGoogleGenerativeAI(\n        model=model_name,\n        temperature=temperature,\n        google_api_key=api_key\n    )\n</code></pre>"},{"location":"modules/#cascadellm.coursemaker.create_chapter_prompt_template","title":"<code>create_chapter_prompt_template()</code>","text":"<p>Create a prompt template for generating chapter explanations.</p> <p>This template instructs the LLM to generate a structured explanation for a single chapter, based on its title and the original content.</p>"},{"location":"modules/#cascadellm.coursemaker.create_chapter_prompt_template--returns","title":"Returns","text":"<p>ChatPromptTemplate     A prompt template for generating chapter explanations.</p>"},{"location":"modules/#cascadellm.coursemaker.create_chapter_prompt_template--examples","title":"Examples","text":"<p>prompt = create_chapter_prompt_template() prompt.format(chapter_title=\"Introduction to AI\", content=\"...\")</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def create_chapter_prompt_template() -&gt; ChatPromptTemplate:\n    \"\"\"\n    Create a prompt template for generating chapter explanations.\n\n    This template instructs the LLM to generate a structured explanation\n    for a single chapter, based on its title and the original content.\n\n    Returns\n    ----\n    ChatPromptTemplate\n        A prompt template for generating chapter explanations.\n\n    Examples\n    -----\n    &gt;&gt;&gt; prompt = create_chapter_prompt_template()\n    &gt;&gt;&gt; prompt.format(chapter_title=\"Introduction to AI\", content=\"...\")\n    \"\"\"\n    return ChatPromptTemplate.from_template(\n        \"\"\"\u4f60\u662f\u4e00\u4f4d\u4e13\u4e1a\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u6b63\u5728\u521b\u5efa\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u5b66\u4e60\u8bfe\u7a0b\u3002\n\n        \u8bf7\u57fa\u4e8e\u4ee5\u4e0b\u5185\u5bb9\uff0c\u4e3a\u7ae0\u8282\u300a{chapter_title}\u300b\u521b\u5efa\u8be6\u7ec6\u7684\u8bb2\u89e3\u3002\n        \u4f60\u7684\u56de\u590d\u5fc5\u987b\u4f7f\u7528\u4ee5\u4e0b\u7ed3\u6784\uff1a\n\n        # \u6807\u9898\u4e0e\u6458\u8981\n        [\u6b64\u7ae0\u8282\u7684\u6807\u9898\uff0c\u4ee5\u53ca2-3\u53e5\u8bdd\u6982\u62ec\u4e3b\u8981\u5185\u5bb9]\n\n        # \u7cfb\u7edf\u6027\u8bb2\u89e3\n        [\u8be6\u7ec6\u89e3\u91ca\u672c\u7ae0\u8282\u7684\u6838\u5fc3\u6982\u5ff5\uff0c\u63d0\u4f9b\u6e05\u6670\u7684\u5b9a\u4e49\u3001\u793a\u4f8b\u548c\u5e94\u7528\u573a\u666f]\n\n        # \u62d3\u5c55\u601d\u8003\n        [\u63d0\u4f9b\u989d\u5916\u7684\u601d\u8003\u89d2\u5ea6\u3001\u5e94\u7528\u5efa\u8bae\u6216\u76f8\u5173\u9886\u57df\u7684\u8fde\u63a5]\n\n        \u8bf7\u786e\u4fdd\u4f60\u7684\u89e3\u91ca\u9488\u5bf9\u521d\u5b66\u8005\uff0c\u4f7f\u7528\u901a\u4fd7\u6613\u61c2\u7684\u8bed\u8a00\uff0c\u5e76\u4fdd\u6301\u903b\u8f91\u6e05\u6670\u3002\n\n        \u539f\u59cb\u5185\u5bb9:\n        {content}\n        \"\"\"\n    )\n</code></pre>"},{"location":"modules/#cascadellm.coursemaker.create_course","title":"<code>create_course(content, llm=None, temperature=0.0, verbose=False, max_chapters=10)</code>","text":"<p>Create a complete structured course from raw content.</p> <p>This function orchestrates the entire course creation process: 1. Generate a table of contents 2. Create detailed explanations for each chapter 3. Generate a comprehensive course summary</p>"},{"location":"modules/#cascadellm.coursemaker.create_course--parameters","title":"Parameters","text":"<p>content : str     The raw content to transform into a course. llm : BaseLanguageModel, optional     The language model to use for generation. If None, the function will     automatically configure a Gemini model. temperature : float, optional     The temperature setting for the LLM, affecting randomness in output.     Default is 0.0 (deterministic output). verbose : bool, optional     Whether to print progress messages during course generation.     Default is False. max_chapters : int, optional     Maximum number of chapters to generate. Default is 10.</p>"},{"location":"modules/#cascadellm.coursemaker.create_course--returns","title":"Returns","text":"<p>Course     A complete course object with all components.</p>"},{"location":"modules/#cascadellm.coursemaker.create_course--examples","title":"Examples","text":"<p>course = create_course(\"Raw content about a topic...\") print(f\"Generated {len(course.chapters)} chapters\")</p> <p>course = create_course(\"Extended content...\", max_chapters=20) print(f\"Generated {len(course.chapters)} chapters\")</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def create_course(content: str, llm: Optional[BaseLanguageModel] = None, temperature: float = 0.0, verbose: bool = False, max_chapters: int = 10) -&gt; Course:\n    \"\"\"\n    Create a complete structured course from raw content.\n\n    This function orchestrates the entire course creation process:\n    1. Generate a table of contents\n    2. Create detailed explanations for each chapter\n    3. Generate a comprehensive course summary\n\n    Parameters\n    ----\n    content : str\n        The raw content to transform into a course.\n    llm : BaseLanguageModel, optional\n        The language model to use for generation. If None, the function will\n        automatically configure a Gemini model.\n    temperature : float, optional\n        The temperature setting for the LLM, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n    verbose : bool, optional\n        Whether to print progress messages during course generation.\n        Default is False.\n    max_chapters : int, optional\n        Maximum number of chapters to generate. Default is 10.\n\n    Returns\n    ----\n    Course\n        A complete course object with all components.\n\n    Examples\n    -----\n    &gt;&gt;&gt; course = create_course(\"Raw content about a topic...\")\n    &gt;&gt;&gt; print(f\"Generated {len(course.chapters)} chapters\")\n\n    &gt;&gt;&gt; course = create_course(\"Extended content...\", max_chapters=20)\n    &gt;&gt;&gt; print(f\"Generated {len(course.chapters)} chapters\")\n    \"\"\"\n    # Initialize the course with the original content\n    course = Course(content=content)\n\n    # If no LLM is provided, configure Gemini\n    if llm is None:\n        if verbose:\n            print(\"Configuring default Gemini LLM...\")\n        llm = get_default_llm(temperature)\n\n    # Step 1: Generate the table of contents\n    if verbose:\n        print(f\"Generating table of contents (max {max_chapters} chapters)...\")\n    chapter_titles = generate_toc(content, llm=llm, temperature=temperature, max_chapters=max_chapters)\n    if verbose:\n        print(f\"Generated {len(chapter_titles)} chapter titles\")\n\n    # Step 2: Generate content for each chapter\n    chapters = []\n    for i, title in enumerate(chapter_titles):\n        if verbose:\n            print(f\"Generating chapter {i+1}/{len(chapter_titles)}: {title}\")\n        chapter = generate_chapter(title, content, llm=llm, temperature=temperature)\n        chapters.append(chapter)\n\n    course.chapters = chapters\n\n    # Step 3: Generate the course summary\n    if chapters:\n        if verbose:\n            print(\"Generating course summary...\")\n        course.summary = generate_summary(content, chapters, llm=llm, temperature=temperature)\n        if verbose:\n            print(\"Course generation complete!\")\n\n    return course\n</code></pre>"},{"location":"modules/#cascadellm.coursemaker.create_summary_prompt_template","title":"<code>create_summary_prompt_template()</code>","text":"<p>Create a prompt template for generating a course summary.</p> <p>This template instructs the LLM to generate a comprehensive summary of the entire course, based on the original content and chapter summaries.</p>"},{"location":"modules/#cascadellm.coursemaker.create_summary_prompt_template--returns","title":"Returns","text":"<p>ChatPromptTemplate     A prompt template for generating course summaries.</p>"},{"location":"modules/#cascadellm.coursemaker.create_summary_prompt_template--examples","title":"Examples","text":"<p>prompt = create_summary_prompt_template() prompt.format(content=\"...\", chapters_summary=\"...\")</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def create_summary_prompt_template() -&gt; ChatPromptTemplate:\n    \"\"\"\n    Create a prompt template for generating a course summary.\n\n    This template instructs the LLM to generate a comprehensive summary\n    of the entire course, based on the original content and chapter summaries.\n\n    Returns\n    ----\n    ChatPromptTemplate\n        A prompt template for generating course summaries.\n\n    Examples\n    -----\n    &gt;&gt;&gt; prompt = create_summary_prompt_template()\n    &gt;&gt;&gt; prompt.format(content=\"...\", chapters_summary=\"...\")\n    \"\"\"\n    return ChatPromptTemplate.from_template(\n        \"\"\"\u4f60\u662f\u4e00\u4f4d\u4e13\u4e1a\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u6b63\u5728\u4e3a\u4e00\u95e8\u8bfe\u7a0b\u521b\u5efa\u603b\u7ed3\u3002\n\n        \u8bf7\u57fa\u4e8e\u539f\u59cb\u5185\u5bb9\u548c\u5404\u7ae0\u8282\u7684\u6458\u8981\uff0c\u521b\u5efa\u4e00\u4e2a\u5168\u9762\u7684\u8bfe\u7a0b\u603b\u7ed3\u3002\n        \u603b\u7ed3\u5e94\u8be5\u6982\u62ec\u8bfe\u7a0b\u7684\u4e3b\u8981\u5185\u5bb9\u3001\u6838\u5fc3\u6982\u5ff5\u548c\u5b66\u4e60\u4ef7\u503c\u3002\n        \u4f7f\u7528\u7b80\u4f53\u4e2d\u6587\uff0c\u786e\u4fdd\u8bed\u8a00\u901a\u4fd7\u6613\u61c2\uff0c\u5e76\u7a81\u51fa\u8bfe\u7a0b\u7684\u5173\u952e\u8981\u70b9\u3002\n\n        \u539f\u59cb\u5185\u5bb9:\n        {content}\n\n        \u7ae0\u8282\u6458\u8981:\n        {chapters_summary}\n        \"\"\"\n    )\n</code></pre>"},{"location":"modules/#cascadellm.coursemaker.create_toc_prompt","title":"<code>create_toc_prompt(max_chapters=10)</code>","text":"<p>Create the prompt template for TOC generation.</p>"},{"location":"modules/#cascadellm.coursemaker.create_toc_prompt--parameters","title":"Parameters","text":"<p>max_chapters : int, optional     Maximum number of chapters to generate. Default is 10.</p>"},{"location":"modules/#cascadellm.coursemaker.create_toc_prompt--returns","title":"Returns","text":"<p>ChatPromptTemplate     A prompt template for generating a table of contents.</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def create_toc_prompt(max_chapters: int = 10) -&gt; ChatPromptTemplate:\n    \"\"\"\n    Create the prompt template for TOC generation.\n\n    Parameters\n    ----\n    max_chapters : int, optional\n        Maximum number of chapters to generate. Default is 10.\n\n    Returns\n    ----\n    ChatPromptTemplate\n        A prompt template for generating a table of contents.\n    \"\"\"\n    return ChatPromptTemplate.from_template(\n        \"\"\"You are a professional educator creating a structured learning curriculum.\n\n        Below is raw content that needs to be organized into a meaningful table of contents.\n        Create a logical structure with 1-{max_chapters} chapter titles in simplified Chinese.\n        Format your response as a numbered list, with each chapter on a new line.\n\n        DO NOT include any explanations, introductions, or additional text.\n        ONLY include the numbered list of chapter titles.\n\n        Raw content:\n        {content}\n        \"\"\"\n    )\n</code></pre>"},{"location":"modules/#cascadellm.coursemaker.generate_chapter","title":"<code>generate_chapter(chapter_title, content, llm=None, temperature=0.0)</code>","text":"<p>Generate a structured explanation for a single chapter.</p> <p>This function uses an LLM to create a detailed, structured explanation for a chapter based on its title and the original content.</p>"},{"location":"modules/#cascadellm.coursemaker.generate_chapter--parameters","title":"Parameters","text":"<p>chapter_title : str     The title of the chapter to explain. content : str     The original raw content to base the explanation on. llm : BaseLanguageModel, optional     The language model to use for generation. If None, the function will      automatically configure a Gemini model. temperature : float, optional     The temperature setting for the LLM, affecting randomness in output.     Default is 0.0 (deterministic output).</p>"},{"location":"modules/#cascadellm.coursemaker.generate_chapter--returns","title":"Returns","text":"<p>ChapterContent     A structured object containing the chapter's title, summary, explanation, and extension.</p>"},{"location":"modules/#cascadellm.coursemaker.generate_chapter--examples","title":"Examples","text":"<p>chapter = generate_chapter(\"Machine Learning Basics\", \"Content about ML...\") print(chapter.summary) 'A brief introduction to the fundamental concepts of machine learning...'</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def generate_chapter(chapter_title: str, content: str, llm: Optional[BaseLanguageModel] = None, temperature: float = 0.0) -&gt; ChapterContent:\n    \"\"\"\n    Generate a structured explanation for a single chapter.\n\n    This function uses an LLM to create a detailed, structured explanation\n    for a chapter based on its title and the original content.\n\n    Parameters\n    ----\n    chapter_title : str\n        The title of the chapter to explain.\n    content : str\n        The original raw content to base the explanation on.\n    llm : BaseLanguageModel, optional\n        The language model to use for generation. If None, the function will \n        automatically configure a Gemini model.\n    temperature : float, optional\n        The temperature setting for the LLM, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n\n    Returns\n    ----\n    ChapterContent\n        A structured object containing the chapter's title, summary, explanation, and extension.\n\n    Examples\n    -----\n    &gt;&gt;&gt; chapter = generate_chapter(\"Machine Learning Basics\", \"Content about ML...\")\n    &gt;&gt;&gt; print(chapter.summary)\n    'A brief introduction to the fundamental concepts of machine learning...'\n    \"\"\"\n    # Create the prompt template\n    prompt = create_chapter_prompt_template()\n\n    # If no LLM is provided, configure Gemini\n    if llm is None:\n        llm = get_default_llm(temperature)\n\n    # Create the LLM chain\n    chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n    )\n\n    # Invoke the chain with the chapter title and content\n    result = chain.invoke({\n        \"chapter_title\": chapter_title,\n        \"content\": content,\n    })\n\n    # Parse the result to extract the structured content\n    try:\n        return parse_chapter_content(chapter_title, result.get(\"text\", \"\"))\n    except Exception as e:\n        # Handle parsing errors by creating a basic chapter with an error note\n        return ChapterContent(\n            title=chapter_title,\n            summary=f\"Error parsing chapter content: {str(e)}\",\n            explanation=\"The LLM response format was unexpected.\",\n            extension=\"Please check the LLM configuration and prompt template.\"\n        )\n</code></pre>"},{"location":"modules/#cascadellm.coursemaker.generate_summary","title":"<code>generate_summary(content, chapters, llm=None, temperature=0.0)</code>","text":"<p>Generate a comprehensive summary for the entire course.</p>"},{"location":"modules/#cascadellm.coursemaker.generate_summary--parameters","title":"Parameters","text":"<p>content : str     The original raw content to base the summary on. chapters : List[ChapterContent]     The list of chapter content objects to include in the summary. llm : BaseLanguageModel, optional     The language model to use for generation. If None, the function will      automatically configure a Gemini model. temperature : float, optional     The temperature setting for the LLM, affecting randomness in output.     Default is 0.0 (deterministic output).</p>"},{"location":"modules/#cascadellm.coursemaker.generate_summary--returns","title":"Returns","text":"<p>str     A comprehensive summary of the entire course.</p>"},{"location":"modules/#cascadellm.coursemaker.generate_summary--examples","title":"Examples","text":"<p>chapters = [ChapterContent(title=\"Chapter 1\", summary=\"Summary 1\")] summary = generate_summary(\"Original content\", chapters) print(summary[:50]) 'This course covers the following key concepts...'</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def generate_summary(content: str, chapters: List[ChapterContent], llm: Optional[BaseLanguageModel] = None, temperature: float = 0.0) -&gt; str:\n    \"\"\"\n    Generate a comprehensive summary for the entire course.\n\n    Parameters\n    ----\n    content : str\n        The original raw content to base the summary on.\n    chapters : List[ChapterContent]\n        The list of chapter content objects to include in the summary.\n    llm : BaseLanguageModel, optional\n        The language model to use for generation. If None, the function will \n        automatically configure a Gemini model.\n    temperature : float, optional\n        The temperature setting for the LLM, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n\n    Returns\n    ----\n    str\n        A comprehensive summary of the entire course.\n\n    Examples\n    -----\n    &gt;&gt;&gt; chapters = [ChapterContent(title=\"Chapter 1\", summary=\"Summary 1\")]\n    &gt;&gt;&gt; summary = generate_summary(\"Original content\", chapters)\n    &gt;&gt;&gt; print(summary[:50])\n    'This course covers the following key concepts...'\n    \"\"\"\n    # Create the prompt template\n    prompt = create_summary_prompt_template()\n\n    # Prepare the chapter summaries\n    chapters_summary = \"\\n\\n\".join([\n        f\"\u7ae0\u8282 {i+1}: {chapter.title}\\n{chapter.summary}\"\n        for i, chapter in enumerate(chapters)\n    ])\n\n    # If no LLM is provided, configure Gemini\n    if llm is None:\n        llm = get_default_llm(temperature)\n\n    # Create the LLM chain\n    chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n    )\n\n    # Invoke the chain with the content and chapter summaries\n    result = chain.invoke({\n        \"content\": content,\n        \"chapters_summary\": chapters_summary,\n    })\n\n    return result.get(\"text\", \"\")\n</code></pre>"},{"location":"modules/#cascadellm.coursemaker.generate_toc","title":"<code>generate_toc(content, llm=None, temperature=0.0, max_chapters=10)</code>","text":"<p>Generate a table of contents from raw content.</p> <p>This function takes raw text content and uses an LLM to generate a structured table of contents with 1-10 chapter titles.</p>"},{"location":"modules/#cascadellm.coursemaker.generate_toc--parameters","title":"Parameters","text":"<p>content : str     The raw text content to analyze and create a table of contents for. llm : BaseLanguageModel, optional     The language model to use for generation. If None, the function will      automatically configure a Gemini model. temperature : float, optional     The temperature setting for the LLM, affecting randomness in output.     Default is 0.0 (deterministic output). max_chapters : int, optional     Maximum number of chapters to generate. Default is 10.</p>"},{"location":"modules/#cascadellm.coursemaker.generate_toc--returns","title":"Returns","text":"<p>List[str]     A list of chapter titles without numbering.</p>"},{"location":"modules/#cascadellm.coursemaker.generate_toc--examples","title":"Examples","text":"<p>toc = generate_toc(\"This is a text about machine learning...\") print(toc) ['Introduction to Machine Learning', 'Types of Machine Learning', ...]</p> <p>toc = generate_toc(\"Content about history...\", max_chapters=20) print(len(toc)) 15  # The actual number may vary based on content</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def generate_toc(content: str, llm: Optional[BaseLanguageModel] = None, temperature: float = 0.0, max_chapters: int = 10) -&gt; List[str]:\n    \"\"\"\n    Generate a table of contents from raw content.\n\n    This function takes raw text content and uses an LLM to generate\n    a structured table of contents with 1-10 chapter titles.\n\n    Parameters\n    ----\n    content : str\n        The raw text content to analyze and create a table of contents for.\n    llm : BaseLanguageModel, optional\n        The language model to use for generation. If None, the function will \n        automatically configure a Gemini model.\n    temperature : float, optional\n        The temperature setting for the LLM, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n    max_chapters : int, optional\n        Maximum number of chapters to generate. Default is 10.\n\n    Returns\n    ----\n    List[str]\n        A list of chapter titles without numbering.\n\n    Examples\n    -----\n    &gt;&gt;&gt; toc = generate_toc(\"This is a text about machine learning...\")\n    &gt;&gt;&gt; print(toc)\n    ['Introduction to Machine Learning', 'Types of Machine Learning', ...]\n\n    &gt;&gt;&gt; toc = generate_toc(\"Content about history...\", max_chapters=20)\n    &gt;&gt;&gt; print(len(toc))\n    15  # The actual number may vary based on content\n    \"\"\"\n    # Create the prompt template\n    prompt = create_toc_prompt(max_chapters=max_chapters)\n\n    # If no LLM is provided, configure Gemini\n    if llm is None:\n        llm = get_default_llm(temperature)\n\n    # Create the LLM chain\n    chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n    )\n\n    # Invoke the chain with the content\n    result = chain.invoke({\n        \"content\": content,\n        \"max_chapters\": max_chapters\n    })\n\n    # Parse the result to extract the chapter titles\n    text = result.get(\"text\", \"\")\n\n    # Split the text by newlines and extract chapter titles\n    lines = text.strip().split(\"\\n\")\n    chapter_titles = []\n\n    for line in lines:\n        # Remove leading numbers, dots, and whitespace\n        line = line.strip()\n        # Match patterns like \"1. \", \"1) \", \"Chapter 1: \", etc.\n        import re\n        line = re.sub(r\"^(\\d+[\\.\\):]|Chapter\\s+\\d+:?)\\s*\", \"\", line)\n\n        if line:  # Skip empty lines\n            chapter_titles.append(line)\n\n    return chapter_titles\n</code></pre>"},{"location":"modules/#cascadellm.coursemaker.get_default_llm","title":"<code>get_default_llm(temperature=0.0)</code>","text":"<p>Get the default LLM for this module (Google Gemini).</p> <p>This is a helper function to automatically configure Gemini when no explicit LLM is provided to the generation functions.</p>"},{"location":"modules/#cascadellm.coursemaker.get_default_llm--parameters","title":"Parameters","text":"<p>temperature : float, optional     The temperature setting for the LLM, affecting randomness in output.     Default is 0.0 (deterministic output).</p>"},{"location":"modules/#cascadellm.coursemaker.get_default_llm--returns","title":"Returns","text":"<p>BaseLanguageModel     A configured Gemini language model.</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def get_default_llm(temperature: float = 0.0) -&gt; BaseLanguageModel:\n    \"\"\"\n    Get the default LLM for this module (Google Gemini).\n\n    This is a helper function to automatically configure Gemini\n    when no explicit LLM is provided to the generation functions.\n\n    Parameters\n    ----\n    temperature : float, optional\n        The temperature setting for the LLM, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n\n    Returns\n    ----\n    BaseLanguageModel\n        A configured Gemini language model.\n    \"\"\"\n    try:\n        return configure_gemini_llm(temperature=temperature)\n    except ImportError as e:\n        # In test environments, we'll return None to allow mocking\n        import sys\n        if 'pytest' in sys.modules:\n            return None\n        # In production, raise the error\n        raise e\n</code></pre>"},{"location":"modules/#cascadellm.coursemaker.parse_chapter_content","title":"<code>parse_chapter_content(chapter_title, text)</code>","text":"<p>Parse the raw text output from the LLM into structured chapter content.</p> <p>This function extracts the summary, explanation, and extension sections from the LLM's response and organizes them into a ChapterContent object.</p>"},{"location":"modules/#cascadellm.coursemaker.parse_chapter_content--parameters","title":"Parameters","text":"<p>chapter_title : str     The title of the chapter. text : str     The raw text output from the LLM.</p>"},{"location":"modules/#cascadellm.coursemaker.parse_chapter_content--returns","title":"Returns","text":"<p>ChapterContent     A structured object containing the chapter's components.</p>"},{"location":"modules/#cascadellm.coursemaker.parse_chapter_content--raises","title":"Raises","text":"<p>ValueError     If the text cannot be parsed into the expected sections.</p> Source code in <code>src\\cascadellm\\coursemaker.py</code> <pre><code>def parse_chapter_content(chapter_title: str, text: str) -&gt; ChapterContent:\n    \"\"\"\n    Parse the raw text output from the LLM into structured chapter content.\n\n    This function extracts the summary, explanation, and extension sections\n    from the LLM's response and organizes them into a ChapterContent object.\n\n    Parameters\n    ----\n    chapter_title : str\n        The title of the chapter.\n    text : str\n        The raw text output from the LLM.\n\n    Returns\n    ----\n    ChapterContent\n        A structured object containing the chapter's components.\n\n    Raises\n    -----\n    ValueError\n        If the text cannot be parsed into the expected sections.\n    \"\"\"\n    # Initialize the chapter content with the title\n    chapter_content = ChapterContent(title=chapter_title)\n\n    # Define section markers\n    sections = {\n        \"summary\": [\"# \u6807\u9898\u4e0e\u6458\u8981\", \"# \u6458\u8981\", \"\u6807\u9898\u4e0e\u6458\u8981\"],\n        \"explanation\": [\"# \u7cfb\u7edf\u6027\u8bb2\u89e3\", \"# \u8bb2\u89e3\", \"\u7cfb\u7edf\u6027\u8bb2\u89e3\"],\n        \"extension\": [\"# \u62d3\u5c55\u601d\u8003\", \"# \u62d3\u5c55\", \"\u62d3\u5c55\u601d\u8003\"]\n    }\n\n    # Split the text by section markers\n    section_text = {}\n    lines = text.split(\"\\n\")\n    current_section = None\n\n    for line in lines:\n        stripped_line = line.strip()\n\n        # Check if this line is a section marker\n        new_section = None\n        for section, markers in sections.items():\n            if any(stripped_line.startswith(marker) for marker in markers):\n                new_section = section\n                break\n\n        if new_section:\n            current_section = new_section\n            section_text[current_section] = []\n        elif current_section:\n            section_text[current_section].append(line)\n\n    # Join the lines for each section and add to the chapter content\n    if \"summary\" in section_text and section_text[\"summary\"]:\n        chapter_content.summary = \"\\n\".join(section_text[\"summary\"]).strip()\n\n    if \"explanation\" in section_text and section_text[\"explanation\"]:\n        chapter_content.explanation = \"\\n\".join(section_text[\"explanation\"]).strip()\n\n    if \"extension\" in section_text and section_text[\"extension\"]:\n        chapter_content.extension = \"\\n\".join(section_text[\"extension\"]).strip()\n\n    return chapter_content \n</code></pre>"}]}