{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GeminiTeacher","text":"<p>GeminiTeacher is an educational content generation toolkit powered by Google's Gemini LLM. It transforms raw text content into structured, well-organized educational courses with minimal effort. Perfect for educators, content creators, and anyone looking to quickly create high-quality learning materials.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install GeminiTeacher directly from PyPI:</p> <pre><code>pip install geminiteacher\n</code></pre> <p>You'll need a Google API key with access to Gemini models. Set it as an environment variable:</p> <pre><code># For Linux/macOS\nexport GOOGLE_API_KEY=\"your-api-key-here\"\n\n# For Windows (Command Prompt)\nset GOOGLE_API_KEY=your-api-key-here\n\n# For Windows (PowerShell)\n$env:GOOGLE_API_KEY=\"your-api-key-here\"\n</code></pre>"},{"location":"#command-line-usage","title":"Command-line Usage","text":"<p>After installation, you can immediately use the <code>geminiteacher</code> command-line tool without writing any code:</p> <pre><code># Basic usage with a text file\ngeminiteacher --input content.txt --output-dir courses --title \"Machine Learning Basics\"\n\n# Enable parallel processing for faster generation\ngeminiteacher --input content.txt --parallel --max-workers 4\n\n# Use a configuration file for more control\ngeminiteacher --config config.yaml\n</code></pre> <p>For detailed command-line options and examples, see the CLI Application documentation.</p>"},{"location":"#python-api-usage","title":"Python API Usage","text":"<p>Generate a complete course with just a few lines of code:</p> <pre><code>import geminiteacher as gt\n\n# Your raw content to transform into a course\ncontent = \"\"\"\nMachine learning is a subfield of artificial intelligence that focuses on developing \nsystems that can learn from and make decisions based on data. Unlike traditional \nprogramming where explicit instructions are provided, machine learning algorithms \nbuild models based on sample data to make predictions or decisions without being \nexplicitly programmed to do so.\n\"\"\"\n\n# Generate a course with parallel processing for speed\ncourse = gt.create_course_parallel(\n    content=content,\n    max_chapters=5,          # Maximum number of chapters to generate\n    fixed_chapter_count=True,  # Generate exactly 5 chapters\n    temperature=0.2,         # Control creativity (0.0-1.0)\n    verbose=True             # Show progress messages\n)\n\n# Print the generated course structure\nprint(f\"Course Summary: {course.summary}\\n\")\nprint(f\"Generated {len(course.chapters)} chapters:\")\nfor i, chapter in enumerate(course.chapters):\n    print(f\"  {i+1}. {chapter.title}\")\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>For detailed usage instructions and API documentation, check out the following sections:</p> <ul> <li>Usage Guide - Detailed guide on using the course generator</li> <li>CLI Application - Command-line interface documentation</li> <li>Modules Overview - Overview of the package structure</li> <li>Parallel Processing - Advanced parallel processing features</li> </ul>"},{"location":"app/","title":"Command-line Application","text":"<p>GeminiTeacher includes a powerful command-line application that makes it easy to generate courses without writing any code.</p>"},{"location":"app/#installation","title":"Installation","text":"<p>The command-line application is installed automatically when you install the GeminiTeacher package:</p> <pre><code>pip install geminiteacher\n</code></pre> <p>After installation, the <code>geminiteacher</code> command should be available in your terminal. If the command is not found, make sure your Python scripts directory is in your PATH.</p>"},{"location":"app/#verifying-installation","title":"Verifying Installation","text":"<p>To verify that the command-line tool is properly installed:</p> <pre><code># Check if the command is available\ngeminiteacher --help\n\n# If the command is not found, you can run it using Python's module syntax:\npython -m geminiteacher.app.generate_course --help\n</code></pre>"},{"location":"app/#setting-up-your-api-key","title":"Setting Up Your API Key","text":"<p>Before using the tool, you must set up your Google API key:</p> <pre><code># For Linux/macOS\nexport GOOGLE_API_KEY=your_api_key_here\n\n# For Windows (Command Prompt)\nset GOOGLE_API_KEY=your_api_key_here\n\n# For Windows (PowerShell)\n$env:GOOGLE_API_KEY=\"your_api_key_here\"\n</code></pre> <p>Alternatively, you can specify the API key in your configuration file.</p>"},{"location":"app/#basic-usage","title":"Basic Usage","text":""},{"location":"app/#quick-start","title":"Quick Start","text":"<p>Generate a course with minimal configuration:</p> <pre><code># Generate a course from a text file\ngeminiteacher --input content.txt --title \"Introduction to Python\" --output-dir courses\n\n# Alternative syntax if the command is not in your PATH\npython -m geminiteacher.app.generate_course --input content.txt --title \"Introduction to Python\" --output-dir courses\n</code></pre>"},{"location":"app/#using-a-configuration-file","title":"Using a Configuration File","text":"<p>For more control, create a <code>config.yaml</code> file:</p> <pre><code># API Configuration\napi:\n  google_api_key: \"your_gemini_api_key_here\"\n  model_name: \"gemini-1.5-pro\"\n\n# Input/Output Settings\ninput:\n  path: \"input/content.txt\"\n\noutput:\n  directory: \"output\"\n\n# Course Settings\ncourse:\n  title: \"My Course\"\n  custom_prompt: \"custom_instructions.txt\"\n\n# Generation Settings\ngeneration:\n  temperature: 0.2\n  max_chapters: 10\n  fixed_chapter_count: false\n\n# Parallel Processing Settings\nparallel:\n  enabled: true\n  max_workers: 4\n  delay_min: 0.2\n  delay_max: 0.8\n  max_retries: 3\n</code></pre> <p>Then run:</p> <pre><code>geminiteacher --config config.yaml\n\n# Alternative syntax\npython -m geminiteacher.app.generate_course --config config.yaml\n</code></pre>"},{"location":"app/#command-line-options","title":"Command-line Options","text":"<p>The application supports the following command-line options:</p> Option Description <code>--config</code>, <code>-c</code> Path to configuration file <code>--input</code>, <code>-i</code> Path to input content file <code>--output-dir</code>, <code>-o</code> Directory to save generated course files <code>--title</code>, <code>-t</code> Course title <code>--custom-prompt</code>, <code>-p</code> Path to custom prompt instructions file <code>--temperature</code> Temperature for generation (0.0-1.0) <code>--max-chapters</code> Maximum number of chapters <code>--fixed-chapter-count</code> Generate exactly max-chapters chapters <code>--parallel</code> Use parallel processing for chapter generation <code>--max-workers</code> Maximum number of worker processes for parallel generation <code>--verbose</code>, <code>-v</code> Enable verbose output <code>--log-file</code> Path to log file"},{"location":"app/#custom-prompts","title":"Custom Prompts","text":"<p>You can customize the generation process by providing a custom prompt file. This file should contain instructions for the LLM about how to generate the course content.</p> <p>Example custom prompt file:</p> <pre><code>Focus on practical examples and include code snippets where relevant.\nEach concept should be explained with a real-world application.\nInclude exercises at the end of each explanation section.\n</code></pre> <p>Use it with:</p> <pre><code>geminiteacher --input content.txt --custom-prompt custom_instructions.txt\n</code></pre>"},{"location":"app/#output-structure","title":"Output Structure","text":"<p>The application generates the following files in the output directory:</p> <ul> <li><code>{course_title}_chapter_{nn}_{chapter_title}.md</code> - Individual chapter files</li> <li><code>{course_title}_summary.md</code> - Course summary file</li> </ul> <p>Each chapter file contains: - Title - Summary - Detailed explanation - Extension thoughts</p>"},{"location":"app/#troubleshooting","title":"Troubleshooting","text":""},{"location":"app/#command-not-found","title":"Command Not Found","text":"<p>If you encounter a \"command not found\" error:</p> <ol> <li> <p>Make sure you've installed the package correctly:    <code>bash    pip install --upgrade geminiteacher</code></p> </li> <li> <p>Use the Python module syntax instead:    <code>bash    python -m geminiteacher.app.generate_course --input content.txt</code></p> </li> <li> <p>Check if your Python scripts directory is in your PATH:    <code>bash    # Find your Python scripts directory    python -c \"import site; print(site.USER_BASE + '/bin')\"  # Linux/macOS    python -c \"import site; print(site.USER_BASE + '\\\\Scripts')\"  # Windows</code></p> </li> </ol>"},{"location":"app/#api-key-issues","title":"API Key Issues","text":"<p>If you encounter authentication errors:</p> <ol> <li>Verify your API key is set correctly:    ```bash    # Linux/macOS    echo $GOOGLE_API_KEY</li> </ol> <p># Windows (Command Prompt)    echo %GOOGLE_API_KEY%</p> <p># Windows (PowerShell)    echo $env:GOOGLE_API_KEY    ```</p> <ol> <li> <p>Ensure your API key has access to Gemini models.</p> </li> <li> <p>Try specifying the API key directly in your config.yaml file.</p> </li> </ol>"},{"location":"app/#programmatic-usage","title":"Programmatic Usage","text":"<p>You can also use the app module programmatically in your Python code:</p> <pre><code>from geminiteacher.app import create_course_with_progressive_save, configure_logging\n\n# Configure logging\nlogger = configure_logging(log_file=\"generation.log\", verbose=True)\n\n# Generate a course\ncourse = create_course_with_progressive_save(\n    content=\"Your content here...\",\n    course_title=\"Python Programming\",\n    output_dir=\"courses\",\n    temperature=0.2,\n    max_chapters=5,\n    fixed_chapter_count=True,\n    custom_prompt=\"Focus on practical examples\",\n    verbose=True,\n    logger=logger\n)\n\n# Access the generated course\nprint(f\"Generated {len(course.chapters)} chapters\")\nfor i, chapter in enumerate(course.chapters):\n    print(f\"Chapter {i+1}: {chapter.title}\")\n</code></pre>"},{"location":"modules/","title":"GeminiTeacher Modules","text":"<p>GeminiTeacher consists of several modules that work together to provide a complete course generation solution.</p>"},{"location":"modules/#core-modules","title":"Core Modules","text":""},{"location":"modules/#coursegenerator-geminiteachercoursemaker","title":"CourseGenerator (<code>geminiteacher.coursemaker</code>)","text":"<p>The main module responsible for generating course content.</p> <pre><code>from geminiteacher import create_course, configure_gemini_llm\n\n# Configure the language model\nllm = configure_gemini_llm(temperature=0.2)\n\n# Generate a course\ncourse = create_course(\n    content=\"Your content here...\",\n    llm=llm,\n    max_chapters=5\n)\n\n# Access the generated course\nprint(f\"Course summary: {course.summary}\")\nfor chapter in course.chapters:\n    print(f\"Chapter: {chapter.title}\")\n    print(f\"Summary: {chapter.summary}\")\n</code></pre>"},{"location":"modules/#parallel-processing-geminiteacherparallel","title":"Parallel Processing (<code>geminiteacher.parallel</code>)","text":"<p>Provides parallel processing capabilities for faster course generation.</p> <pre><code>from geminiteacher import parallel_generate_chapters, configure_gemini_llm\nfrom geminiteacher import Course, ChapterContent\n\n# Configure the language model\nllm = configure_gemini_llm(temperature=0.2)\n\n# Generate chapters in parallel\nchapter_titles = [\"Introduction\", \"Basic Concepts\", \"Advanced Topics\"]\nchapters = parallel_generate_chapters(\n    content=\"Your content here...\",\n    chapter_titles=chapter_titles,\n    llm=llm,\n    max_workers=3,\n    course_title=\"My Course\",\n    output_dir=\"output\"\n)\n\n# Create a course with the generated chapters\ncourse = Course(\n    summary=\"Course summary\",\n    chapters=chapters\n)\n</code></pre>"},{"location":"modules/#command-line-application-geminiteacherapp","title":"Command-line Application (<code>geminiteacher.app</code>)","text":"<p>A full-featured command-line application for generating courses.</p>"},{"location":"modules/#command-line-usage","title":"Command-line Usage","text":"<p>After installation, you can use the <code>geminiteacher</code> command directly:</p> <pre><code># Basic usage with config file\ngeminiteacher --config config.yaml\n\n# Specify input file and output directory\ngeminiteacher --input content.txt --output-dir courses\n\n# Set course title and use parallel processing\ngeminiteacher --input content.txt --title \"Machine Learning Basics\" --parallel\n</code></pre>"},{"location":"modules/#programmatic-usage","title":"Programmatic Usage","text":"<p>You can also use the app module programmatically:</p> <pre><code>from geminiteacher.app import create_course_with_progressive_save, configure_logging\n\n# Configure logging\nlogger = configure_logging(log_file=\"generation.log\", verbose=True)\n\n# Generate a course with progressive saving\ncourse = create_course_with_progressive_save(\n    content=\"Your content here...\",\n    course_title=\"Python Programming\",\n    output_dir=\"courses\",\n    temperature=0.2,\n    max_chapters=5,\n    verbose=True,\n    logger=logger\n)\n</code></pre>"},{"location":"modules/#data-models","title":"Data Models","text":"<p>GeminiTeacher uses the following data models:</p>"},{"location":"modules/#course","title":"Course","text":"<pre><code>class Course:\n    \"\"\"\n    Represents a complete course with summary and chapters.\n    \"\"\"\n    summary: str\n    chapters: List[ChapterContent]\n</code></pre>"},{"location":"modules/#chaptercontent","title":"ChapterContent","text":"<pre><code>class ChapterContent:\n    \"\"\"\n    Represents a single chapter in a course.\n    \"\"\"\n    title: str\n    summary: str\n    explanation: str\n    extension: str\n</code></pre>"},{"location":"parallel/","title":"Parallel Processing","text":"<p>The <code>parallel</code> module provides tools for parallel execution of tasks with controlled API rate limiting and robust error handling. This module is particularly useful for accelerating the chapter generation process while respecting API rate limits.</p>"},{"location":"parallel/#overview","title":"Overview","text":"<p>The parallel processing capabilities include:</p> <ol> <li>Controlled Parallel Execution: Execute tasks in parallel with configurable workers</li> <li>Rate Limit Management: Add randomized delays between API requests to avoid rate limits</li> <li>Robust Error Handling: Automatically retry failed requests with exponential backoff</li> <li>Ordered Results: Ensure outputs are returned in the same order as inputs, regardless of completion time</li> <li>Progressive File Saving: Each chapter is saved to disk as soon as it's generated, preventing data loss</li> </ol>"},{"location":"parallel/#key-functions","title":"Key Functions","text":""},{"location":"parallel/#parallel_generate_chapters","title":"parallel_generate_chapters","text":"<p>The main orchestration function that handles parallel generation of course chapters:</p> <pre><code>import geminiteacher as gt\nfrom geminiteacher.parallel import parallel_generate_chapters\n\n# Define chapter titles to generate\nchapter_titles = [\n    \"Introduction to Machine Learning\",\n    \"Supervised Learning Algorithms\",\n    \"Unsupervised Learning Techniques\"\n]\n\n# Generate chapters in parallel\nchapters = parallel_generate_chapters(\n    chapter_titles=chapter_titles,\n    content=\"Your raw content here\",\n    max_workers=4,              # Number of parallel workers (processes)\n    delay_range=(0.2, 0.8),     # Random delay between API requests in seconds\n    max_retries=3,              # Number of retry attempts for failed requests\n    course_title=\"ML_Course\",   # Title for saved files\n    output_dir=\"courses\"        # Directory to save generated chapters\n)\n\n# Process the generated chapters\nfor i, chapter in enumerate(chapters):\n    print(f\"Chapter {i+1}: {chapter.title}\")\n</code></pre>"},{"location":"parallel/#generate_chapter_with_retry","title":"generate_chapter_with_retry","text":"<p>A robust wrapper around the standard <code>generate_chapter</code> function that adds retry logic:</p> <pre><code>import geminiteacher as gt\nfrom geminiteacher.parallel import generate_chapter_with_retry\n\n# Generate a single chapter with retry logic\nchapter = generate_chapter_with_retry(\n    chapter_title=\"Introduction to Neural Networks\",\n    content=\"Your raw content here\",\n    max_retries=3,              # Maximum number of retry attempts\n    retry_delay=1.0             # Base delay between retries (will increase exponentially)\n)\n\nprint(f\"Chapter title: {chapter.title}\")\nprint(f\"Summary: {chapter.summary[:100]}...\")\n</code></pre>"},{"location":"parallel/#parallel_map_with_delay","title":"parallel_map_with_delay","text":"<p>A generic function for applying any function to a list of items in parallel with controlled delays:</p> <pre><code>from geminiteacher.parallel import parallel_map_with_delay\nimport time\n\n# Define a function to execute in parallel\ndef process_item(item, prefix=\"Item\"):\n    # Simulate some work\n    time.sleep(0.5)\n    return f\"{prefix}: {item}\"\n\n# Items to process\nitems = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\"]\n\n# Process items in parallel with controlled delays\nresults = parallel_map_with_delay(\n    func=process_item,\n    items=items,\n    max_workers=3,              # Number of parallel workers\n    delay_range=(0.1, 0.5),     # Random delay between task submissions\n    prefix=\"Processed\"          # Additional parameter passed to process_item\n)\n\n# Results are in the same order as the input items\nfor item, result in zip(items, results):\n    print(f\"Original: {item} \u2192 Result: {result}\")\n</code></pre>"},{"location":"parallel/#progressive-file-saving","title":"Progressive File Saving","text":"<p>A key feature of the parallel processing module is its ability to save chapters to disk as they are generated. This provides several benefits:</p> <ol> <li>Data Safety: Even if the process is interrupted, completed chapters are already saved</li> <li>Progress Tracking: You can monitor progress by watching files appear in the output directory</li> <li>Immediate Access: Start reviewing early chapters while later ones are still being generated</li> </ol> <p>Example of how files are saved:</p> <pre><code>import geminiteacher as gt\n\ncourse = gt.create_course_parallel(\n    content=\"Your content here\",\n    course_title=\"Data_Science\",\n    output_dir=\"my_courses\"\n)\n\n# Files will be saved in a structure like:\n# my_courses/\n#   \u2514\u2500\u2500 Data_Science/\n#       \u251c\u2500\u2500 chapter_01_Introduction_to_Data_Science.md\n#       \u251c\u2500\u2500 chapter_02_Data_Collection_and_Cleaning.md\n#       \u2514\u2500\u2500 chapter_03_Exploratory_Data_Analysis.md\n</code></pre> <p>Each chapter file contains the structured content with title, summary, explanation, and extension sections.</p>"},{"location":"parallel/#api-rate-limits-consideration","title":"API Rate Limits Consideration","text":"<p>When working with external APIs like Google's Gemini, rate limits are an important consideration. The <code>parallel</code> module helps manage these limits through controlled submission timing:</p> <ol> <li>Random Delays: Adds a configurable random delay between API requests to avoid overwhelming the API</li> <li>Exponential Backoff: When retrying failed requests, uses exponential backoff to gradually increase wait times</li> <li>Configurable Workers: Allows limiting the number of concurrent processes to respect API parallelism limits</li> </ol>"},{"location":"parallel/#recommended-settings-for-google-gemini-api","title":"Recommended Settings for Google Gemini API","text":"<p>For the Google Gemini API, the following settings work well for most scenarios:</p> <ul> <li><code>max_workers</code>: 2-6 (depending on your API tier)</li> <li><code>delay_range</code>: (0.2, 1.0) seconds</li> <li><code>max_retries</code>: 3</li> </ul> <p>These settings balance speed with API reliability. For higher API tiers with more generous rate limits, you can increase <code>max_workers</code> and decrease the delay range.</p>"},{"location":"parallel/#error-handling","title":"Error Handling","text":"<p>The parallel module implements comprehensive error handling:</p> <ol> <li>Retries for Empty Responses: Automatically retries when the API returns empty content</li> <li>Exception Recovery: Catches and handles API errors with automatic retries</li> <li>Fallback Content: If all retries fail, returns a structured error message instead of failing completely</li> </ol> <p>This ensures robustness even when dealing with unreliable network conditions or API instability.</p>"},{"location":"parallel/#api-reference","title":"API Reference","text":""},{"location":"parallel/#core-functions","title":"Core Functions","text":"<p>Generate multiple chapters in parallel with retry logic and rate limiting.</p> <p>This function orchestrates the parallel generation of multiple chapters, handling API rate limits and retrying failed requests. Each chapter is saved to disk as soon as it's generated.</p> <p>Generate a chapter with retry logic for handling API failures.</p> <p>This function wraps the generate_chapter function with retry logic to handle transient API errors, timeouts, or empty responses.</p> <p>Execute a function on multiple items in parallel with a delay between submissions.</p> <p>This function uses ProcessPoolExecutor to parallelize the execution of a function across multiple items, while introducing a random delay between task submissions to avoid overwhelming external APIs with simultaneous requests.</p>"},{"location":"parallel/#geminiteacher.parallel.parallel_generate_chapters--parameters","title":"Parameters","text":"<p>chapter_titles : List[str]     List of chapter titles to generate. content : str     The raw content to use for generating chapters. llm : Optional[BaseLanguageModel], optional     The language model to use. If None, a default model will be configured. temperature : float, optional     The temperature setting for generation. Default is 0.0. custom_prompt : Optional[str], optional     Custom instructions to append to the chapter generation prompt. max_workers : Optional[int], optional     Maximum number of worker processes. If None, uses the default. delay_range : tuple, optional     Range (min, max) in seconds for the random delay between task submissions.     Default is (0.1, 0.5). max_retries : int, optional     Maximum number of retry attempts per chapter. Default is 3. course_title : str, optional     Title of the course for saving files. Default is \"course\". output_dir : str, optional     Directory to save the chapter files. Default is \"output\".</p>"},{"location":"parallel/#geminiteacher.parallel.parallel_generate_chapters--returns","title":"Returns","text":"<p>List[ChapterContent]     List of generated chapter contents in the same order as the input titles.</p> Source code in <code>src\\geminiteacher\\parallel.py</code> <pre><code>def parallel_generate_chapters(\n    chapter_titles: List[str],\n    content: str,\n    llm: Optional[BaseLanguageModel] = None,\n    temperature: float = 0.0,\n    custom_prompt: Optional[str] = None,\n    max_workers: Optional[int] = None,\n    delay_range: tuple = (0.1, 0.5),\n    max_retries: int = 3,\n    course_title: str = \"course\",\n    output_dir: str = \"output\"\n) -&gt; List[ChapterContent]:\n    \"\"\"\n    Generate multiple chapters in parallel with retry logic and rate limiting.\n\n    This function orchestrates the parallel generation of multiple chapters,\n    handling API rate limits and retrying failed requests. Each chapter is\n    saved to disk as soon as it's generated.\n\n    Parameters\n    ----------\n    chapter_titles : List[str]\n        List of chapter titles to generate.\n    content : str\n        The raw content to use for generating chapters.\n    llm : Optional[BaseLanguageModel], optional\n        The language model to use. If None, a default model will be configured.\n    temperature : float, optional\n        The temperature setting for generation. Default is 0.0.\n    custom_prompt : Optional[str], optional\n        Custom instructions to append to the chapter generation prompt.\n    max_workers : Optional[int], optional\n        Maximum number of worker processes. If None, uses the default.\n    delay_range : tuple, optional\n        Range (min, max) in seconds for the random delay between task submissions.\n        Default is (0.1, 0.5).\n    max_retries : int, optional\n        Maximum number of retry attempts per chapter. Default is 3.\n    course_title : str, optional\n        Title of the course for saving files. Default is \"course\".\n    output_dir : str, optional\n        Directory to save the chapter files. Default is \"output\".\n\n    Returns\n    -------\n    List[ChapterContent]\n        List of generated chapter contents in the same order as the input titles.\n    \"\"\"\n    logger = logging.getLogger(\"geminiteacher.parallel\")\n    logger.info(f\"Starting parallel generation of {len(chapter_titles)} chapters with {max_workers or multiprocessing.cpu_count()} workers\")\n\n    # Get API key from the LLM if provided or environment\n    api_key = None\n    model_name = \"gemini-1.5-pro\"\n\n    if llm is not None:\n        # Try to extract API key and model name from the provided LLM\n        try:\n            # This assumes LLM is a ChatGoogleGenerativeAI instance\n            api_key = getattr(llm, \"google_api_key\", None)\n            model_name = getattr(llm, \"model\", model_name)\n            logger.info(f\"Using model: {model_name}\")\n        except Exception:\n            logger.warning(\"Could not extract API key from provided LLM, will use environment variables\")\n\n    # Create a list of (index, chapter_title) tuples to preserve order\n    indexed_titles = list(enumerate(chapter_titles))\n\n    logger.info(f\"Using delay range: {delay_range[0]}-{delay_range[1]}s between tasks\")\n    logger.info(f\"Saving chapters progressively to {output_dir}/{course_title}/\")\n\n    # Generate chapters in parallel with delay between submissions and save each one as it completes\n    results = parallel_map_with_delay(\n        _worker_generate_and_save_chapter,\n        indexed_titles,\n        max_workers=max_workers,\n        delay_range=delay_range,\n        content=content,\n        course_title=course_title,\n        output_dir=output_dir,\n        api_key=api_key,\n        model_name=model_name,\n        temperature=temperature,\n        custom_prompt=custom_prompt,\n        max_retries=max_retries,\n        retry_delay=1.0\n    )\n\n    # Extract just the chapter content from the results (idx, chapter, file_path)\n    chapters = [result[1] for result in results]\n\n    logger.info(f\"Completed parallel generation of {len(chapters)} chapters\")\n    return chapters \n</code></pre>"},{"location":"parallel/#geminiteacher.parallel.generate_chapter_with_retry--parameters","title":"Parameters","text":"<p>chapter_title : str     The title of the chapter to generate. content : str     The raw content to use for generating the chapter. llm : Optional[BaseLanguageModel], optional     The language model to use. If None, a default model will be configured. temperature : float, optional     The temperature setting for generation. Default is 0.0. custom_prompt : Optional[str], optional     Custom instructions to append to the chapter generation prompt. max_retries : int, optional     Maximum number of retry attempts. Default is 3. retry_delay : float, optional     Base delay between retries in seconds. Default is 1.0.</p>"},{"location":"parallel/#geminiteacher.parallel.generate_chapter_with_retry--returns","title":"Returns","text":"<p>ChapterContent     The generated chapter content.</p>"},{"location":"parallel/#geminiteacher.parallel.generate_chapter_with_retry--notes","title":"Notes","text":"<p>This function implements an exponential backoff strategy for retries, with each retry attempt waiting longer than the previous one.</p> Source code in <code>src\\geminiteacher\\parallel.py</code> <pre><code>def generate_chapter_with_retry(\n    chapter_title: str, \n    content: str, \n    llm: Optional[BaseLanguageModel] = None,\n    temperature: float = 0.0,\n    custom_prompt: Optional[str] = None,\n    max_retries: int = 3,\n    retry_delay: float = 1.0\n) -&gt; ChapterContent:\n    \"\"\"\n    Generate a chapter with retry logic for handling API failures.\n\n    This function wraps the generate_chapter function with retry logic to handle\n    transient API errors, timeouts, or empty responses.\n\n    Parameters\n    ----------\n    chapter_title : str\n        The title of the chapter to generate.\n    content : str\n        The raw content to use for generating the chapter.\n    llm : Optional[BaseLanguageModel], optional\n        The language model to use. If None, a default model will be configured.\n    temperature : float, optional\n        The temperature setting for generation. Default is 0.0.\n    custom_prompt : Optional[str], optional\n        Custom instructions to append to the chapter generation prompt.\n    max_retries : int, optional\n        Maximum number of retry attempts. Default is 3.\n    retry_delay : float, optional\n        Base delay between retries in seconds. Default is 1.0.\n\n    Returns\n    -------\n    ChapterContent\n        The generated chapter content.\n\n    Notes\n    -----\n    This function implements an exponential backoff strategy for retries,\n    with each retry attempt waiting longer than the previous one.\n    \"\"\"\n    logger = logging.getLogger(\"geminiteacher.parallel\")\n\n    for attempt in range(max_retries + 1):\n        try:\n            logger.info(f\"Generating chapter '{chapter_title}' (attempt {attempt + 1}/{max_retries + 1})\")\n            chapter = generate_chapter(\n                chapter_title=chapter_title,\n                content=content,\n                llm=llm,\n                temperature=temperature,\n                custom_prompt=custom_prompt\n            )\n\n            # Check if we got a valid response (non-empty explanation)\n            if chapter.explanation.strip():\n                logger.info(f\"Successfully generated chapter '{chapter_title}' (length: {len(chapter.explanation)} chars)\")\n                return chapter\n            else:\n                raise ValueError(\"Empty chapter explanation received\")\n\n        except Exception as e:\n            if attempt &lt; max_retries:\n                # Calculate backoff with jitter\n                backoff = retry_delay * (2 ** attempt) + random.uniform(0, 1)\n                logger.warning(\n                    f\"Chapter generation failed for '{chapter_title}' \"\n                    f\"(attempt {attempt + 1}/{max_retries + 1}): {str(e)}. \"\n                    f\"Retrying in {backoff:.2f}s...\"\n                )\n                time.sleep(backoff)\n            else:\n                logger.error(\n                    f\"All retry attempts failed for chapter '{chapter_title}'. \"\n                    f\"Last error: {str(e)}\"\n                )\n                # Return a basic chapter with error information\n                return ChapterContent(\n                    title=chapter_title,\n                    summary=\"Error: Failed to generate chapter content after multiple attempts.\",\n                    explanation=f\"The chapter generation process encountered repeated errors: {str(e)}\",\n                    extension=\"Please try regenerating this chapter or check your API configuration.\"\n                )\n</code></pre>"},{"location":"parallel/#geminiteacher.parallel.parallel_map_with_delay--parameters","title":"Parameters","text":"<p>func : Callable[..., T]     The function to execute in parallel. items : List[Any]     The list of items to process. max_workers : Optional[int], optional     Maximum number of worker processes. If None, uses the default     (typically the number of CPU cores). delay_range : tuple, optional     Range (min, max) in seconds for the random delay between task submissions.     Default is (0.1, 0.5). **kwargs     Additional keyword arguments to pass to the function.</p>"},{"location":"parallel/#geminiteacher.parallel.parallel_map_with_delay--returns","title":"Returns","text":"<p>List[T]     List of results in the same order as the input items.</p>"},{"location":"parallel/#geminiteacher.parallel.parallel_map_with_delay--examples","title":"Examples","text":"<p>def process_item(item, factor=1): ...     return item * factor items = [1, 2, 3, 4, 5] results = parallel_map_with_delay(process_item, items, factor=2) print(results) [2, 4, 6, 8, 10]</p> Source code in <code>src\\geminiteacher\\parallel.py</code> <pre><code>def parallel_map_with_delay(\n    func: Callable[..., T],\n    items: List[Any],\n    max_workers: Optional[int] = None,\n    delay_range: tuple = (0.1, 0.5),\n    **kwargs\n) -&gt; List[T]:\n    \"\"\"\n    Execute a function on multiple items in parallel with a delay between submissions.\n\n    This function uses ProcessPoolExecutor to parallelize the execution of a function\n    across multiple items, while introducing a random delay between task submissions\n    to avoid overwhelming external APIs with simultaneous requests.\n\n    Parameters\n    ----------\n    func : Callable[..., T]\n        The function to execute in parallel.\n    items : List[Any]\n        The list of items to process.\n    max_workers : Optional[int], optional\n        Maximum number of worker processes. If None, uses the default\n        (typically the number of CPU cores).\n    delay_range : tuple, optional\n        Range (min, max) in seconds for the random delay between task submissions.\n        Default is (0.1, 0.5).\n    **kwargs\n        Additional keyword arguments to pass to the function.\n\n    Returns\n    -------\n    List[T]\n        List of results in the same order as the input items.\n\n    Examples\n    --------\n    &gt;&gt;&gt; def process_item(item, factor=1):\n    ...     return item * factor\n    &gt;&gt;&gt; items = [1, 2, 3, 4, 5]\n    &gt;&gt;&gt; results = parallel_map_with_delay(process_item, items, factor=2)\n    &gt;&gt;&gt; print(results)\n    [2, 4, 6, 8, 10]\n    \"\"\"\n    logger = logging.getLogger(\"geminiteacher.parallel\")\n    results = []\n    total_items = len(items)\n\n    # Export the current log level to the environment for worker processes\n    os.environ[\"GeminiTeacher_LOG_LEVEL\"] = logger.getEffectiveLevel().__str__()\n\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        # Submit all tasks with a delay between submissions\n        futures = []\n        for i, item in enumerate(items):\n            # Add a small random delay to avoid overwhelming the API\n            delay = random.uniform(delay_range[0], delay_range[1])\n            logger.debug(f\"Submitting task {i+1}/{total_items} with delay: {delay:.2f}s\")\n            time.sleep(delay)\n\n            # Submit the task to the process pool\n            future = executor.submit(func, item, **kwargs)\n            futures.append(future)\n            logger.info(f\"Submitted task {i+1}/{total_items}\")\n\n        # Collect results in the original order\n        for i, future in enumerate(futures):\n            try:\n                logger.info(f\"Waiting for task {i+1}/{total_items} to complete\")\n                result = future.result()\n                results.append(result)\n                logger.info(f\"Completed task {i+1}/{total_items}\")\n            except Exception as e:\n                logger.error(f\"Task {i+1}/{total_items} failed: {str(e)}\")\n                # Re-raise the exception to maintain the expected behavior\n                raise\n\n    return results\n</code></pre>"},{"location":"parallel/#performance-considerations","title":"Performance Considerations","text":"<p>When using parallel processing, consider the following to optimize performance:</p> <ol> <li>CPU Cores: The optimal <code>max_workers</code> is typically close to the number of available CPU cores</li> <li>Memory Usage: Each worker process requires memory, so limit <code>max_workers</code> on memory-constrained systems</li> <li>API Rate Limits: Always respect API rate limits by adjusting <code>delay_range</code> and <code>max_workers</code></li> <li>Task Granularity: Parallel processing works best when individual tasks take significant time</li> </ol>"},{"location":"parallel/#integration-with-course-generator","title":"Integration with Course Generator","text":"<p>The parallel module integrates seamlessly with the coursemaker module through the <code>create_course_parallel</code> function:</p> <pre><code>import geminiteacher as gt\n\n# Generate a course using parallel processing\ncourse = gt.create_course_parallel(\n    \"Your raw content here\",\n    max_workers=4,\n    delay_range=(0.2, 0.8),\n    max_retries=3,\n    course_title=\"Advanced_Topics\",\n    output_dir=\"output/courses\"\n)\n\nprint(f\"Generated {len(course.chapters)} chapters in parallel\")\n</code></pre>"},{"location":"parallel/#limitations","title":"Limitations","text":"<ul> <li>Increased memory usage compared to sequential processing</li> <li>Potential for higher API costs due to faster request rates</li> <li>Debugging can be more complex in parallel environments</li> </ul>"},{"location":"parallel/#future-enhancements","title":"Future Enhancements","text":"<p>Future versions may include: - Adaptive rate limiting based on API response times - Better telemetry for monitoring API usage - Support for concurrent.futures.ThreadPoolExecutor for I/O-bound tasks - Dynamic worker allocation based on system resources </p>"},{"location":"usage/","title":"Usage Guide","text":"<p>GeminiTeacher provides powerful tools for automatically generating structured educational courses from raw text content. This guide will walk you through the installation, setup, and various ways to use the package.</p>"},{"location":"usage/#overview","title":"Overview","text":"<p>The course generation process follows a four-phase pipeline:</p> <ol> <li>Table of Contents Generation: The system analyzes the input content and creates a logical structure with chapter titles.</li> <li>Chapter Generation: Detailed explanations are created for each chapter with a consistent structure.</li> <li>Summary Generation: A comprehensive course summary ties everything together.</li> <li>File Saving: When using parallel processing, chapters are saved to disk as they're generated.</li> </ol> <p>All generated content follows a structured format designed for educational purposes.</p>"},{"location":"usage/#installation","title":"Installation","text":"<p>Install GeminiTeacher directly from PyPI:</p> <pre><code>pip install geminiteacher\n</code></pre>"},{"location":"usage/#setting-up-google-gemini-api","title":"Setting Up Google Gemini API","text":"<p>GeminiTeacher uses Google's Gemini API for all LLM operations:</p> <ol> <li>Make sure you have the package installed:</li> </ol> <pre><code>pip install geminiteacher\n</code></pre> <ol> <li>Set up your Gemini API key as an environment variable:</li> </ol> <pre><code># On Linux/macOS\nexport GOOGLE_API_KEY=your_gemini_api_key_here\n\n# On Windows PowerShell\n$env:GOOGLE_API_KEY = \"your_gemini_api_key_here\"\n\n# On Windows Command Prompt\nset GOOGLE_API_KEY=your_gemini_api_key_here\n</code></pre>"},{"location":"usage/#command-line-usage","title":"Command-line Usage","text":"<p>The easiest way to use GeminiTeacher is through its command-line interface, which is automatically installed with the package.</p>"},{"location":"usage/#basic-command-line-usage","title":"Basic Command-line Usage","text":"<p>After installation, you can use the <code>geminiteacher</code> command directly:</p> <pre><code># Generate a course from a text file\ngeminiteacher --input content.txt --output-dir courses --title \"Machine Learning Basics\"\n\n# If the command is not found, you can use the Python module syntax:\npython -m geminiteacher.app.generate_course --input content.txt --output-dir courses\n</code></pre>"},{"location":"usage/#advanced-command-line-options","title":"Advanced Command-line Options","text":"<p>For more control, you can use additional command-line options:</p> <pre><code># Enable parallel processing for faster generation\ngeminiteacher --input content.txt --parallel --max-workers 4\n\n# Set a specific temperature for more creative outputs\ngeminiteacher --input content.txt --temperature 0.3\n\n# Use a custom prompt file for specialized instructions\ngeminiteacher --input content.txt --custom-prompt my_instructions.txt\n\n# Generate a fixed number of chapters\ngeminiteacher --input content.txt --max-chapters 5 --fixed-chapter-count\n</code></pre>"},{"location":"usage/#using-a-configuration-file","title":"Using a Configuration File","text":"<p>For the most control, create a <code>config.yaml</code> file:</p> <pre><code># API Configuration\napi:\n  google_api_key: \"your_gemini_api_key_here\"  # Or use environment variable\n  model_name: \"gemini-1.5-pro\"\n\n# Input/Output Settings\ninput:\n  path: \"input/content.txt\"\n\noutput:\n  directory: \"output\"\n\n# Course Settings\ncourse:\n  title: \"Machine Learning\"\n  custom_prompt: \"custom_instructions.txt\"\n\n# Generation Settings\ngeneration:\n  temperature: 0.2\n  max_chapters: 10\n  fixed_chapter_count: false\n\n# Parallel Processing Settings\nparallel:\n  enabled: true\n  max_workers: 4\n  delay_min: 0.2\n  delay_max: 0.8\n  max_retries: 3\n</code></pre> <p>Then run:</p> <pre><code>geminiteacher --config config.yaml\n</code></pre> <p>For complete details on the command-line interface, see the CLI Application documentation.</p>"},{"location":"usage/#python-api-usage","title":"Python API Usage","text":""},{"location":"usage/#quick-start","title":"Quick Start","text":"<p>If you prefer to use GeminiTeacher programmatically, you can use the high-level import:</p> <pre><code>import geminiteacher as gt\n\n# Load content from a file or string\nwith open(\"your_content.txt\", \"r\") as f:\n    raw_content = f.read()\n\n# Generate a structured course\ncourse = gt.create_course(raw_content)\n\n# The course object contains all generated components\nprint(f\"Generated {len(course.chapters)} chapters\")\nprint(f\"Summary: {course.summary[:100]}...\")\n</code></pre>"},{"location":"usage/#parallel-processing-for-speed","title":"Parallel Processing for Speed","text":"<p>For faster course generation, use parallel processing to generate chapters concurrently:</p> <pre><code>import geminiteacher as gt\n\n# Load content from a file or string\nwith open(\"your_content.txt\", \"r\") as f:\n    raw_content = f.read()\n\n# Generate a structured course with parallel processing\ncourse = gt.create_course_parallel(\n    raw_content, \n    max_workers=4,              # Number of parallel workers\n    delay_range=(0.2, 0.8),     # Random delay between API requests\n    max_retries=3,              # Retry attempts for failed API calls\n    course_title=\"my_course\",   # Title for saved files\n    output_dir=\"courses\"        # Directory to save generated chapters\n)\n\n# The course object contains all generated components\nprint(f\"Generated {len(course.chapters)} chapters in parallel\")\nprint(f\"Summary: {course.summary[:100]}...\")\n</code></pre>"},{"location":"usage/#using-the-app-module-directly","title":"Using the App Module Directly","text":"<p>For more advanced features like progressive saving and detailed logging:</p> <pre><code>from geminiteacher.app import create_course_with_progressive_save, configure_logging\n\n# Configure logging\nlogger = configure_logging(log_file=\"generation.log\", verbose=True)\n\n# Generate a course with progressive saving\ncourse = create_course_with_progressive_save(\n    content=\"Your content here...\",\n    course_title=\"Python Programming\",\n    output_dir=\"courses\",\n    temperature=0.2,\n    max_chapters=5,\n    custom_prompt=\"Focus on practical examples\",\n    verbose=True,\n    logger=logger\n)\n</code></pre>"},{"location":"usage/#accessing-course-components","title":"Accessing Course Components","text":"<p>The course generator produces structured data that you can easily access:</p> <pre><code># Iterate through chapters\nfor i, chapter in enumerate(course.chapters):\n    print(f\"Chapter {i+1}: {chapter.title}\")\n    print(\"Summary:\", chapter.summary)\n    print(\"Explanation:\", chapter.explanation[:100] + \"...\")\n    print(\"Extension:\", chapter.extension[:100] + \"...\")\n    print(\"---\")\n\n# Access the course summary\nprint(\"Course Summary:\", course.summary)\n</code></pre>"},{"location":"usage/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/#using-individual-components","title":"Using Individual Components","text":"<p>You can also use the individual components of the pipeline for more control:</p> <pre><code>from geminiteacher import configure_gemini_llm, generate_toc, generate_chapter, generate_summary\n\n# Configure the Gemini API with custom parameters\nllm = configure_gemini_llm(\n    model_name=\"gemini-1.5-flash\",  # Use a different model\n    temperature=0.3                  # Adjust temperature for more varied outputs\n)\n\n# Generate just the table of contents\nchapter_titles = generate_toc(\n    raw_content, \n    llm=llm,\n    max_chapters=5,            # Maximum number of chapters\n    fixed_chapter_count=True   # Generate exactly 5 chapters\n)\nprint(f\"Generated {len(chapter_titles)} chapter titles\")\n\n# Generate a single chapter\nchapter = generate_chapter(\n    \"Introduction to AI\", \n    raw_content, \n    llm=llm,\n    custom_prompt=\"Focus on practical examples and code snippets.\"\n)\nprint(chapter.title)\nprint(chapter.summary)\n\n# Generate a summary from chapters\nchapters = [chapter]  # You would typically have multiple chapters\nsummary = generate_summary(raw_content, chapters, llm=llm)\nprint(summary)\n</code></pre>"},{"location":"usage/#using-parallel-components","title":"Using Parallel Components","text":"<p>For more control over the parallel processing:</p> <pre><code>from geminiteacher import generate_toc, generate_summary, configure_gemini_llm\nfrom geminiteacher.parallel import parallel_generate_chapters\n\n# Configure the Gemini API\nllm = configure_gemini_llm()\n\n# Generate table of contents\nchapter_titles = generate_toc(raw_content, llm=llm)\n\n# Generate chapters in parallel with custom parameters\nchapters = parallel_generate_chapters(\n    chapter_titles=chapter_titles,\n    content=raw_content,\n    llm=llm,\n    max_workers=4,\n    delay_range=(0.2, 0.8),\n    max_retries=3,\n    course_title=\"advanced_course\",\n    output_dir=\"output/courses\"\n)\n\n# Generate a summary from the chapters\nsummary = generate_summary(raw_content, chapters, llm=llm)\n</code></pre>"},{"location":"usage/#custom-prompts","title":"Custom Prompts","text":"<p>You can customize how chapters are generated by providing custom instructions:</p> <pre><code>import geminiteacher as gt\n\ncustom_prompt = \"\"\"\nFocus on practical examples and include code snippets where relevant.\nEach concept should be explained with a real-world application.\nInclude exercises at the end of each explanation section.\n\"\"\"\n\ncourse = gt.create_course(\n    raw_content,\n    custom_prompt=custom_prompt,\n    temperature=0.3  # Higher temperature for more creative outputs\n)\n</code></pre>"},{"location":"usage/#api-reference","title":"API Reference","text":""},{"location":"usage/#core-functions","title":"Core Functions","text":"<p>Create a complete structured course from raw content.</p> <p>This function orchestrates the entire course creation process: 1. Generate a table of contents 2. Create detailed explanations for each chapter 3. Generate a comprehensive course summary</p> <p>Create a course with parallel chapter generation.</p> <p>This function creates a course by generating chapters in parallel using multiple processes. This can significantly speed up the course generation process, especially for courses with many chapters.</p> <p>Generate a table of contents from raw content.</p> <p>This function takes raw text content and uses an LLM to generate a structured table of contents with 1-10 chapter titles.</p> <p>Generate a structured explanation for a single chapter.</p> <p>This function uses an LLM to create a detailed, structured explanation for a chapter based on its title and the original content.</p> <p>Generate a comprehensive summary for the entire course.</p> <p>Configure and return a Google Gemini model for use with the coursemaker module.</p>"},{"location":"usage/#geminiteacher.coursemaker.create_course--parameters","title":"Parameters","text":"<p>content : str     The raw content to transform into a course. llm : BaseLanguageModel, optional     The language model to use for generation. If None, the function will     automatically configure a Gemini model. temperature : float, optional     The temperature setting for the LLM, affecting randomness in output.     Default is 0.0 (deterministic output). verbose : bool, optional     Whether to print progress messages during course generation.     Default is False. max_chapters : int, optional     Maximum number of chapters to generate. Default is 10. fixed_chapter_count : bool, optional     If True, generate exactly max_chapters. If False, generate between 1 and max_chapters     based on content complexity. Default is False. custom_prompt : Optional[str], optional     Custom instructions to append to the \"\u7cfb\u7edf\u6027\u8bb2\u89e3\" section of each chapter. Default is None.</p>"},{"location":"usage/#geminiteacher.coursemaker.create_course--returns","title":"Returns","text":"<p>Course     A complete course object with all components.</p>"},{"location":"usage/#geminiteacher.coursemaker.create_course--examples","title":"Examples","text":"<p>course = create_course(\"Raw content about a topic...\") print(f\"Generated {len(course.chapters)} chapters\")</p> <p>course = create_course(\"Extended content...\", max_chapters=20) print(f\"Generated {len(course.chapters)} chapters\")</p> <p>course = create_course(\"Simple content...\", max_chapters=5, fixed_chapter_count=True) print(f\"Generated {len(course.chapters)} chapters\")</p> Source code in <code>src\\geminiteacher\\coursemaker.py</code> <pre><code>def create_course(content: str, llm: Optional[BaseLanguageModel] = None, temperature: float = 0.0, verbose: bool = False, max_chapters: int = 10, fixed_chapter_count: bool = False, custom_prompt: Optional[str] = None) -&gt; Course:\n    \"\"\"\n    Create a complete structured course from raw content.\n\n    This function orchestrates the entire course creation process:\n    1. Generate a table of contents\n    2. Create detailed explanations for each chapter\n    3. Generate a comprehensive course summary\n\n    Parameters\n    ----\n    content : str\n        The raw content to transform into a course.\n    llm : BaseLanguageModel, optional\n        The language model to use for generation. If None, the function will\n        automatically configure a Gemini model.\n    temperature : float, optional\n        The temperature setting for the LLM, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n    verbose : bool, optional\n        Whether to print progress messages during course generation.\n        Default is False.\n    max_chapters : int, optional\n        Maximum number of chapters to generate. Default is 10.\n    fixed_chapter_count : bool, optional\n        If True, generate exactly max_chapters. If False, generate between 1 and max_chapters\n        based on content complexity. Default is False.\n    custom_prompt : Optional[str], optional\n        Custom instructions to append to the \"\u7cfb\u7edf\u6027\u8bb2\u89e3\" section of each chapter. Default is None.\n\n    Returns\n    ----\n    Course\n        A complete course object with all components.\n\n    Examples\n    -----\n    &gt;&gt;&gt; course = create_course(\"Raw content about a topic...\")\n    &gt;&gt;&gt; print(f\"Generated {len(course.chapters)} chapters\")\n\n    &gt;&gt;&gt; course = create_course(\"Extended content...\", max_chapters=20)\n    &gt;&gt;&gt; print(f\"Generated {len(course.chapters)} chapters\")\n\n    &gt;&gt;&gt; course = create_course(\"Simple content...\", max_chapters=5, fixed_chapter_count=True)\n    &gt;&gt;&gt; print(f\"Generated {len(course.chapters)} chapters\")\n    &gt;&gt;&gt; # Will always print \"Generated 5 chapters\"\n\n    &gt;&gt;&gt; custom = \"\u8bf7\u7279\u522b\u5173\u6ce8\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\uff0c\u5e76\u63d0\u4f9b\u66f4\u591a\u4ee3\u7801\u793a\u4f8b\u3002\"\n    &gt;&gt;&gt; course = create_course(\"Content about coding...\", custom_prompt=custom)\n    \"\"\"\n    # Initialize the course with the original content\n    course = Course(content=content)\n\n    # If no LLM is provided, configure Gemini\n    if llm is None:\n        if verbose:\n            print(\"Configuring default Gemini LLM...\")\n        llm = get_default_llm(temperature)\n\n    # Step 1: Generate the table of contents\n    if verbose:\n        print(f\"Generating table of contents (max {max_chapters} chapters)...\")\n        if fixed_chapter_count:\n            print(f\"Using fixed chapter count mode: exactly {max_chapters} chapters\")\n        if custom_prompt:\n            print(\"Using custom prompt for chapter generation\")\n\n    chapter_titles = generate_toc(\n        content, \n        llm=llm, \n        temperature=temperature, \n        max_chapters=max_chapters,\n        fixed_chapter_count=fixed_chapter_count\n    )\n\n    if verbose:\n        print(f\"Generated {len(chapter_titles)} chapter titles\")\n\n    # Step 2: Generate content for each chapter\n    chapters = []\n    for i, title in enumerate(chapter_titles):\n        if verbose:\n            print(f\"Generating chapter {i+1}/{len(chapter_titles)}: {title}\")\n        chapter = generate_chapter(\n            title, \n            content, \n            llm=llm, \n            temperature=temperature,\n            custom_prompt=custom_prompt\n        )\n        chapters.append(chapter)\n\n    course.chapters = chapters\n\n    # Step 3: Generate the course summary\n    if chapters:\n        if verbose:\n            print(\"Generating course summary...\")\n        course.summary = generate_summary(content, chapters, llm=llm, temperature=temperature)\n        if verbose:\n            print(\"Course generation complete!\")\n\n    return course\n</code></pre>"},{"location":"usage/#geminiteacher.coursemaker.create_course--will-always-print-generated-5-chapters","title":"Will always print \"Generated 5 chapters\"","text":"<p>custom = \"\u8bf7\u7279\u522b\u5173\u6ce8\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\uff0c\u5e76\u63d0\u4f9b\u66f4\u591a\u4ee3\u7801\u793a\u4f8b\u3002\" course = create_course(\"Content about coding...\", custom_prompt=custom)</p>"},{"location":"usage/#geminiteacher.coursemaker.create_course_parallel--parameters","title":"Parameters","text":"<p>content : str     The raw content to transform into a course llm : Optional[BaseLanguageModel], optional     Language model to use. If None, a default model will be configured. temperature : float, optional     Temperature for generation. Default is 0.0. verbose : bool, optional     Whether to print progress messages. Default is False. max_chapters : int, optional     Maximum number of chapters. Default is 10. fixed_chapter_count : bool, optional     Whether to use fixed chapter count. Default is False. custom_prompt : Optional[str], optional     Custom prompt instructions. Default is None. max_workers : Optional[int], optional     Maximum number of worker processes. If None, uses the default. delay_range : tuple, optional     Range (min, max) in seconds for the random delay between task submissions.     Default is (0.1, 0.5). max_retries : int, optional     Maximum number of retry attempts per chapter. Default is 3. course_title : str, optional     Title of the course for saving files. Default is \"course\". output_dir : str, optional     Directory to save the chapter files. Default is \"output\".</p>"},{"location":"usage/#geminiteacher.coursemaker.create_course_parallel--returns","title":"Returns","text":"<p>Course     The generated course object</p> Source code in <code>src\\geminiteacher\\coursemaker.py</code> <pre><code>def create_course_parallel(\n    content: str, \n    llm: Optional[BaseLanguageModel] = None, \n    temperature: float = 0.0, \n    verbose: bool = False, \n    max_chapters: int = 10, \n    fixed_chapter_count: bool = False, \n    custom_prompt: Optional[str] = None,\n    max_workers: Optional[int] = None,\n    delay_range: tuple = (0.1, 0.5),\n    max_retries: int = 3,\n    course_title: str = \"course\",\n    output_dir: str = \"output\"\n) -&gt; Course:\n    \"\"\"\n    Create a course with parallel chapter generation.\n\n    This function creates a course by generating chapters in parallel using\n    multiple processes. This can significantly speed up the course generation\n    process, especially for courses with many chapters.\n\n    Parameters\n    ----------\n    content : str\n        The raw content to transform into a course\n    llm : Optional[BaseLanguageModel], optional\n        Language model to use. If None, a default model will be configured.\n    temperature : float, optional\n        Temperature for generation. Default is 0.0.\n    verbose : bool, optional\n        Whether to print progress messages. Default is False.\n    max_chapters : int, optional\n        Maximum number of chapters. Default is 10.\n    fixed_chapter_count : bool, optional\n        Whether to use fixed chapter count. Default is False.\n    custom_prompt : Optional[str], optional\n        Custom prompt instructions. Default is None.\n    max_workers : Optional[int], optional\n        Maximum number of worker processes. If None, uses the default.\n    delay_range : tuple, optional\n        Range (min, max) in seconds for the random delay between task submissions.\n        Default is (0.1, 0.5).\n    max_retries : int, optional\n        Maximum number of retry attempts per chapter. Default is 3.\n    course_title : str, optional\n        Title of the course for saving files. Default is \"course\".\n    output_dir : str, optional\n        Directory to save the chapter files. Default is \"output\".\n\n    Returns\n    -------\n    Course\n        The generated course object\n    \"\"\"\n    # Import here to avoid circular imports\n    from geminiteacher.parallel import parallel_generate_chapters\n\n    # Initialize the course with the original content\n    course = Course(content=content)\n\n    # If no LLM is provided, configure Gemini\n    if llm is None:\n        if verbose:\n            print(\"Configuring default Gemini LLM...\")\n        llm = get_default_llm(temperature)\n\n    # Step 1: Generate the table of contents\n    if verbose:\n        print(f\"Generating table of contents (max {max_chapters} chapters)...\")\n        if fixed_chapter_count:\n            print(f\"Using fixed chapter count mode: exactly {max_chapters} chapters\")\n        if custom_prompt:\n            print(\"Using custom prompt for chapter generation\")\n\n    chapter_titles = generate_toc(\n        content, \n        llm=llm, \n        temperature=temperature, \n        max_chapters=max_chapters,\n        fixed_chapter_count=fixed_chapter_count\n    )\n\n    if verbose:\n        print(f\"Generated {len(chapter_titles)} chapter titles\")\n\n    # Step 2: Generate content for each chapter in parallel\n    if chapter_titles:\n        # Get API key from the LLM if it's a ChatGoogleGenerativeAI instance\n        api_key = None\n        model_name = \"gemini-1.5-pro\"\n\n        if llm is not None:\n            try:\n                # Try to extract API key from the LLM instance\n                api_key = getattr(llm, \"google_api_key\", None)\n                model_name = getattr(llm, \"model\", model_name)\n            except Exception:\n                pass\n\n        chapters = parallel_generate_chapters(\n            chapter_titles=chapter_titles,\n            content=content,\n            llm=llm,  # This will be used to extract API key and model if possible, but not passed to worker processes\n            temperature=temperature,\n            custom_prompt=custom_prompt,\n            max_workers=max_workers,\n            delay_range=delay_range,\n            max_retries=max_retries,\n            course_title=course_title,\n            output_dir=output_dir\n        )\n\n        course.chapters = chapters\n\n        # Step 3: Generate the course summary\n        if verbose:\n            print(\"Generating course summary...\")\n        course.summary = generate_summary(content, chapters, llm=llm, temperature=temperature)\n        if verbose:\n            print(\"Course generation complete!\")\n\n    return course \n</code></pre>"},{"location":"usage/#geminiteacher.coursemaker.generate_toc--parameters","title":"Parameters","text":"<p>content : str     The raw text content to analyze and create a table of contents for. llm : BaseLanguageModel, optional     The language model to use for generation. If None, the function will      automatically configure a Gemini model. temperature : float, optional     The temperature setting for the LLM, affecting randomness in output.     Default is 0.0 (deterministic output). max_chapters : int, optional     Maximum number of chapters to generate. Default is 10. fixed_chapter_count : bool, optional     If True, generate exactly max_chapters. If False, generate between 1 and max_chapters     based on content complexity. Default is False.</p>"},{"location":"usage/#geminiteacher.coursemaker.generate_toc--returns","title":"Returns","text":"<p>List[str]     A list of chapter titles without numbering.</p>"},{"location":"usage/#geminiteacher.coursemaker.generate_toc--examples","title":"Examples","text":"<p>toc = generate_toc(\"This is a text about machine learning...\") print(toc) ['Introduction to Machine Learning', 'Types of Machine Learning', ...]</p> <p>toc = generate_toc(\"Content about history...\", max_chapters=20) print(len(toc)) 15  # The actual number may vary based on content</p> <p>toc = generate_toc(\"Content about a simple topic...\", max_chapters=5, fixed_chapter_count=True) print(len(toc)) 5  # Will always generate exactly 5 chapters</p> Source code in <code>src\\geminiteacher\\coursemaker.py</code> <pre><code>def generate_toc(content: str, llm: Optional[BaseLanguageModel] = None, temperature: float = 0.0, max_chapters: int = 10, fixed_chapter_count: bool = False) -&gt; List[str]:\n    \"\"\"\n    Generate a table of contents from raw content.\n\n    This function takes raw text content and uses an LLM to generate\n    a structured table of contents with 1-10 chapter titles.\n\n    Parameters\n    ----\n    content : str\n        The raw text content to analyze and create a table of contents for.\n    llm : BaseLanguageModel, optional\n        The language model to use for generation. If None, the function will \n        automatically configure a Gemini model.\n    temperature : float, optional\n        The temperature setting for the LLM, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n    max_chapters : int, optional\n        Maximum number of chapters to generate. Default is 10.\n    fixed_chapter_count : bool, optional\n        If True, generate exactly max_chapters. If False, generate between 1 and max_chapters\n        based on content complexity. Default is False.\n\n    Returns\n    ----\n    List[str]\n        A list of chapter titles without numbering.\n\n    Examples\n    -----\n    &gt;&gt;&gt; toc = generate_toc(\"This is a text about machine learning...\")\n    &gt;&gt;&gt; print(toc)\n    ['Introduction to Machine Learning', 'Types of Machine Learning', ...]\n\n    &gt;&gt;&gt; toc = generate_toc(\"Content about history...\", max_chapters=20)\n    &gt;&gt;&gt; print(len(toc))\n    15  # The actual number may vary based on content\n\n    &gt;&gt;&gt; toc = generate_toc(\"Content about a simple topic...\", max_chapters=5, fixed_chapter_count=True)\n    &gt;&gt;&gt; print(len(toc))\n    5  # Will always generate exactly 5 chapters\n    \"\"\"\n    # Create the prompt template\n    prompt = create_toc_prompt(max_chapters=max_chapters, fixed_chapter_count=fixed_chapter_count)\n\n    # If no LLM is provided, configure Gemini\n    if llm is None:\n        llm = get_default_llm(temperature)\n\n    # Create the LLM chain\n    chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n    )\n\n    # Invoke the chain with the content\n    result = chain.invoke({\n        \"content\": content,\n        \"max_chapters\": max_chapters\n    })\n\n    # Get the raw text from the result\n    text = result.get(\"text\", \"\")\n\n    # Log the raw LLM response for debugging\n    import logging\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(\"coursemaker\")\n    logger.info(f\"Raw LLM response for TOC generation:\\n{text}\")\n\n    # Split the text by newlines and extract chapter titles\n    lines = text.strip().split(\"\\n\")\n    chapter_titles = []\n\n    for line in lines:\n        # Remove leading numbers, dots, and whitespace\n        line = line.strip()\n        # Match patterns like \"1. \", \"1) \", \"Chapter 1: \", etc.\n        import re\n        line = re.sub(r\"^(\\d+[\\.\\):]|Chapter\\s+\\d+:?)\\s*\", \"\", line)\n\n        if line:  # Skip empty lines\n            chapter_titles.append(line)\n\n    return chapter_titles\n</code></pre>"},{"location":"usage/#geminiteacher.coursemaker.generate_chapter--parameters","title":"Parameters","text":"<p>chapter_title : str     The title of the chapter to explain. content : str     The original raw content to base the explanation on. llm : BaseLanguageModel, optional     The language model to use for generation. If None, the function will      automatically configure a Gemini model. temperature : float, optional     The temperature setting for the LLM, affecting randomness in output.     Default is 0.0 (deterministic output). custom_prompt : Optional[str], optional     Custom instructions to append to the \"\u7cfb\u7edf\u6027\u8bb2\u89e3\" section. Default is None.</p>"},{"location":"usage/#geminiteacher.coursemaker.generate_chapter--returns","title":"Returns","text":"<p>ChapterContent     A structured object containing the chapter's title, summary, explanation, and extension.</p>"},{"location":"usage/#geminiteacher.coursemaker.generate_chapter--examples","title":"Examples","text":"<p>chapter = generate_chapter(\"Machine Learning Basics\", \"Content about ML...\") print(chapter.summary) 'A brief introduction to the fundamental concepts of machine learning...'</p> <p>custom = \"\u8bf7\u7279\u522b\u5173\u6ce8\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\uff0c\u5e76\u63d0\u4f9b\u66f4\u591a\u4ee3\u7801\u793a\u4f8b\u3002\" chapter = generate_chapter(\"Machine Learning Basics\", \"Content about ML...\", custom_prompt=custom)</p> Source code in <code>src\\geminiteacher\\coursemaker.py</code> <pre><code>def generate_chapter(chapter_title: str, content: str, llm: Optional[BaseLanguageModel] = None, temperature: float = 0.0, custom_prompt: Optional[str] = None) -&gt; ChapterContent:\n    \"\"\"\n    Generate a structured explanation for a single chapter.\n\n    This function uses an LLM to create a detailed, structured explanation\n    for a chapter based on its title and the original content.\n\n    Parameters\n    ----\n    chapter_title : str\n        The title of the chapter to explain.\n    content : str\n        The original raw content to base the explanation on.\n    llm : BaseLanguageModel, optional\n        The language model to use for generation. If None, the function will \n        automatically configure a Gemini model.\n    temperature : float, optional\n        The temperature setting for the LLM, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n    custom_prompt : Optional[str], optional\n        Custom instructions to append to the \"\u7cfb\u7edf\u6027\u8bb2\u89e3\" section. Default is None.\n\n    Returns\n    ----\n    ChapterContent\n        A structured object containing the chapter's title, summary, explanation, and extension.\n\n    Examples\n    -----\n    &gt;&gt;&gt; chapter = generate_chapter(\"Machine Learning Basics\", \"Content about ML...\")\n    &gt;&gt;&gt; print(chapter.summary)\n    'A brief introduction to the fundamental concepts of machine learning...'\n\n    &gt;&gt;&gt; custom = \"\u8bf7\u7279\u522b\u5173\u6ce8\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\uff0c\u5e76\u63d0\u4f9b\u66f4\u591a\u4ee3\u7801\u793a\u4f8b\u3002\"\n    &gt;&gt;&gt; chapter = generate_chapter(\"Machine Learning Basics\", \"Content about ML...\", custom_prompt=custom)\n    \"\"\"\n    # Create the prompt template\n    prompt = create_chapter_prompt_template(custom_prompt=custom_prompt)\n\n    # If no LLM is provided, configure Gemini\n    if llm is None:\n        llm = get_default_llm(temperature)\n\n    # Create the LLM chain\n    chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n    )\n\n    # Invoke the chain with the chapter title and content\n    result = chain.invoke({\n        \"chapter_title\": chapter_title,\n        \"content\": content,\n    })\n\n    # Parse the result to extract the structured content\n    try:\n        return parse_chapter_content(chapter_title, result.get(\"text\", \"\"))\n    except Exception as e:\n        # Handle parsing errors by creating a basic chapter with an error note\n        return ChapterContent(\n            title=chapter_title,\n            summary=f\"Error parsing chapter content: {str(e)}\",\n            explanation=\"The LLM response format was unexpected.\",\n            extension=\"Please check the LLM configuration and prompt template.\"\n        )\n</code></pre>"},{"location":"usage/#geminiteacher.coursemaker.generate_summary--parameters","title":"Parameters","text":"<p>content : str     The original raw content to base the summary on. chapters : List[ChapterContent]     The list of chapter content objects to include in the summary. llm : BaseLanguageModel, optional     The language model to use for generation. If None, the function will      automatically configure a Gemini model. temperature : float, optional     The temperature setting for the LLM, affecting randomness in output.     Default is 0.0 (deterministic output).</p>"},{"location":"usage/#geminiteacher.coursemaker.generate_summary--returns","title":"Returns","text":"<p>str     A comprehensive summary of the entire course.</p>"},{"location":"usage/#geminiteacher.coursemaker.generate_summary--examples","title":"Examples","text":"<p>chapters = [ChapterContent(title=\"Chapter 1\", summary=\"Summary 1\")] summary = generate_summary(\"Original content\", chapters) print(summary[:50]) 'This course covers the following key concepts...'</p> Source code in <code>src\\geminiteacher\\coursemaker.py</code> <pre><code>def generate_summary(content: str, chapters: List[ChapterContent], llm: Optional[BaseLanguageModel] = None, temperature: float = 0.0) -&gt; str:\n    \"\"\"\n    Generate a comprehensive summary for the entire course.\n\n    Parameters\n    ----\n    content : str\n        The original raw content to base the summary on.\n    chapters : List[ChapterContent]\n        The list of chapter content objects to include in the summary.\n    llm : BaseLanguageModel, optional\n        The language model to use for generation. If None, the function will \n        automatically configure a Gemini model.\n    temperature : float, optional\n        The temperature setting for the LLM, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n\n    Returns\n    ----\n    str\n        A comprehensive summary of the entire course.\n\n    Examples\n    -----\n    &gt;&gt;&gt; chapters = [ChapterContent(title=\"Chapter 1\", summary=\"Summary 1\")]\n    &gt;&gt;&gt; summary = generate_summary(\"Original content\", chapters)\n    &gt;&gt;&gt; print(summary[:50])\n    'This course covers the following key concepts...'\n    \"\"\"\n    # Create the prompt template\n    prompt = create_summary_prompt_template()\n\n    # Prepare the chapter summaries\n    chapters_summary = \"\\n\\n\".join([\n        f\"\u7ae0\u8282 {i+1}: {chapter.title}\\n{chapter.summary}\"\n        for i, chapter in enumerate(chapters)\n    ])\n\n    # If no LLM is provided, configure Gemini\n    if llm is None:\n        llm = get_default_llm(temperature)\n\n    # Create the LLM chain\n    chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n    )\n\n    # Invoke the chain with the content and chapter summaries\n    result = chain.invoke({\n        \"content\": content,\n        \"chapters_summary\": chapters_summary,\n    })\n\n    return result.get(\"text\", \"\")\n</code></pre>"},{"location":"usage/#geminiteacher.coursemaker.configure_gemini_llm--parameters","title":"Parameters","text":"<p>api_key : str, optional     The Google API key for accessing Gemini models. If None, will use the      GOOGLE_API_KEY environment variable. model_name : str, optional     The name of the Gemini model to use. Default is \"gemini-1.5-pro\". temperature : float, optional     The temperature setting for generation, affecting randomness in output.     Default is 0.0 (deterministic output).</p>"},{"location":"usage/#geminiteacher.coursemaker.configure_gemini_llm--returns","title":"Returns","text":"<p>BaseLanguageModel     A configured Gemini language model ready to use with coursemaker functions.</p>"},{"location":"usage/#geminiteacher.coursemaker.configure_gemini_llm--examples","title":"Examples","text":"<p>llm = configure_gemini_llm(temperature=0.2) course = create_course(\"Content to transform...\", llm=llm)</p>"},{"location":"usage/#geminiteacher.coursemaker.configure_gemini_llm--notes","title":"Notes","text":"<p>This function requires the langchain-google-genai package to be installed: pip install langchain-google-genai</p> Source code in <code>src\\geminiteacher\\coursemaker.py</code> <pre><code>def configure_gemini_llm(api_key: Optional[str] = None, model_name: str = \"gemini-1.5-pro\", temperature: float = 0.0) -&gt; BaseLanguageModel:\n    \"\"\"\n    Configure and return a Google Gemini model for use with the coursemaker module.\n\n    Parameters\n    ----\n    api_key : str, optional\n        The Google API key for accessing Gemini models. If None, will use the \n        GOOGLE_API_KEY environment variable.\n    model_name : str, optional\n        The name of the Gemini model to use. Default is \"gemini-1.5-pro\".\n    temperature : float, optional\n        The temperature setting for generation, affecting randomness in output.\n        Default is 0.0 (deterministic output).\n\n    Returns\n    ----\n    BaseLanguageModel\n        A configured Gemini language model ready to use with coursemaker functions.\n\n    Examples\n    -----\n    &gt;&gt;&gt; llm = configure_gemini_llm(temperature=0.2)\n    &gt;&gt;&gt; course = create_course(\"Content to transform...\", llm=llm)\n\n    Notes\n    -----\n    This function requires the langchain-google-genai package to be installed:\n    pip install langchain-google-genai\n    \"\"\"\n    try:\n        from langchain_google_genai import ChatGoogleGenerativeAI\n    except ImportError:\n        raise ImportError(\n            \"The langchain-google-genai package is required to use Gemini models. \"\n            \"Please install it with: pip install langchain-google-genai\"\n        )\n\n    # Use the provided API key or get it from environment variables\n    if api_key is None:\n        import os\n        api_key = os.environ.get(\"GOOGLE_API_KEY\")\n        if not api_key:\n            raise ValueError(\n                \"No API key provided. Either pass an api_key parameter or \"\n                \"set the GOOGLE_API_KEY environment variable.\"\n            )\n\n    return ChatGoogleGenerativeAI(\n        model=model_name,\n        temperature=temperature,\n        google_api_key=api_key\n    )\n</code></pre>"},{"location":"usage/#parallel-processing","title":"Parallel Processing","text":"<p>Generate multiple chapters in parallel with retry logic and rate limiting.</p> <p>This function orchestrates the parallel generation of multiple chapters, handling API rate limits and retrying failed requests. Each chapter is saved to disk as soon as it's generated.</p> <p>Generate a chapter with retry logic for handling API failures.</p> <p>This function wraps the generate_chapter function with retry logic to handle transient API errors, timeouts, or empty responses.</p> <p>Execute a function on multiple items in parallel with a delay between submissions.</p> <p>This function uses ProcessPoolExecutor to parallelize the execution of a function across multiple items, while introducing a random delay between task submissions to avoid overwhelming external APIs with simultaneous requests.</p>"},{"location":"usage/#geminiteacher.parallel.parallel_generate_chapters--parameters","title":"Parameters","text":"<p>chapter_titles : List[str]     List of chapter titles to generate. content : str     The raw content to use for generating chapters. llm : Optional[BaseLanguageModel], optional     The language model to use. If None, a default model will be configured. temperature : float, optional     The temperature setting for generation. Default is 0.0. custom_prompt : Optional[str], optional     Custom instructions to append to the chapter generation prompt. max_workers : Optional[int], optional     Maximum number of worker processes. If None, uses the default. delay_range : tuple, optional     Range (min, max) in seconds for the random delay between task submissions.     Default is (0.1, 0.5). max_retries : int, optional     Maximum number of retry attempts per chapter. Default is 3. course_title : str, optional     Title of the course for saving files. Default is \"course\". output_dir : str, optional     Directory to save the chapter files. Default is \"output\".</p>"},{"location":"usage/#geminiteacher.parallel.parallel_generate_chapters--returns","title":"Returns","text":"<p>List[ChapterContent]     List of generated chapter contents in the same order as the input titles.</p> Source code in <code>src\\geminiteacher\\parallel.py</code> <pre><code>def parallel_generate_chapters(\n    chapter_titles: List[str],\n    content: str,\n    llm: Optional[BaseLanguageModel] = None,\n    temperature: float = 0.0,\n    custom_prompt: Optional[str] = None,\n    max_workers: Optional[int] = None,\n    delay_range: tuple = (0.1, 0.5),\n    max_retries: int = 3,\n    course_title: str = \"course\",\n    output_dir: str = \"output\"\n) -&gt; List[ChapterContent]:\n    \"\"\"\n    Generate multiple chapters in parallel with retry logic and rate limiting.\n\n    This function orchestrates the parallel generation of multiple chapters,\n    handling API rate limits and retrying failed requests. Each chapter is\n    saved to disk as soon as it's generated.\n\n    Parameters\n    ----------\n    chapter_titles : List[str]\n        List of chapter titles to generate.\n    content : str\n        The raw content to use for generating chapters.\n    llm : Optional[BaseLanguageModel], optional\n        The language model to use. If None, a default model will be configured.\n    temperature : float, optional\n        The temperature setting for generation. Default is 0.0.\n    custom_prompt : Optional[str], optional\n        Custom instructions to append to the chapter generation prompt.\n    max_workers : Optional[int], optional\n        Maximum number of worker processes. If None, uses the default.\n    delay_range : tuple, optional\n        Range (min, max) in seconds for the random delay between task submissions.\n        Default is (0.1, 0.5).\n    max_retries : int, optional\n        Maximum number of retry attempts per chapter. Default is 3.\n    course_title : str, optional\n        Title of the course for saving files. Default is \"course\".\n    output_dir : str, optional\n        Directory to save the chapter files. Default is \"output\".\n\n    Returns\n    -------\n    List[ChapterContent]\n        List of generated chapter contents in the same order as the input titles.\n    \"\"\"\n    logger = logging.getLogger(\"geminiteacher.parallel\")\n    logger.info(f\"Starting parallel generation of {len(chapter_titles)} chapters with {max_workers or multiprocessing.cpu_count()} workers\")\n\n    # Get API key from the LLM if provided or environment\n    api_key = None\n    model_name = \"gemini-1.5-pro\"\n\n    if llm is not None:\n        # Try to extract API key and model name from the provided LLM\n        try:\n            # This assumes LLM is a ChatGoogleGenerativeAI instance\n            api_key = getattr(llm, \"google_api_key\", None)\n            model_name = getattr(llm, \"model\", model_name)\n            logger.info(f\"Using model: {model_name}\")\n        except Exception:\n            logger.warning(\"Could not extract API key from provided LLM, will use environment variables\")\n\n    # Create a list of (index, chapter_title) tuples to preserve order\n    indexed_titles = list(enumerate(chapter_titles))\n\n    logger.info(f\"Using delay range: {delay_range[0]}-{delay_range[1]}s between tasks\")\n    logger.info(f\"Saving chapters progressively to {output_dir}/{course_title}/\")\n\n    # Generate chapters in parallel with delay between submissions and save each one as it completes\n    results = parallel_map_with_delay(\n        _worker_generate_and_save_chapter,\n        indexed_titles,\n        max_workers=max_workers,\n        delay_range=delay_range,\n        content=content,\n        course_title=course_title,\n        output_dir=output_dir,\n        api_key=api_key,\n        model_name=model_name,\n        temperature=temperature,\n        custom_prompt=custom_prompt,\n        max_retries=max_retries,\n        retry_delay=1.0\n    )\n\n    # Extract just the chapter content from the results (idx, chapter, file_path)\n    chapters = [result[1] for result in results]\n\n    logger.info(f\"Completed parallel generation of {len(chapters)} chapters\")\n    return chapters \n</code></pre>"},{"location":"usage/#geminiteacher.parallel.generate_chapter_with_retry--parameters","title":"Parameters","text":"<p>chapter_title : str     The title of the chapter to generate. content : str     The raw content to use for generating the chapter. llm : Optional[BaseLanguageModel], optional     The language model to use. If None, a default model will be configured. temperature : float, optional     The temperature setting for generation. Default is 0.0. custom_prompt : Optional[str], optional     Custom instructions to append to the chapter generation prompt. max_retries : int, optional     Maximum number of retry attempts. Default is 3. retry_delay : float, optional     Base delay between retries in seconds. Default is 1.0.</p>"},{"location":"usage/#geminiteacher.parallel.generate_chapter_with_retry--returns","title":"Returns","text":"<p>ChapterContent     The generated chapter content.</p>"},{"location":"usage/#geminiteacher.parallel.generate_chapter_with_retry--notes","title":"Notes","text":"<p>This function implements an exponential backoff strategy for retries, with each retry attempt waiting longer than the previous one.</p> Source code in <code>src\\geminiteacher\\parallel.py</code> <pre><code>def generate_chapter_with_retry(\n    chapter_title: str, \n    content: str, \n    llm: Optional[BaseLanguageModel] = None,\n    temperature: float = 0.0,\n    custom_prompt: Optional[str] = None,\n    max_retries: int = 3,\n    retry_delay: float = 1.0\n) -&gt; ChapterContent:\n    \"\"\"\n    Generate a chapter with retry logic for handling API failures.\n\n    This function wraps the generate_chapter function with retry logic to handle\n    transient API errors, timeouts, or empty responses.\n\n    Parameters\n    ----------\n    chapter_title : str\n        The title of the chapter to generate.\n    content : str\n        The raw content to use for generating the chapter.\n    llm : Optional[BaseLanguageModel], optional\n        The language model to use. If None, a default model will be configured.\n    temperature : float, optional\n        The temperature setting for generation. Default is 0.0.\n    custom_prompt : Optional[str], optional\n        Custom instructions to append to the chapter generation prompt.\n    max_retries : int, optional\n        Maximum number of retry attempts. Default is 3.\n    retry_delay : float, optional\n        Base delay between retries in seconds. Default is 1.0.\n\n    Returns\n    -------\n    ChapterContent\n        The generated chapter content.\n\n    Notes\n    -----\n    This function implements an exponential backoff strategy for retries,\n    with each retry attempt waiting longer than the previous one.\n    \"\"\"\n    logger = logging.getLogger(\"geminiteacher.parallel\")\n\n    for attempt in range(max_retries + 1):\n        try:\n            logger.info(f\"Generating chapter '{chapter_title}' (attempt {attempt + 1}/{max_retries + 1})\")\n            chapter = generate_chapter(\n                chapter_title=chapter_title,\n                content=content,\n                llm=llm,\n                temperature=temperature,\n                custom_prompt=custom_prompt\n            )\n\n            # Check if we got a valid response (non-empty explanation)\n            if chapter.explanation.strip():\n                logger.info(f\"Successfully generated chapter '{chapter_title}' (length: {len(chapter.explanation)} chars)\")\n                return chapter\n            else:\n                raise ValueError(\"Empty chapter explanation received\")\n\n        except Exception as e:\n            if attempt &lt; max_retries:\n                # Calculate backoff with jitter\n                backoff = retry_delay * (2 ** attempt) + random.uniform(0, 1)\n                logger.warning(\n                    f\"Chapter generation failed for '{chapter_title}' \"\n                    f\"(attempt {attempt + 1}/{max_retries + 1}): {str(e)}. \"\n                    f\"Retrying in {backoff:.2f}s...\"\n                )\n                time.sleep(backoff)\n            else:\n                logger.error(\n                    f\"All retry attempts failed for chapter '{chapter_title}'. \"\n                    f\"Last error: {str(e)}\"\n                )\n                # Return a basic chapter with error information\n                return ChapterContent(\n                    title=chapter_title,\n                    summary=\"Error: Failed to generate chapter content after multiple attempts.\",\n                    explanation=f\"The chapter generation process encountered repeated errors: {str(e)}\",\n                    extension=\"Please try regenerating this chapter or check your API configuration.\"\n                )\n</code></pre>"},{"location":"usage/#geminiteacher.parallel.parallel_map_with_delay--parameters","title":"Parameters","text":"<p>func : Callable[..., T]     The function to execute in parallel. items : List[Any]     The list of items to process. max_workers : Optional[int], optional     Maximum number of worker processes. If None, uses the default     (typically the number of CPU cores). delay_range : tuple, optional     Range (min, max) in seconds for the random delay between task submissions.     Default is (0.1, 0.5). **kwargs     Additional keyword arguments to pass to the function.</p>"},{"location":"usage/#geminiteacher.parallel.parallel_map_with_delay--returns","title":"Returns","text":"<p>List[T]     List of results in the same order as the input items.</p>"},{"location":"usage/#geminiteacher.parallel.parallel_map_with_delay--examples","title":"Examples","text":"<p>def process_item(item, factor=1): ...     return item * factor items = [1, 2, 3, 4, 5] results = parallel_map_with_delay(process_item, items, factor=2) print(results) [2, 4, 6, 8, 10]</p> Source code in <code>src\\geminiteacher\\parallel.py</code> <pre><code>def parallel_map_with_delay(\n    func: Callable[..., T],\n    items: List[Any],\n    max_workers: Optional[int] = None,\n    delay_range: tuple = (0.1, 0.5),\n    **kwargs\n) -&gt; List[T]:\n    \"\"\"\n    Execute a function on multiple items in parallel with a delay between submissions.\n\n    This function uses ProcessPoolExecutor to parallelize the execution of a function\n    across multiple items, while introducing a random delay between task submissions\n    to avoid overwhelming external APIs with simultaneous requests.\n\n    Parameters\n    ----------\n    func : Callable[..., T]\n        The function to execute in parallel.\n    items : List[Any]\n        The list of items to process.\n    max_workers : Optional[int], optional\n        Maximum number of worker processes. If None, uses the default\n        (typically the number of CPU cores).\n    delay_range : tuple, optional\n        Range (min, max) in seconds for the random delay between task submissions.\n        Default is (0.1, 0.5).\n    **kwargs\n        Additional keyword arguments to pass to the function.\n\n    Returns\n    -------\n    List[T]\n        List of results in the same order as the input items.\n\n    Examples\n    --------\n    &gt;&gt;&gt; def process_item(item, factor=1):\n    ...     return item * factor\n    &gt;&gt;&gt; items = [1, 2, 3, 4, 5]\n    &gt;&gt;&gt; results = parallel_map_with_delay(process_item, items, factor=2)\n    &gt;&gt;&gt; print(results)\n    [2, 4, 6, 8, 10]\n    \"\"\"\n    logger = logging.getLogger(\"geminiteacher.parallel\")\n    results = []\n    total_items = len(items)\n\n    # Export the current log level to the environment for worker processes\n    os.environ[\"GeminiTeacher_LOG_LEVEL\"] = logger.getEffectiveLevel().__str__()\n\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        # Submit all tasks with a delay between submissions\n        futures = []\n        for i, item in enumerate(items):\n            # Add a small random delay to avoid overwhelming the API\n            delay = random.uniform(delay_range[0], delay_range[1])\n            logger.debug(f\"Submitting task {i+1}/{total_items} with delay: {delay:.2f}s\")\n            time.sleep(delay)\n\n            # Submit the task to the process pool\n            future = executor.submit(func, item, **kwargs)\n            futures.append(future)\n            logger.info(f\"Submitted task {i+1}/{total_items}\")\n\n        # Collect results in the original order\n        for i, future in enumerate(futures):\n            try:\n                logger.info(f\"Waiting for task {i+1}/{total_items} to complete\")\n                result = future.result()\n                results.append(result)\n                logger.info(f\"Completed task {i+1}/{total_items}\")\n            except Exception as e:\n                logger.error(f\"Task {i+1}/{total_items} failed: {str(e)}\")\n                # Re-raise the exception to maintain the expected behavior\n                raise\n\n    return results\n</code></pre>"},{"location":"usage/#app-module","title":"App Module","text":"<p>Generate a course with progressive saving of chapters.</p> <p>This function creates a course and saves each chapter as it's generated, providing a more robust approach for long-running generations.</p> <p>Configure logging for the application.</p>"},{"location":"usage/#geminiteacher.app.generate_course.create_course_with_progressive_save--parameters","title":"Parameters","text":"<p>content : str     The raw content to transform into a course course_title : str     Title of the course output_dir : str     Directory to save the generated files llm : Optional[BaseLanguageModel], optional     Language model to use. If None, a default model will be configured. temperature : float, optional     Temperature for generation. Default is 0.0. verbose : bool, optional     Whether to print progress messages. Default is False. max_chapters : int, optional     Maximum number of chapters. Default is 10. fixed_chapter_count : bool, optional     Whether to use fixed chapter count. Default is False. custom_prompt : Optional[str], optional     Custom prompt instructions. Default is None. max_workers : Optional[int], optional     Maximum number of worker processes. If None, uses the default. delay_range : tuple, optional     Range (min, max) in seconds for the random delay between task submissions.     Default is (0.1, 0.5). max_retries : int, optional     Maximum number of retry attempts per chapter. Default is 3. logger : Optional[logging.Logger], optional     Logger instance to use. If None, a new logger will be created.</p>"},{"location":"usage/#geminiteacher.app.generate_course.create_course_with_progressive_save--returns","title":"Returns","text":"<p>Course     The generated course object</p> Source code in <code>src\\geminiteacher\\app\\generate_course.py</code> <pre><code>def create_course_with_progressive_save(\n    content: str,\n    course_title: str,\n    output_dir: str,\n    llm=None,\n    temperature: float = 0.0,\n    verbose: bool = False,\n    max_chapters: int = 10,\n    fixed_chapter_count: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_workers: Optional[int] = None,\n    delay_range: tuple = (0.1, 0.5),\n    max_retries: int = 3,\n    logger=None\n) -&gt; Course:\n    \"\"\"\n    Generate a course with progressive saving of chapters.\n\n    This function creates a course and saves each chapter as it's generated,\n    providing a more robust approach for long-running generations.\n\n    Parameters\n    ----------\n    content : str\n        The raw content to transform into a course\n    course_title : str\n        Title of the course\n    output_dir : str\n        Directory to save the generated files\n    llm : Optional[BaseLanguageModel], optional\n        Language model to use. If None, a default model will be configured.\n    temperature : float, optional\n        Temperature for generation. Default is 0.0.\n    verbose : bool, optional\n        Whether to print progress messages. Default is False.\n    max_chapters : int, optional\n        Maximum number of chapters. Default is 10.\n    fixed_chapter_count : bool, optional\n        Whether to use fixed chapter count. Default is False.\n    custom_prompt : Optional[str], optional\n        Custom prompt instructions. Default is None.\n    max_workers : Optional[int], optional\n        Maximum number of worker processes. If None, uses the default.\n    delay_range : tuple, optional\n        Range (min, max) in seconds for the random delay between task submissions.\n        Default is (0.1, 0.5).\n    max_retries : int, optional\n        Maximum number of retry attempts per chapter. Default is 3.\n    logger : Optional[logging.Logger], optional\n        Logger instance to use. If None, a new logger will be created.\n\n    Returns\n    -------\n    Course\n        The generated course object\n    \"\"\"\n    if logger is None:\n        logger = logging.getLogger(\"geminiteacher.app\")\n\n    # Create the course using parallel processing with progressive saving\n    course = create_course(\n        content,\n        llm=llm,\n        temperature=temperature,\n        verbose=verbose,\n        max_chapters=max_chapters,\n        fixed_chapter_count=fixed_chapter_count,\n        custom_prompt=custom_prompt\n    )\n\n    # Save the course to files\n    save_course_to_files(course_title, course, output_dir)\n\n    return course\n</code></pre>"},{"location":"usage/#geminiteacher.app.generate_course.configure_logging--parameters","title":"Parameters","text":"<p>log_file : str, optional     Path to the log file verbose : bool, optional     Whether to use verbose (DEBUG) logging</p>"},{"location":"usage/#geminiteacher.app.generate_course.configure_logging--returns","title":"Returns","text":"<p>logging.Logger     Configured logger instance</p> Source code in <code>src\\geminiteacher\\app\\generate_course.py</code> <pre><code>def configure_logging(log_file=None, verbose=False):\n    \"\"\"\n    Configure logging for the application.\n\n    Parameters\n    ----------\n    log_file : str, optional\n        Path to the log file\n    verbose : bool, optional\n        Whether to use verbose (DEBUG) logging\n\n    Returns\n    -------\n    logging.Logger\n        Configured logger instance\n    \"\"\"\n    log_level = logging.DEBUG if verbose else logging.INFO\n\n    # Configure root logger\n    logging.basicConfig(\n        level=log_level,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.StreamHandler()  # Console handler\n        ]\n    )\n\n    # Create a logger for this application\n    logger = logging.getLogger(\"geminiteacher.app\")\n\n    # Add file handler if log_file is provided\n    if log_file:\n        log_dir = os.path.dirname(log_file)\n        if log_dir:\n            os.makedirs(log_dir, exist_ok=True)\n\n        file_handler = logging.FileHandler(log_file)\n        file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n        file_handler.setFormatter(file_formatter)\n        file_handler.setLevel(log_level)\n\n        # Add the file handler to the logger\n        logger.addHandler(file_handler)\n\n        # Also add the file handler to the geminiteacher loggers\n        for module in [\"geminiteacher\", \"geminiteacher.coursemaker\", \"geminiteacher.parallel\"]:\n            module_logger = logging.getLogger(module)\n            module_logger.addHandler(file_handler)\n            module_logger.setLevel(log_level)\n\n    # Set environment variable with log level for worker processes\n    os.environ[\"GeminiTeacher_LOG_LEVEL\"] = str(log_level)\n\n    return logger\n</code></pre>"},{"location":"usage/#data-models","title":"Data Models","text":"<p>               Bases: <code>BaseModel</code></p> <p>A complete course with chapters and summary.</p> <p>               Bases: <code>BaseModel</code></p> <p>A structured representation of a chapter's content.</p>"},{"location":"usage/#geminiteacher.coursemaker.Course--attributes","title":"Attributes","text":"<p>content : str     The original raw content used to generate the course. chapters : List[ChapterContent]     The list of chapters in the course. summary : str     A comprehensive summary of the entire course.</p> Source code in <code>src\\geminiteacher\\coursemaker.py</code> <pre><code>class Course(BaseModel):\n    \"\"\"\n    A complete course with chapters and summary.\n\n    Attributes\n    ----\n    content : str\n        The original raw content used to generate the course.\n    chapters : List[ChapterContent]\n        The list of chapters in the course.\n    summary : str\n        A comprehensive summary of the entire course.\n    \"\"\"\n\n    content: str\n    chapters: List[ChapterContent] = []\n    summary: str = \"\"\n</code></pre>"},{"location":"usage/#geminiteacher.coursemaker.ChapterContent--attributes","title":"Attributes","text":"<p>title : str     The title of the chapter. summary : str     A brief summary or introduction to the chapter. explanation : str     The detailed explanation of the chapter's content. extension : str     Additional thoughts or extensions related to the chapter.</p> Source code in <code>src\\geminiteacher\\coursemaker.py</code> <pre><code>class ChapterContent(BaseModel):\n    \"\"\"\n    A structured representation of a chapter's content.\n\n    Attributes\n    ----\n    title : str\n        The title of the chapter.\n    summary : str\n        A brief summary or introduction to the chapter.\n    explanation : str\n        The detailed explanation of the chapter's content.\n    extension : str\n        Additional thoughts or extensions related to the chapter.\n    \"\"\"\n\n    title: str\n    summary: str = \"\"\n    explanation: str = \"\"\n    extension: str = \"\"\n</code></pre>"},{"location":"usage/#limitations","title":"Limitations","text":"<ul> <li>For production use, a valid Google API key must be provided.</li> <li>API rate limits may affect parallel processing performance.</li> <li>Large content may require higher token limits from your Gemini API plan. </li> </ul>"}]}